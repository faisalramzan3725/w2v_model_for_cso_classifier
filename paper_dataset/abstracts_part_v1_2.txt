Progress is reported in the design and analysis of spread-spectrum packet radio networks for the factory of the future. This progress includes the accurate analysis of the probability of packet success in a direct-sequence spread-spectrum communication channel and analytical results on the transient behavior of packet radio networks. The first use of these networks will be to provide communications between a transport system controller and a fleet of autonomous guided vehicles. >
Manufacturing decision making with FACTOR
The FACTOR system is designed to support the effective management of the capacity of manufacturing organization. This philosophy is best described as total capacity management (TCM). The TCM fundamental principle suggests that through a thorough understanding of a system's capacity and the ability to control that capacity, a manufacturing system can profitably and predictably deliver quality products to its customers. This tutorial covers the basic concepts of FACTOR. FACTOR has been applied to engineering, design, scheduling and planning problems within many manufacturing organizations. Topics covered include: the FACTOR modeling constructs, integration with existing production data, the use of FACTOR for schedule creation and adjustment, FACTOR/AIM, and new enhancements to the products.
Learning with Technology: Using Discussion Forums to Augment a Traditional-Style Class
There is considerable evidence that using technology as an instructional tool improves student learning and educational outcomes (Hanna & de Nooy, 2003). In developing countries, pre-university education focuses on memorization, although meting the mission of AUST requires students to manage technology and to think more independently. This study examines the impact of incorporating a discussion forum on the achievement of university students enrolled in a Distance Education course, Educational Technology Department at Ajman University of Science and Technology (AUST), United Arab Emirates. The study was conducted with 34 students divided into two sections, one a treatment group and one a control group. Both sections were exposed to the same teaching techniques covering the same course material on Distance Education. Four weeks after the course had commenced they were given the same teacher constructed test. However, after the first test, the treated group was exposed to the use of a World Wide Web (WWW) interactive discussion forum. At the end of the semester-long treatment period, a final test was given to both groups, and student scores were analyzed for any statistically significant difference. Questionnaires and interviews were also conducted to see if students had enjoyed the experience. The results of the study indicated that students in both groups showed learning improvement over the course of one semester, but discussion forums had an obvious impact on student achievement and attitude in distance learning/ educational technology course.
CCHMM_PROF: a HMM-based Coiled-Coil Predictor with Evolutionary Information
Motivation: The widespread coiled-coil structural motif in proteins is known to mediate a variety of biological interactions. Recognizing a coiled-coil containing sequence and locating its coiled-coil domains are key steps towards the determination of the protein structure and function. Different tools are available for predicting coiled-coil domains in protein sequences, including those based on positionspecific score matrices and machine learning methods. Results: In this article, we introduce a hidden Markov model (CCHMM_PROF) that exploits the information contained in multiple sequence alignments (profiles) to predict coiled-coil regions. The new method discriminates coiled-coil sequences with an accuracy of 97% and achieves a true positive rate of 79% with only 1% of false positives. Furthermore, when predicting the location of coiled-coil segments in protein sequences, the method reaches an accuracy of 80% at the residue level and a best per-segment and perprotein efficiency of 81% and 80%, respectively. The results indicate that CCHMM_PROF outperforms all the existing tools and can be adopted for large-scale genome annotation. Availability: The dataset is available at http://www.biocomp.unibo .it/∼lisa/coiled-coils. The predictor is freely available at http://gpcr .biocomp.unibo.it/cgi/predictors/cchmmprof/pred_cchmmprof.cgi. Contact: piero@biocomp.unibo.it
Performance of local area network protocols for hard real-time applications
Simulation experiments show that the token ring protocol gave a lower average message delay at low transfer rates, but the token bus protocol gave a better overall performance for applications where only average delay is of interest. On the other hand, in hard real-time systems, the criterion of importance is not the average message delay, but the maximum message delay and the ability to meet deadlines. Slotted ring in this case is a much better protocol than the others because of its low maximum message delay and more predictable message delay. Because of this, and because the average performance of the slotted ring remains good as the size or the transfer rate of the network increases, the slotted ring protocol is preferred over the token ring and token bus protocols for hard real-time systems. >
A simple algorithm for decomposing convex structuring elements
A finite subset of Z/sup 2/ is called a structuring element. The paper presents a new and simple algorithm for decomposing a convex structuring element as a sequence of Minkowski additions of a minimum number of subsets of the elementary square (i.e., the 3/spl times/3 square centered at the origin). Besides its simplicity, the advantage of this algorithm over some known algorithms is that it generates a sequence of non necessarily convex subsets, which means subsets with smaller cardinality and consequently faster implementation of the corresponding dilations and erosions. The algorithm is based on algebraic and geometrical properties of Minkowski additions. Theoretical analysis of correctness and computational time complexity are also presented.
A BIST pattern generator design for near-perfect fault coverage
A new design methodology for a pattern generator is proposed, formulated in the context of on-chip BIST. The design methodology is circuit-specific and uses synthesis techniques to design BIST generators. The pattern generator consists of two components: a pseudorandom pattern generator (like an LFSR or, preferably, a GLFSR) and a combinational logic to map the outputs of the pseudorandom pattern generator. This combinational logic is synthesized to produce a given set of target patterns by mapping the outputs of the pseudorandom pattern generator. It is shown that, for a particular CUT, an area-efficient combinational logic block can be designed/synthesized to achieve 100 (or almost 100) percent single stuck-at fault coverage using a small number of test the This method is significantly different from weighted pattern generation and can guarantee testing of all hard-to-detect faults without expensive test point insertion. Experimental results on common benchmark netlists demonstrate that the fault coverage of the proposed pattern generator is significantly higher compared to conventional pattern generation techniques. The design technique for the logic mapper is unique and can be used effectively to improve existing pattern generators for combinational logic and scan-based BIST structures.
MODEL OF CARDIAC TISSUE AS A CONDUCTIVE SYSTEM WITH INTERACTING PACEMAKERS AND REFRACTORY TIME
The model of the cardiac tissue as a conductive system with two interacting pacemakers and a refractory time is proposed. In the parametric space of the model the phase locking areas are investigated in detail. The obtained results make possible to predict the behavior of excitable systems with two pacemakers, depending on the type and intensity of their interaction and the initial phase. Comparison of the described phenomena with intrinsic pathologies of cardiac rhythms is given.
The Systolic Pixel: A Visible Surface Algorithm for VLSI.
Abstract#R##N##R##N#The Systolic Pixel or Spixel is a novel architecture for an intelligent pixel-based graphics database for geometric-solid models. An algorithm is described which performs visible surface calculations for any complexity of coloured 3-dimensional (3-D) surface and which structures geometric-solid model data in a natural way. The algorithm/architecture of the spixel features a simple set of priority rules acting upon data in nearest neighbour locations and a simple set of movement rules of data to nearest neighbour locations. The spixel is constructed out of identical functional units. These features are attractive for an implementation of the algorithm in Very Large Scale Integration (VLSI).
Generating Case Markers in Machine Translation
We study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly (English) to a language with a rich system of surface case markers (Japanese). We propose an extension of n-best re-ranking as a method of integrating such models into a statistical MT system and show that this method substantially outperforms standard n-best re-ranking. Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric. Human evaluation also confirms the results.
Soft-limiter receivers for coded DS/DPSK systems
The performance of a soft-limiter metric and a quantized soft-limiter metric is evaluated for coded DS/DPSK (direct sequence/differential phase shift keying) in the presence of worst case pulse jamming and background noise. The metrics are easy to implement and do not require jammer state information. Instead they rely on the use of receiver thresholds, which must be adjusted according to the code rate and the received bit-energy-to-background-noise ratio. The performance of the metrics is evaluated by using the cutoff rate criterion and a number of specific convolutional and block codes. It is shown that the metrics can offer a significant soft-decision decoding gain and can perform to within 0.5-1.5 dB of the maximum-likelihood soft-decision metric with perfect jammer state information. >
Technology Dependence In Function Point Analysis: A Case Study And Critical Review
Because Function Point Analysis (FPA) has now been in use for a decade, and in spite of its increasing popularity has met with some recent criticisms, it is time to review how appropriate it still is for today's technologies. A critical review of the FPA approach examines in particular the pioneering and continuing work of Albrecht and more recent work by Symons. Technological dependencies in FPA-type metrics are identified and a general model for deriving a new FPA-type metric for a new software technology is given. A model for the calibration of FPA-type metrics for new technologies in terms of a reference technology is also presented. Such calibration is essential for comparative productivity studies. The role of module estimation in exposing parts of the 'anatomy' of the FPA approach is investigated. The derivation and calibration models are applied to a significant case study in which a new FPA-type metric suited to a particular software development technology is derived, calibrated and compared with other published versions of FPA metrics.
Pruning recurrent neural networks for improved generalization performance
The experimental results in this paper demonstrate that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages. The technique also permits the extraction of symbolic knowledge in the form of deterministic finite-state automata (DFA) which are more consistent with the rules to be learned. Weight decay has also been shown to improve a network's generalization performance. Simulations with two small DFA (/spl les/10 states) and a large finite-memory machine (64 states) demonstrate that the performance improvement due to pruning/retraining is generally superior to the improvement due to training with weight decay. In addition, there is no need to guess a 'good' decay rate. >
Adaptive design of digital filters
In this paper, we present a novel technique for the design of FIR and IIR digital filters. The design approach begins with the specification of a discrete set of arbitrary magnitude and phase characteristics which describe a desired filter response. These frequency domain characteristics are used to create an ideal "pseudo-filter" whose impulse response is unknown and possibly non-causal, but whose input/output characteristics can be determined for a finite sum of sinusoids. Time-domain techniques common to adaptive system identification are then used to identify a realizable FIR or IIR digital filter which best matches the pseudo-filter. The advantages of this method include the ability to specify response at arbitrarily-spaced frequencies, to use arbitrary cost weighting, and to apply (possibly non-linear) constraints to the range of the filter coefficients.
Experimental Examination in simulated interactive situation between people and mobile robot with preliminary-announcement and indication function of upcoming operation
This paper presents the result of the experimental examination by "passing each other" and "positional prediction" in simulated interactive situation between people and mobile robot. We have developed four prototype robots based on four proposed methods for preliminarily announcing and indicating to people the speed and direction of upcoming movement of mobile robot moving on two-dimensional plane. We observed significant difference between when there was a preliminary-announcement and indication (PAI) function and when there was not even in each experiment. Therefore the effect of preliminary-announcement and indication of upcoming operation was declared. In addition the feature and effective usage of each type of preliminary-announcement and indication method were clarified. That is, the method of announcing state of operation just after the present is effective when a person has to judge to which direction he should get on immediately due to the feature that simple information can be quickly transmitted. The method of indicating operations from the present to some future time continuously is effective when a person wants to avoid contact or collision surely and correctly owing to the feature that complicated information can be accurately transmitted. We would like to verify the result in various conditions such as the case that traffic lines are obliquely crossed.
Opportunistic Beamforming over Rayleigh Channels with Partial Side Information
In recent years, diversity techniques have evolved into highly attractive technology for wireless communications in different forms. For instance, the channel fluctuations of the users in a network are exploited as multiuser diversity by scheduling the user with the best signal-to-noise ratio (SNR). When fading is slow, beamforming at a multiple antenna transmitter is used to induce artificial channel fluctuations to ensure multiuser diversity in the network. Such a beamforming scheme is called opportunistic beamforming since the transmitter uses random beamforming to artificially induce opportunism in the network [1]. Opportunism requires a large number of users in the system in order to reach the performance of the true beamforming that uses perfect channel state information (CSI). In this paper we investigate the benefit of having partial CSI at an opportunistic transmitter. In the investigation, we focus on the maximum normalized SNR scheduling where user's feedback consists of SNR relative to its channel gain. We show that opportunism can be beneficially used to increase the average throughput of the system. Simulations support the analytical average throughput results obtained as the amount of CSI and the number of users vary.
An ILP formulation for system level throughput and power optimization in multiprocessor SoC architectures
System-level low power scheduling techniques are required for optimizing the performance and power of embedded applications that are mapped to multiprocessor System-on-Chip (SoC) architectures. In this paper, we present an integer linear programming (ILP) formulation that combines loop transformations (pipelining and unrolling) and system-level low power optimization techniques (dynamic voltage scaling (DVS) and power management (DPM)) to minimize the power consumption, while satisfying the period and deadline constraints of the application. We also present three modifications that relax one or more constraints in the optimal formulation in order to obtain smaller run times. We present experimental analysis by applying the formulations on an MPEG decoder algorithm. All results are compared against two existing techniques. Our formulations result in large system-level power reductions (max: 48.2%, min: 15.92%, avg: 31.9%). The modified ILP formulations result in exponential decrease in runtimes, and a corresponding linear degradation in the result quality.
Enhancement of semantics in CBIR
Although much research has been done in the area of content based image retrieval (CBIR), little progress has been made to fully implement an engine solely based on the search of image content. This paper examines one of the basic problems in pattern recognition which highlights the difficulty in the area of content understanding in CBIR, i.e. the inability of current systems to fully incorporate low level features of image, such as intensity, colour, texture, shape and spatial constraints characteristics, with the high level features such as semantic content. To further the development of content based image processing, semantic algorithms should be combined with low level features and be used to process the image objects.
MASISH: a database for gene expression in maize seeds
Grass seeds are complex organs composed by multiple tissues and cell types which develop coordinately to produce a viable embryo. The identification of genes involved in seed development is of great interest, but systematic spatial analyses of gene expression on maize seeds at the cell level have not yet been performed. MASISH is an online database holding information for gene expression spatial patterns in maize seeds based on in situ hybridization experiments. The web-based query interface allows the execution of gene queries and provides hybridization images, published references and information of the analyzed genes. Availability: http://masish.uab.cat/ Contact: cvsgmp@cid.csic.es The maize kernel is classified botanically as a caryopsis. In consequence it is a fruit composed by one seed and the remnants of the seed coats and nucellus and is permanently enclosed in the pericarp. The endosperm occupies most of the seed and is basically a storage organ that accumulates starch and proteins. The aleurone layer is part of the endosperm and consists in a continuous layer of large cubical cells which accumulate protein and lipid granules and surrounds most of the endosperm. In the area of the pedicel, which connects the seed to the mother plant, the cells adopt a special morphology, typical of transfer cells, and form the basal transfer cell layer. The embryo consists of an embryonic axis and a single cotyledon, which is called the scutellum. The embryo axis is formed by the plumule, covered by the coleoptile and the radicle, covered by coleorhiza. All these organs are almost completely surrounded by the scutellum, an organ whose major function is to accumulate nutrient reserves, mainly lipids and proteins. A single layer of cells directly in contact with the endosperm, which is called the scutellar epithelium, is important in the digestion and transport of the nutrients from the endosperm to the embryo axis during germination. Both endosperm and embryo derive from the fusion of gametes, but while the embryo is derived from the fertilized egg, triploid endosperm is derived from fertilized polar nuclei. Surrounding the endosperm and embryo lays the pericarp, a protective organ derived from maternal tissues (more information at http://masish.uab.cat/masish/images/maizeseedanatomy.pdf). Full genome sequencing allows the identification of the complete catalog of genes in a species. However, the roles of a high proportion of these genes remain unknown. The description of
Unified Blind Method for Multi-Image Super-Resolution and Single/Multi-Image Blur Deconvolution
This paper presents, for the first time, a unified blind method for multi-image super-resolution (MISR or SR), single-image blur deconvolution (SIBD), and multi-image blur deconvolution (MIBD) of low-resolution (LR) images degraded by linear space-invariant (LSI) blur, aliasing, and additive white Gaussian noise (AWGN). The proposed approach is based on alternating minimization (AM) of a new cost function with respect to the unknown high-resolution (HR) image and blurs. The regularization term for the HR image is based upon the Huber-Markov random field (HMRF) model, which is a type of variational integral that exploits the piecewise smooth nature of the HR image. The blur estimation process is supported by an edge-emphasizing smoothing operation, which improves the quality of blur estimates by enhancing strong soft edges toward step edges, while filtering out weak structures. The parameters are updated gradually so that the number of salient edges used for blur estimation increases at each iteration. For better performance, the blur estimation is done in the filter domain rather than the pixel domain, i.e., using the gradients of the LR and HR images. The regularization term for the blur is Gaussian (L2 norm), which allows for fast noniterative optimization in the frequency domain. We accelerate the processing time of SR reconstruction by separating the upsampling and registration processes from the optimization procedure. Simulation results on both synthetic and real-life images (from a novel computational imager) confirm the robustness and effectiveness of the proposed method.
Synthesis of multi-qudit hybrid and d-valued quantum logic circuits by decomposition
Recent research in generalizing quantum computation from 2-valued qudits to d-valued qudits has shown practical advantages for scaling up a quantum computer. A further generalization leads to quantum computing with hybrid qudits where two or more qudits have different finite dimensions. Advantages of hybrid and d-valued gates (circuits) and their physical realizations have been studied in detail by Muthukrishnan and Stroud [Multi-valued logic gates for quantum computation, Phys. Rev. A 62 (2000) 052309. [10]], Daboul et al. [Quantum gates on hybrid qudits, J. Phys. A Math. Gen. 36 (2003) 2525-2536. [5]], and Bartlett et al. [Quantum encodings in spin systems and harmonic oscillators, Phys. Rev. A 65 (2002) 052316. [17]]. In both cases, a quantum computation is performed when a unitary evolution operator, acting as a quantum logic gate, transforms the state of qudits in a quantum system. Unitary operators can be represented by square unitary matrices. If the system consists of a single qudit, then Tilma et al. [Generalized Euler angle parameterization for SU(N), J. Phys. A Math. Gen. 35 (2002) 10467-10501. [15]] have shown that the unitary evolution matrix (gate) can be synthesized in terms of its Euler angle parametrization. However, if the quantum system consists of multiple qudits, then a gate may be synthesized by matrix decomposition techniques such as QR factorization and the cosine-sine decomposition (CSD). In this article, we present a CSD based synthesis method for n qudit hybrid quantum gates, and as a consequence, derive a CSD based synthesis method for n qudit gates where all the qudits have the same dimension.
Point location in zones of k -flats in arrangements
Abstract   Let  A ( H ) be the arrangement of a set H of n hyperplanes in  d -space. A  k -flat is a  k -dimensional affine subspace of  d -space. The  zone  of a  k -flat f with respect to H is the set of all faces in  A ( H ) that intersect f. this paper we study some problems on zones of  k -flats. Our most important result is a data structure for point location in the zone of a  k -flat. This structure uses   O  (n      ⌊  d  2  ⌋+e    +n     k+e   )   preprocessing time and space and has a query time of O(log 2   n ). We also show how to test efficiently whether two flats are visible from each other with respect to a set of hyperplanes. Then point location in m faces in arrangements is studied. Our data structure for this problem has size   O  (n      ⌊  d  2  ⌋+e    m      ⌈  d  2  ⌉  d    )   and the query time is O(log 2   n ).
Dynamic optimal battery array management in high energy density fuel cell/battery hybrid power source
The goal of this paper is to address the problem of dynamic optimal battery array management to extend the life of battery array used in the hybrid power source. Unlike previous efforts which mainly solve the problem using the off-line optimization, our design addresses the problem using the idea of “feedback”. The proposed approach can handle the possible dynamic uncertainty of the hybrid power source introduced by battery failure or insert of a new battery. The detailed battery model and the converter model are derived to facilitate development of dynamic optimal management algorithm. The other goal of this paper is to call possible attention of the control community in potential contribution for newest smart grid technology.
Expectation-maximization approach to Boolean factor analysis
Methods for hidden structure of high-dimensional binary data discovery are one of the most important challenges facing machine learning community researchers. There are many approaches in literature that try to solve this hitherto rather ill-defined task. In the present study, we propose a most general generative model of binary data for Boolean factor analysis and introduce new Expectation-Maximization Boolean Factor Analysis algorithm which maximizes likelihood of Boolean Factor Analysis solution. Using the so-called bars problem benchmark, we compare efficiencies of Expectation-Maximization Boolean Factor Analysis algorithm with Dendritic Inhibition neural network. Then we discuss advantages and disadvantages of both approaches as regards results quality and methods efficiency.
Bridgeless SEPIC PFC Rectifier With Reduced Components and Conduction Losses
In this paper, a new bridgeless single-ended primary inductance converter power-factor-correction rectifier is introduced. The proposed circuit provides lower conduction losses with reduced components simultaneously. In conventional PFC converters (continuous-conduction-mode boost converter), a voltage loop and a current loop are required for PFC. In the proposed converter, the control circuit is simplified, and no current loop is required while the converter operates in discontinuous conduction mode. Theoretical analysis and simulation results are provided to explain circuit operation. A prototype of the proposed converter is realized, and the results are presented. The measured efficiency shows 1% improvement in comparison to conventional SEPIC rectifier.
Spurious-Free Dynamic Range of a Uniform Quantizer
Quantization plays an important role in many systems where analog-to-digital conversion and/or digital-to-analog conversion take place. If the quantization error is correlated with the input signal, then the spectrum of the quantization error will contain spurious peaks. Although analytical formulas describing this effect exist, numerical evaluation can take much effort. This brief provides approximations for the spurious-free dynamic range (SFDR) of a uniform quantizer with a single sinusoidal input, with and without additive Gaussian noise. It is shown that the SFDR increases by approximately 8 dB/bit, in case there is no noise. Generalizing this result to multitone inputs results in an additional 2 dB/bit per additional tone. Additive Gaussian noise decorrelates the sinusoid(s) and the quantization error, which results in a dramatic increase in SFDR.
Binding time in distributed shared memory architectures
The paper revisits three distributed shared memory (DSM) architectures to clarify them with their binding times for new addresses at the local memory: page fault time, node miss time, and cache miss time. The DSM architectures which have different binding times arrange data in different ways with different overheads at an event of reference. Since a large number of cache misses can occur in a large (relative to the cache size) working set, binding at the page fault time alone cannot efficiently utilize locality of reference at the local memory. In a small working set, most of the addresses bound to the local memory at a node miss time are not effective due to the low cache miss rate. The paper shows that binding at the cache miss time can improve system performance.
Reasoning about Human Intention Change for Individualized Runtime Software Service Evolution
While software evolution has been studied extensively in software engineering, few of these efforts have involved a systematic exploration of human epistemological attitudes, such as human desire and intention, as the driving force of software service evolution. Our work proposes a theoretical framework to monitor and reason about human intention and its changes, which in turn can be used to determine how software and services should evolve to be individualized and better serve each user. Extending the Situ framework, we explore the service satisfiability problem through sub-world coverage following Kripke semantics, which enjoys wide application in AI and other fields related to human epistemic reasoning.
A comparative assessment of Web accessibility and technical standards conformance in four EU states
The Internet is playing a progressively more important part in our day–to–day life, through its power of making information universally available. People with disabilities have particular opportunities to benefit. Using the Internet in conjunction with dedicated assistive technologies, tasks that were very difficult if not impossible to achieve for people with various types of disability can now be made fully accessible — at least, in principle. However, in practice, many online resources and services are still poorly accessible to those with disability due to unsatisfactory Web content design.#N##N#Design of accessible Web content is codified in standards and guidelines of the World Wide Web Consortium (W3C). Conformance with W3C’s Web Content Accessibility Guidelines 1.0 (WCAG) (and/or similar, derivative guidelines) is now the subject of considerable activity, both legal and technical, in many different jurisdictions.#N##N#This paper presents results of a comparative survey of Web accessibility guidelines and HTML standards conformance for samples of Web sites drawn from Ireland, the United Kingdom, France and Germany. It also gives some recommendations on how to improve the accessibility level of Web content.#N##N#A particular conclusion of the study is that the general level of Web accessibility guidelines and HTML standards conformance in all of the samples studied is very poor; and that the pattern of failure is strikingly consistent in the four samples. Although considerable efforts are being made to promote Web accessibility for users with disabilities, this is certainly not yet manifesting itself in improving Web accessibility and HTML validity.
Evolution of a four wheeled active suspension rover with minimal actuation for rough terrain mobility
In this paper we deduce the evolution of a four wheeled active suspension rover from a five wheeled passive suspension rover. The aim of this paper is to design a suspension mechanism which utilizes the advantages of both passive suspension and active suspension rover. Both the design considered here are simpler than the existing suspension mechanisms in the sense that the number of links as wells as the number of joints have been significantly reduced without compromising the climbing capability of the rover. We first analyze the kinematics of the five wheeled rover and its motion pattern while climbing an obstacle and try to deduce the same motion pattern and capability in the four wheeled rover. Both the suspension mechanism consists of two planar closed kinematic chains on each side of the rover. We also deduce the control strategy for the active suspension rover wherein only two actuators are used to control the internal configuration of the rover. To the best of author's knowledge this is the minimum number of actuators required to control the internal configuration of a active suspension while operating on a fully 3D rough terrain. Extensive uneven terrain simulations are performed for both 5-wheeled and 4-wheeled rover and a comparative analysis has been done on maximum coefficient of friction and torque requirements.
Data fusion algorithms for network anomaly detection: classification and evaluation
In this paper, the problem of discovering anomalies in a large-scale network based on the data fusion of heterogeneous monitors is considered. We present a classification of anomaly detection algorithms based on data fusion, and motivated by this classification, the operational principles and characteristics of two different representative approaches, one based on the Demster-Shafer theory of evidence and one based on principal component analysis, are described. The detection effectiveness of these strategies are evaluated and compared under different attack scenarios, based on both real data and simulations. Our study and corresponding numerical results revealed that in principle the conditions under which they operate efficiently are complementary, and therefore could be used effectively in an integrated way to detect a wider range of attacks..
A Method for Real-Time Identification of Malformed BGP Messages
The BGP routing system is one of the key component of today's Internet infrastructure responsible for carrying data traffic across different Autonomous Systems (ASes). Recently, malformed BGP messages have become a threat to the operational community as they repeatedly cause BGP session resets until identified. However, the identification of the message itself is often difficult in large ISP networks. In this paper, we propose a novel method for real-time identification of these messages by using passively collects BGP messages. Our method focuses on the frequency of observed attributes and values of prefixes advertised by each AS. Based on our heuristics that common attributes are observed at similar time scale, we periodically measure the usage frequency of attributes from BGP messages observed in real-time and mark attributes and values used by minority of the AS as suspicious. We verify the efficiency of our method using BGP data obtained from operational networks.
Link Characteristics Measuring in 2.4 GHz Body Area Sensor Networks
With the increasing demands on the remote healthcare and the rich human-machine interacting, body area sensor network (BASN) has been attracting more and more attention. In practice, understanding the link performance and its dynamics in the emerging BASN applications is very important to design reliable, real-time, and energy-efficient protocols. In this paper we study the link characteristics of body area sensor network (BASN) through extensive experiments with very realistic configurations. We evaluate the packet reception ratio, RSSI, LQI, and movement intensity of body under indoor and outdoor environments, all of which can provide direct insights to practical account.
Implicit spatial inference with sparse local features
This paper introduces a novel way to leverage the implicit geometry of sparse local features (e.g. SIFT operator) for the purposes of object detection and segmentation. A two-class Bayesian scheme is used as a framework, and the likelihood is derived from the real-valued classification of machine learning algorithm Gentle AdaBoost, whose output is transformed to a probabilistic distribution using either of two models investigated; Log-Sigmoid or Bi-Gaussian. The main contribution is a novel scheme for the injection of prior contextual spatial information. This occurs on a uniquely designed Markov Random Field defined by Delaunay Tri- angulation of the feature points. Our experiments show that this framework is useful for object detection and segmentation, and we achieve good, mostly invariant results in these tasks.
A geometrical framework for the determination of ambiguous directions in subspace methods
In signal subspace parameter estimation techniques, like MUSIC, degradations may occur due to parasite peaks in the spectrum, which may be connected to high sidelobes in the beam pattern or to ambiguities themselves. This paper studies the presence of ambiguities in an array of given planar geometry. We propose a general framework for the analysis and thus we obtain a generalisation of results published by Lo and Marple (1992) and by Proukakis and Manikas (see Proc. ICASSP'94, vol.4, p.549-52, 1994) for rank one and two ambiguities. For rank k/spl ges/3 ambiguities the study is restricted to linear arrays, for which we derive original and synthetic results. We present a geometrical construction that is able to determine all the ambiguous directions which can appear for a given linear array. The method allows determination of any rank ambiguities and for each ambiguous direction set, the rank of ambiguity is obtained. The search is exhaustive. Application of the method requires no assumption for the linear array and is easy to implement. An example is detailed for a non-uniform linear array.
Nonrigid Intraoperative Cortical Surface Tracking Using Game Theory
During neurosurgery, nonrigid brain deformation prevents preoperatively acquired images from accurately depicting the intraoperative brain. Stereo vision systems can be used to track cortical surface deformation and update preoperative brain images in conjunction with a biomechanical model. However, these stereo systems are often plagued with calibration error, which can corrupt the deformation estimation. In order to decouple the effects of camera calibration and surface deformation, a framework is needed which can solve for disparate and often competing variables. Game theory, which was developed specifically to handle decision making in this type of competitive environment, has been applied to various fields from economics to biology. In this paper, we apply game theory to cortical surface tracking and use it to infer information about the physical processes of brain deformation and image acquisition.
A quality control mechanism for networked virtual reality system with video capability
Introduction of motion video including live video into networked virtual reality systems makes virtual spaces more attractive. To handle live video in networked virtual reality systems based on VRML, the scalability of networked virtual reality systems becomes very important on the Internet where the performance of the network and the end systems varies dynamically. We propose a new quality control mechanism suitable for networked virtual reality systems with live video capability. Our approach is to introduce the notion of the importance of presence (IoP) which represents the importance of objects in virtual spaces. According to IoP, the degree of the deterioration of object presentation will be determined in case of the starvation of system resources.
50th Anniversary Article: The Evolution of Research on Information Systems: A Fiftieth-Year Survey of the Literature in Management Science
The development of the information systems (IS) literature inManagement Science during the past 50 years reflects the inception, growth, and maturation of several different research streams. The five research streams we identify incorporate different definitions of the managerial problems that relate to IS, the alternate theoretical perspectives and different methodological paradigms to study them, and the levels of the organization at which their primary results impact managerial practice. Thedecision support and design science research stream studies the application of computers in decision support, control, and managerial decision making. Thevalue of information research stream reflects relationships established based on economic analysis of information as a commodity in the management of the firm. Thehuman-computer systems design research stream emphasizes the cognitive basis for effective systems design. TheIS organization and strategy research stream focuses the level of analysis on the locus of value of the IS investment instead of on the perceptions of a system or its user. Theeconomics of information systems and technology research stream emphasizes the application of theoretical perspectives and methods from analytical and empirical economics to managerial problems involving IS and information technologies (IT). Based on a discussion of these streams, we evaluate the IS literature's core contributions to theoretical and managerial knowledge, and make some predictions about the road that lies ahead for IS researchers.
Transient Signal Detection with Neural Networks: The Search for the Desired Signal
Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (0/1 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.
Recursive construction and evolution of collaborative business processes
Virtual Enterprises (VEs) bring together expertise and processes of different companies to react to a market opportunity.#N#Here we propose a novel approach to support the collaborative construction and evolution of such VEs and their business processes,#N#comprising a model of the VE, and a set of model construction rules and operators. Our approach is based on the principles#N#of iterative elaboration, devolved decision-making and situatedness, and achieves flexibility by treating the processes of#N#work, coordination and selection in a uniform manner. We argue that certain assumptions behind existing approaches make them#N#unsuitable to the business practices we observed in the target business ecosystem. We then show how the proposed approach#N#can underpin software support for informal business practices of collaborative process construction by manufacturing SMEs.
Workspace of a six-revolute decoupled robot manipulator
In this paper we study the working space of a six-revolute decoupled robot manipulator. A simple and direct method is presented to obtain the boundaries of the total and primary workspace. The technique is based on finding the limit configurations of the general geometry positioning mechanism of the decoupled manipulator. In order to do this we derive a fourth-order displacement equation in the first joint variable. It is shown that the method only requires the simultaneous solution of two second-order nonlinear equations.
The rise and fall of an executive information system: a case study
The progress of an executive information system project within a manufacturing organization over a period of 9 years is described. The case study illustrates the importance of the interaction between the business environment, the organizational environment and the perceptions and interpretations of events and facts by stakeholders on the success or failure of an information system. It shows the importance of context in the development and implementation of an executive information system and the dynamic nature of the influence of social, economic and technical factors. The reasons for the initial success and the subsequent failure of the EIS within the company are explored from a contingency perspective.
Adapting the eBlock Platform for Middle School STEM Projects: Initial Platform Usability Testing
The benefits of project-based learning environments are well documented; however, setting up and maintaining these environments can be challenging due to the high cost and expertise associated with these platforms. To alleviate some of these roadblocks, the existing eBlock platform which is composed of fixed function building blocks targeted to enable nonexperts users to easily build a variety of interactive electronic systems is expanded to incorporate newly defined integer-based building blocks to enable a wider range of project possibilities for middle school STEM projects. We discuss various interface possibilities, including initial usability experiments, and summarize our overall experiences and observations in working with local middles school students utilizing the eBlock platform.
Cooperative Multi-Antenna Relaying in Heterogeneous Networks
In this paper, we investigate the performance of heterogeneous networks with multi-antenna cooperative relays. Specifically, threshold-based maximum ratio combining (MRC) and selection combining (SC) schemes are adopted for decoding at the relays and the end-to-end (E2E) error rate performance is analyzed by assuming a Nakagami channel model. Numerical results show that the deployment of multi-antennas can reduce the number of required relay nodes, and thus significantly reduce the system cost. On the other hand, the selection of optimal decoding threshold depends on the number of relay nodes, number of antennas, as well as the average SNR value at the receiver. It is also demonstrated that when the BER requirement is not high, the SC relaying scheme is sufficient to provide satisfactory performance, such that the complexity of relays can be effectively reduced.
A visual representation for knowledge structures
Knowledge-based systems often represent their knowledge as a network of interrelated units. Such networks are commonly presented to the user as a diagram of nodes connected by lines. These diagrams have provided a powerful visual metaphor for knowledge representation. However, their complexity can easily become unmanageable as the knowledge base (KB) grows.  This paper describes an alternate visual representation for navigating knowledge structures, based on a virtual museum metaphor. This representation uses nested boxes rather than linked nodes to represent relations. The intricate structure of the knowledge base is conveyed by a combination of position, size, color, and font cues, MUE (Museum Unit Editor) was implemented using this representation to provide a graphic front end for the Cyc knowledge base.
Variety Is the Spice of (Virtual) Life
Before an environment can be populated with characters, a set of models must first be acquired and prepared. Sometimes it may be possible for artists to create each virtual character individually - for example, if only a small number of individuals are needed, or there are many artists available to create a larger population of characters. However, for most applications that need large and heterogeneous groups or crowds, more automatic methods of generating large numbers of humans, animals or other characters are needed. Fortunately, depending on the context, it is not the case that all types of variety are equally important. Sometimes quite simple methods for creating variations, which do not over-burden the computing resources available, can be as effective as, and perceptually equivalent to, far more resource-intensive approaches. In this paper, we present some recent research and development efforts that aim to create and evaluate variety for characters, in their bodies, faces, movements, behaviours and sounds.
Finite horizon linear quadratic regulation for linear discrete time-varying systems with single/multiple input delay(s)
This paper studies the linear quadratic regulation problem for linear discrete time-varying systems with one or multiple delays in control input. This type of input-delay system can be used to model delayed actuation, where the system depends on the input after various (could be more than one) time delays. We provide explicit forms of the finite horizon closed-loop optimal control laws. Numerical examples are also provided to show the performance of our derived control laws.
VirusMeter: Preventing Your Cellphone from Spies
Due to the rapid advancement of mobile communication technology, mobile devices nowadays can support a variety of data services that are not traditionally available. With the growing popularity of mobile devices in the last few years, attacks targeting them are also surging. Existing mobile malware detection techniques, which are often borrowed from solutions to Internet malware detection, do not perform as effectively due to the limited computing resources on mobile devices.#R##N##R##N#In this paper, we propose VirusMeter, a novel and general malware detection method, to detect anomalous behaviors on mobile devices. The rationale underlying VirusMeter is the fact that mobile devices are usually battery powered and any malicious activity would inevitably consume some battery power. By monitoring power consumption on a mobile device, VirusMeter catches misbehaviors that lead to abnormal power consumption. For this purpose, VirusMeter relies on a concise user-centric power model that characterizes power consumption of common user behaviors. In a real-time mode, VirusMeter can perform fast malware detection with trivial runtime overhead. When the battery is charging (referred to as a battery-charging mode), VirusMeter applies more sophisticated machine learning techniques to further improve the detection accuracy. To demonstrate its feasibility and effectiveness, we have implemented a VirusMeter prototype on Nokia 5500 Sport and used it to evaluate some real cellphone malware, including FlexiSPY and Cabir. Our experimental results show that VirusMeter can effectively detect these malware activities with less than 1.5% additional power consumption in real time.
Author Profiling for Vietnamese Blogs
This paper presents the first work in the task of author profiling for Vietnamese blogs. This task is important in threat identification and marketing intelligence. We have developed a Vietnamese Blog Profiling framework to automatically predict age, gender, geographic origin and occupation of weblogs’ authors purely based on language use. The experiments on the blogs corpus we collected show very promising results with accuracy of around 80% across all traits.
Integrating heterogeneous personal devices with public display-based information services
Based on a requirements analysis for public location information displays in on-campus settings, we describe the implementation of a system called "SynchroBoard". Especially, we elaborate on mechanisms to integrate different personal devices in this framework.
Performance analysis of wireless fair queuing algorithms with compensation mechanism
Scheduling packet transmission over wireless links requires quantification of the QoS performance such as delay and packet loss in terms of known system parameters. One of the key issues is how to account for effects of the compensation mechanism on the system's QoS performance. In this paper, we develop a model, namely the two-stage tandem queuing (TSTQ), to characterize the behaviors of packet flows in the system, which applies the wireless fair scheduling with the compensation mechanism. Using queueing analysis, we derive performance parameters: average delay and packet loss rate, in closed-form expressions. These expressions are functions of the source, wireless channel and compensation mechanism parameters. Moreover, the trade-off relationship between delay and packet loss rate is revealed, which is controlled by the parameter of lagging bound. Numerical and simulation results are used to verify the validity of the modeling and analysis work.
Channel Assignment for Wireless Networks Modelled as d-Dimensional Square Grids
In this paper, we study the problem of channel assignment for wireless networks modelled as d-dimensional grids. In particular, for d-dimensional square grids, we present optimal assignments that achieve a channel separation of 2 for adjacent stations where the reuse distance is 3 or 4. We also introduce the notion of a colouring schema for d- dimensional square grids, and present an algorithm that assigns colours to the vertices of the grid satisfying the schema constraints.
Efficient Implementation of the Overlap Operator on Multi-GPUs
Lattice QCD calculations were one of the first applications to show the potential of GPUs in the area of high performance computing. Our interest is to find ways to effectively use GPUs for lattice calculations using the overlap operator. The large memory footprint of these codes requires the use of multiple GPUs in parallel. In this paper we show the methods we used to implement this operator efficiently. We run our codes both on a GPU cluster and a CPU cluster with similar interconnects. We find that to match performance the CPU cluster requires 20-30 times more CPU cores than GPUs.
Flat vs. symbiotic evolutionary subspace clusterings
Subspace clustering coevolves the attribute space supporting clusters at the same time as parameterizing the cluster location and combination. Typically, a 'flat' representation is pursued in which individuals describe both the property of individual clusters as well as the combination of clusters used to define the overall solution; hereafter F-ESC. Conversely, a symbiotic approach was recently proposed in which candidate clusters and the combination of clusters are coevolved from independent populations; hereafter S-ESC. In this work a common framework is pursued in order for flat and symbiotic evolutionary subspace clustering to be compared directly. We show that F-ESC might match S-ESC results for data sets with high proportions of cluster support, however, the gap between the two algorithm increases as cluster support decreases.
Context-Dependent Force-Feedback Steering Wheel to Enhance Drivers' On-Road Performances
In this paper the topic of the augmented cognition applied to the driving task, and specifically to the steering maneuver, is discussed. We analyze how the presence of haptic feedback on the steering wheel could help drivers to perform a visually-guided task by providing relevant information like vehicle speed and trajectory. Starting from these considerations, a Context-Dependant Steering Wheel force feedback (CDSW) had been developed, able to provide to the driver the most suitable feeling of the vehicle dynamics according to the driven context. With a driving simulator the CSWD software had been tested twice and then compared with a traditional steering wheel.
Direct segmentation of smooth, multiple point regions
The purpose of reverse engineering is to convert a large point cloud into an accurate, fair and consistent CAD model. For a class of conventional engineering objects we have the 'a priori' assumption that the object is bounded exclusively by simple, analytic surfaces. In this case it is possible to generate the model with a minimal amount of user interaction. The key issue is segmentation, i.e., to separate the point cloud into smaller regions, where each can be approximated by a single surface. While this is relatively simple, where the regions are bounded by sharp edges, problems arise when smoothly connected regions need to be separated. The direct segmentation method described in this paper is based on a special sequence of tests, by means of which a large point cloud can be robustly splitted into smaller subregions until no further subdivision is possible. Surfaces of linear extrusion and revolution are also detected. The structure of the smooth, multiple regions is the basis of constrained surface fitting in the final model building phase.
Research on Dynamic Reputation Management Model Based on PageRank
For the purpose of developing a usable trust relationship between the resource providers (hosts) and the resource consumers (users) in an open computing environment and providing a unified management of the reputation degree of the resource provides and users, a dynamic reputation management model based on Google PageRank (DRMPR) is proposed. The DRMPR system can achieve self-study from a large amount of data and feedback, and with the system obtaining a plenty of resources, the judgment is more accurate. At the end of the paper, an experimental project has been built to demonstrate that the DRMPR can provide a unified management of the reputation degree of the resource provides and users accurately.
Parameterization of the MISO IFC rate region: the case of partial channel state information
We study the achievable rate region of the multiple-input single-output (MISO) interference channel (IFC), under the assumption that all receivers treat the interference as additive Gaussian noise. We assume the case of two users, and that the channel state information (CSI) is only partially known at the transmitters. Our main result is a characterization of Pareto-optimal transmit strategies, for channel matrices that satisfy a certain technical condition. Numerical examples are provided to illustrate the theoretical results.
Introducing a rasch-type anthropomorphism scale
In human-robot interaction research, much attention is given to the extent to which people perceive humanlike attributes in robots. Generally, the concept anthropomorphism is used to describe this process. Anthropomorphism is defined in different ways, with much focus on either typical human attributes or uniquely human attributes. This difference has caused different measurement tools to be developed. We argue that anthropomorphism can best be described as a continuum ranging from low to high human likeness, and should be measured accordingly. We found that anthropomorphic characteristics can be invariantly ordered according to the ease with which these can be ascribed to robots.
Knowledge formation and dialogue using the KRAKEN toolset
The KRAKEN toolset is a comprehensive interface for knowledge acquisition that operates in conjunction with the Cyc knowledge base. The KRAKEN system is designed to allow subject-matter experts to make meaningful additions to an existing knowledge base, without the benefit of training in the areas of artificial intelligence, ontology development, or logical representation. Users interact with KRAKEN via a natural-language interface, which translates back and forth between English and the KB's logical representation language. A variety of specialized tools are available to guide users through the process of creating new concepts, stating facts about those concepts, and querying the knowledge base. KRAKEN has undergone two independent performance evaluations. In this paper we describe the general structure and several of the features of KRAKEN, focussing on key aspects of its functionality in light of the specific knowledge-formation and acquisition challenges they are intended to address.
APTEEN: A Hybrid Protocol for Efficient Routing and Comprehensive Information Retrieval in Wireless Sensor Networks
Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increasing deployment in coming years, as they enable reliable monitoring and analysis of the environment. In this paper, we propose a hybrid routing protocol (APTEEN) which allows for comprehensive information retrieval. The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic intervals in a very energy efficient manner. Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and persistent queries respectively. We evaluated the performance of these protocols and observe that these protocols are observed to outperform existing protocols in terms of energy consumption and longevity of the network.
Motion parameter estimation of multiple ground moving targets in multi-static passive radar systems
Multi-static passive radar (MPR) systems typically use narrowband signals and operate under weak signal conditions, making them difficult to reliably estimate motion parameters of ground moving targets. On the other hand, the availability of multiple spatially separated illuminators of opportunity provides a means to achieve multi-static diversity and overall signal enhancement. In this paper, we consider the problem of estimating motion parameters, including velocity and acceleration, of multiple closely located ground moving targets in a typical MPR platform with focus on weak signal conditions, where traditional time-frequency analysis-based methods become unreliable or infeasible. The underlying problem is reformulated as a sparse signal reconstruction problem in a discretized parameter search space. While the different bistatic links have distinct Doppler signatures, they share the same set of motion parameters of the ground moving targets. Therefore, such motion parameters act as a common sparse support to enable the exploitation of group sparsity-based methods for robust motion parameter estimation. This provides a means of combining signal energy from all available illuminators of opportunity and, thereby, obtaining a reliable estimation even when each individual signal is weak. Because the maximum likelihood (ML) estimation of motion parameters involves a multi-dimensional search and its performance is sensitive to target position errors, we also propose a technique that decouples the target motion parameters, yielding a two-step process that sequentially estimates the acceleration and velocity vectors with a reduced dimensionality of the parameter search space. We compare the performance of the sequential method against the ML estimation with the consideration of imperfect knowledge of the initial target positions. The Cramer-Rao bound (CRB) of the underlying parameter estimation problem is derived for a general multiple-target scenario in an MPR system. Simulation results are provided to compare the performance of the sparse signal reconstruction-based methods against the traditional time-frequency-based methods as well as the CRB.
Topology Preserving Marching Cubes-like Algorithms on the Face-Centered Cubic Grid
The well-known marching cubes algorithm is modified to apply to the face-centered cubic (fee) grid. Thus, the local configurations that are considered when extracting the local surface patches are not cubic anymore. This paper presents three different partitionings of the fee grid to be used for the local configurations. The three candidates are evaluated theoretically and experimentally and compared with the original marching cubes algorithm. It is proved that the reconstructed surface is topologically equivalent to the surface of the original object when the surface of the original object that is digitized is smooth and a sufficiently dense fee grid is used.
An on-line signature verification system based on fusion of local and global information
An on-line signature verification system exploiting both local and global information through decision-level fusion is presented. Global information is extracted with a feature-based representation and recognized by using Parzen Windows Classifiers. Local information is extracted as time functions of various dynamic properties and recognized by using Hidden Markov Models. Experimental results are given on the large MCYT signature database (330 signers, 16500 signatures) for random and skilled forgeries. Feature selection experiments based on feature ranking are carried out. It is shown experimentally that the machine expert based on local information outperforms the system based on global analysis when enough training data is available. Conversely, it is found that global analysis is more appropriate in the case of small training set size. The two proposed systems are also shown to give complementary recognition information which is successfully exploited using decision-level score fusion.
Tumor-Immune Interaction, Surgical Treatment, and Cancer Recurrence in a Mathematical Model of Melanoma
Malignant melanoma is a cancer of the skin arising in the melanocytes. We present a mathematical model of melanoma invasion into healthy tissue with an immune response. We use this model as a framework with which to investigate primary tumor invasion and treatment by surgical excision. We observe that the presence of immune cells can destroy tumors, hold them to minimal expansion, or, through the production of angiogenic factors, induce tumorigenic expansion. We also find that the tumor–immune system dynamic is critically important in determining the likelihood and extent of tumor regrowth following resection. We find that small metastatic lesions distal to the primary tumor mass can be held to a minimal size via the immune interaction with the larger primary tumor. Numerical experiments further suggest that metastatic disease is optimally suppressed by immune activation when the primary tumor is moderately, rather than minimally, metastatic. Furthermore, satellite lesions can become aggressively tumorigenic upon removal of the primary tumor and its associated immune tissue. This can lead to recurrence where total cancer mass increases more quickly than in primary tumor invasion, representing a clinically more dangerous disease state. These results are in line with clinical case studies involving resection of a primary melanoma followed by recurrence in local metastases.
Metric rectification for perspective images of planes
We describe the geometry constraints and algorithmic implementation for metric rectification of planes. The rectification allows metric properties, such as angles and length ratios, to be measured on the world plane from a perspective image. The novel contributions are: first, that in a stratified context the various forms of providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane-this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements.
Improved Pairwise Key Establishment for Wireless Sensor Networks
Due to the resource constraints, pre-distributing secret keys into sensor nodes before they are deployed is an applicable approach to achieve information security in wireless sensor networks. Several key pre-distribution schemes have been proposed in literature to establish pairwise keys between sensor nodes; they are either too complicated, or insecure for some common attacks. To address these weaknesses, we propose an improved pairwise key establishment mechanism for wireless sensor networks in this paper. Compared with existing approaches, our scheme has better network resilience against node capture attack. Analysis and simulation results show that our scheme performs better than earlier proposed schemes in terms of network connectivity, key storage overhead, maximum supported network size, computational and communication overheads
Applications of Plotkin-terms: partitions and morphisms for closed terms
This theoretical pearl is about the closed term model of pure untyped lambda-terms modulo β-convertibility. A consequence of one of the results is that for arbitrary distinct combinators (closed lambda terms) M, M′, N, N′ there is a combinator H such thatdisplay formula hereThe general result, which comes from Statman (1998), is that uniformly r.e. partitions of the combinators, such that each ‘block’ is closed under β-conversion, are of the form {H−1{M}}M∈ΛΦ. This is proved by making use of the idea behind the so-called Plotkin-terms, originally devised to exhibit some global but non-uniform applicative behaviour. For expository reasons we present the proof below. The following consequences are derived: a characterization of morphisms and a counter-example to the perpendicular lines lemma for β-conversion.
Cost/Revenue Optimisation of Multi-Service Cellular Planning for City Centre E-UMTS
An overview of all-IP enhanced Universal Mobile Telecommunication System, E-UMTS, service needs in the business city centre, BCC, scenario is first presented. Then, E-UMTS traffic generation and activity models are described and characterised. System level simulations are carried out and the enhanced performance is demonstrated based in a single quality parameter, which simultaneously accounts for call blocking and handover failure probabilities. End-to-end delays do not present a limitation. By considering a grade of service of 1% for the quality parameter, and different hypothesis for costs and prices, an optimum coverage distance is obtained around ~200-425 m, which maximises the supported throughput per km 2 . However, results for the profit in percentage indicates that coverage distances in the range 395-425 m should be used in BCC.
Implementing fault-tolerance in real-time systems by automatic program transformations
We present a formal approach to implement and certify fault-tolerance in real-time embedded systems. The fault-intolerant initial system consists of a set of independent periodic tasks scheduled onto a set of fail-silent processors. We transform the tasks such that, assuming the availability of an additional spare processor, the system tolerates one failure at a time (transient or permanent). Failure detection is implemented using heartbeating, and failure masking using checkpointing and roll-back. These techniques are described and implemented by automatic program transformations on the tasks' programs. The proposed formal approach to fault-tolerance by program transformation highlights the benefits of separation of concerns and allows us to establish correctness properties.
Using literature-based discovery to identify disease candidate genes
Summary  We present BITOLA, an interactive literature-based biomedical discovery support system. The goal of this system is to discover new, potentially meaningful relations between a given starting concept of interest and other concepts, by mining the bibliographic database MEDLINE ® . To make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations, we integrate background knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as LocusLink and Human Genome Organization (HUGO). BITOLA can also be used as an alternative way of searching the MEDLINE database. The system is available at http://www.mf.uni-lj.si/bitola/.
Modeling nondisclosure in terms of the subject-instruction stream
A formal definition is given of nondisclosure for a computing system and the author describes a functional decomposition of the system into two kinds of activities, namely, the selection and execution of subject instructions. Security requirements for each of the two resulting subsystems are given, and it is proved that, if each subsystem satisfies its security requirements, then the entire system satisfies the given nondisclosure property. Finally, in order to show how security can be enforced by the system, an access-control model is given for subject-instruction processing that guarantees satisfaction of the given security requirements for subject-instruction processing. >
Galois theory and a new homotopy double groupoid of a map of spaces
The authors have used generalised Galois Theory to construct a homotopy double groupoid of a surjective Þbration of Kan simplicial sets. Here we apply this to construct a new homotopy double groupoid of a map of spaces, which includes constructions by others of a 2-groupoid, cat 1 -group or crossed module. An advantage of our construction is that the double groupoid can give an algebraic model of a foliated bundle. 1
Lane-Change Decision Aid System Based on Motion-Driven Vehicle Tracking
Overtaking and lane changing are very dangerous driving maneuvers due to possible driver distraction and blind spots. We propose an aid system based on image processing to help the driver in these situations. The main purpose of an overtaking monitoring system is to segment the rear view and track the overtaking vehicle. We address this task with an optic-flow-driven scheme, focusing on the visual field in the side mirror by placing a camera on top of it. When driving a car, the ego-motion optic-flow pattern is very regular, i.e., all the static objects (such as trees, buildings on the roadside, or landmarks) move backwards. An overtaking vehicle, on the other hand, generates an optic-flow pattern in the opposite direction, i.e., moving forward toward the vehicle. This well-structured motion scenario facilitates the segmentation of regular motion patterns that correspond to the overtaking vehicle. Our approach is based on two main processing stages: First, the computation of optical flow in real time uses a customized digital signal processor (DSP) particularly designed for this task and, second, the tracking stage itself, based on motion pattern analysis, which we address using a standard processor. We present a validation benchmark scheme to evaluate the viability and robustness of the system using a set of overtaking vehicle sequences to determine a reliable vehicle-detection distance.
Inquire: predicate-based use and reuse
There are four fundamental aspects of use and reuse in building systems from components: conceptualization, retrieval, selection and correct use. The most important barrier to use and reuse is that of conceptualization. The Inscape environment is a specification-based software development environment integrated by the constructive use of formal interface specifications. The purpose of the formal interface specifications and the semantic interconnections is to make explicit the invisible semantic dependencies that result in conventionally-built systems. The important ingredient provided by Inquire in conceptualization, retrieval, selection and use is the set of predicates that describe the semantics of the elements in the interface. These predicates define the abstractions that are germane to the module interface and describe the properties of data objects and the assumptions and results of operations in a module. Use and reuse of components is based on a component's ability to provide needed semantics at a particular point in a system. It is the purpose of Inquire, the browser and predicate-based search mechanism, to aid both the environment and the user in the search for the components that will provide the desired predicates that are required to build and evolve an implementation correctly. >
Least squares detection of multiple changes in fractional ARIMA processes
We address the problem of estimating changes in fractional integrated ARMA (FARIMA) processes. These changes may be in the long range dependence (LRD) parameter or the ARMA parameters. The signal is divided into "elementary" segments: the objective is then to estimate the segments in which the changes occur. This estimation is achieved by minimizing a penalized least-squares criterion based on the parameter estimates computed in each segment. The optimization problem is then solved using a dynamic programming algorithm. Simulation results on synthetic data (computer network traffic) are reported.
Shape from point features
We present a nonparametric and efficient method for shape localization that improves on the traditional sub-window search in capturing the fine geometry of an object from a small number of feature points. Our method implies that the discrete set of features capture more appearance and shape information than is commonly exploited. We use the a-complex by Edelsbrunner et al. to build a filtration of simplicial complexes from a user-provided set of features. The optimal value of a is determined automatically by a search for the densest complex connected component, resulting in a parameter-free algorithm. Given K features, localization occurs in O(K logK) time. For VGA-resolution images, computation takes typically less than 10 milliseconds. We use our method for interactive object cut, with promising results.
Formation control for cooperative containment of a diffusing substance
We present a decentralized controller to keep a group of agents at equal spacing while moving around the perimeter of a loop defined by a constant distance from a convex polygon, motivated by a cooperative containment problem. Traveling at constant speed, the agents achieve and maintain their formation by using small steering adjustments to equalize the distance between themselves and their respective leading and following neighbors. Since the formation moves around a common loop, an agent can move forward or back in formation by respectively steering slighting inside or outside the reference loop. These adjustments are controlled with the use of variable radius parameters for each agent that are shown to converge to the desired reference loop as equal spacing is achieved. We show that the proposed controller renders the desired formation locally asymptotically stable and provide simulations to demonstrate the performance of the controller for an example scenario in which the formation must recover from the loss of an agent.
A Consensus Support System Model for Group Decision-Making Problems With Multigranular Linguistic Preference Relations
The group decision-making framework with linguistic preference relations is studied. In this context, we assume that there exist several experts who may have different background and knowledge to solve a particular problem and, therefore, different linguistic term sets (multigranular linguistic information) could be used to express their opinions. The aim of this paper is to present a model of consensus support system to assist the experts in all phases of the consensus reaching process of group decision-making problems with multigranular linguistic preference relations. This consensus support system model is based on i) a multigranular linguistic methodology, ii) two consensus criteria, consensus degrees and proximity measures, and iii) a guidance advice system. The multigranular linguistic methodology permits the unification of the different linguistic domains to facilitate the calculus of consensus degrees and proximity measures on the basis of experts' opinions. The consensus degrees assess the agreement amongst all the experts' opinions, while the proximity measures are used to find out how far the individual opinions are from the group opinion. The guidance advice system integrated in the consensus support system model acts as a feedback mechanism, and it is based on a set of advice rules to help the experts change their opinions and to find out which direction that change should follow in order to obtain the highest degree of consensus possible. There are two main advantages provided by this model of consensus support system. Firstly, its ability to cope with group decision-making problems with multigranular linguistic preference relations, and, secondly, the figure of the moderator, traditionally presents in the consensus reaching process, is replaced by the guidance advice system, and in such a way, the whole group decision-making process is automated
An Aspect-Oriented Approach to Resource Composition in Petri net-based Software Architectural Models
Petri net has been widely used for modeling software systems due to its mathematical soundness and support of various tools. In many cases, performance-related analyses of Petri net for a software system need to consider the resource limitations caused by a specific platform. In terms of aspect-oriented approach, the interference of various resources scattered across a software system can be regarded as a specific concern that is only necessary during its performance-related analysis process. To capture the interactions of resources within the Petri net-based software architectural model, we propose an aspect-oriented resource composition model and the XML-based representation language called the resource extension markup language (ReML) for its description. In our approach, one or more resource composition models can be developed and described in ReML separately from the development of base software architectural models. Through the weaving process, a resource composition model can be applied to extend several Petri net-based software architectural models. The resource weaver generates an augmented Petri net used for analyzing performance characteristics of the extended software architectural model with resource interactions. Our aspect-oriented approach facilitates selecting an optimal or superior resource composition model for a software architectural model, and vice versa, which is illustrated using two exemplary server models.
Building sub-knowledge bases using concept lattices
A theory of concept (Galois) lattices was first introduced by Wille. An extension of his work to simple structures called concept sublattices has also been published. This paper shows that concept sublattices can be applied to (i) determining subsumption of specifications and (ii) decomposing specifications in terms of others. I show that the latter application of the theory may provide us with new conceptualizations of a specification.
Coordination strategies for networked control systems: A power system application
In this paper we present a distributed supervisory strategy for load/frequency control problems in networked multi-area power systems. Coordination between the control center and the areas is accomplished via data networks subject to communication latency which is modelled by time-varying time-delay. The aim here is at finding strategies able of reconfiguring, whenever necessary in response to unexpected load changes and/or faults, the nominal set-points on frequency and generated power of each area so that viable evolutions arise for the overall networked system and a new suitable equilibrium is reached.
Cross-talk attack monitoring and localization in all-optical networks
The effects of an attack connection can propagate quickly to different parts of an all-optical transparent network. Such attacks affect the normal traffic and can either cause service degradation or outright service denial. Quick detection and localization of an attack source can avoid losing large amounts of data in an all-optical network. Attack monitors can collect the information from connections and nodes for diagnostic purpose. However, to detect attack sources, it is not necessary to put monitors at all nodes. Since those connections affected by the attack connection would provide valuable information for diagnosis, we show that by placing a relatively small number of monitors on a selected set of nodes in a network is sufficient to achieve the required level of performance. However, the actual monitor placement, routing, and attack diagnosis are challenging problems that need research attention. In this paper, we first develop our models of crosstalk attack and monitor node. With these models, we prove the necessary and sufficient condition for one-crosstalk-attack diagnosable networks. Next, we develop a scalable diagnosis method which can localize the attack connection efficiently with sparse monitor nodes in the network.
Adaptive algorithms for balanced multidimensional clustering
The G-K-D tree (generalized K-D tree) method aims at reducing the average number of data page accesses per query, but it ignores the cost of index search. The authors propose two adaptive algorithms that take into consideration both data page access cost and index page access cost. It attempts to find a minimum total cost. Experimental results indicate that the proposed algorithms are superior to the G-K-D tree method. >
An optimized stereo vision implementation for embedded systems: application to RGB and Infra-Red images
The aim of this paper is to demonstrate the applicability and the effectiveness of a computationally demanding stereo-matching algorithm in different low-cost and low-complexity embedded devices, by focusing on the analysis of timing and image quality performances. Various optimizations have been implemented to allow its deployment on specific hardware architectures while decreasing memory and processing time requirements: (1) reduction of color channel information and resolution for input images; (2) low-level software optimizations such as parallel computation, replacement of function calls or loop unrolling; (3) reduction of redundant data structures and internal data representation. The feasibility of a stereo vision system on a low-cost platform is evaluated by using standard datasets and images taken from infra-red cameras. Analysis of the resulting disparity map accuracy with respect to a full-size dataset is performed as well as the testing of suboptimal solutions.
Parametric Mixing for Centralized VOIP Conferencing using ITU-T Recommendation G.722.2
VoIP conferencing with a centralized speech mixing bridge introduces additional end-to-end latency into packetized voice communication. This paper investigates how full tandem speech decoding, time-domain mixing, speech encoding cycle can be circumvented by instead extracting the coded speech parameters and performing the speech packet mixing without time-domain reconstruction. By mixing through coded speech parameters, we show that nearly an 85% decrease in computational complexity can be achieved over full tandem mixing of two speakers for G.722.2, thus significantly reducing the packet latency at the centralized speech mixing bridge. For the G.722.2 parametric mixer presented, linear prediction coefficients (LPCs), pitch lags, fixed codebooks, and gains, are extracted (without full speech reconstruction) from the encoded bit stream, mixed, and then re-encoded instead of the full tandem approach where each speech frame must be fully reconstructed. We investigate the mixing in two scenarios: i) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 12.65 kbps, and ii) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 18.25 kbps. PAMS is used to evaluate the speech quality of the parametric mixer, resulting in an average distortion of 0.37 MOS (compared to tandem mixing) as shown by simulations using typical conversation models.
Self-adaptive, on-line reclustering of complex object data
A likely trend in the development of future CAD, CASE and office information systems will be the use of object-oriented database systems to manage their internal data stores. The entities that these applications will retrieve, such as electronic parts and their connections or customer service records, are typically large complex objects composed of many interconnected heterogeneous objects, not thousands of tuples. These applications may exhibit widely shifting usage patterns due to their interactive mode of operation. Such a class of applications would demand clustering methods that are appropriate for clustering large complex objects and that can adapt on-line to the shifting usage patterns. While most object-oriented clustering methods allow grouping of heterogeneous objects, they  are usually static and can only be changed off-line. We present one possible architecture for performing complex object reclustering in an on-line manner that is adaptive to changing usage patterns. Our architecture involves the decomposition of a clustering method into concurrently operating components that each handle one of the fundamental tasks involved in reclustering, namely statistics collection, cluster analysis, and reorganization. We present the results of an experiment performed to evaluate its behavior. These results show that the average miss rate for object accesses can be effectively reduced using a combination of rules that we have developed for deciding when cluster analyses and reorganizations should be performed.
Reim & ReImInfer: checking and inference of reference immutability and method purity
Reference immutability ensures that a reference is not used to modify the referenced object, and enables the safe sharing of object structures. A pure method does not cause side-effects on the objects that existed in the pre-state of the method execution. Checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations. We present ReIm, a type system for reference immutability, and ReImInfer, a corresponding type inference analysis. The type system is concise and context-sensitive. The type inference analysis is precise and scalable, and requires no manual annotations. In addition, we present a novel application of the reference immutability type system: method purity inference.   To support our theoretical results, we implemented the type system and the type inference analysis for Java. We include a type checker to verify the correctness of the inference result. Empirical results on Java applications and libraries of up to 348kLOC show that our approach achieves both scalability and precision.
MODEL-FREE CONTROL AND INTELLIGENT PID CONTROLLERS: TOWARDS A POSSIBLE TRIVIALIZATION OF NONLINEAR CONTROL?
We are introducing a model-free control and a control with a restricted model for finite-dimensional complex systems. This control design may be viewed as a contribution to ``intelligent'' PID controllers, the tuning of which becomes quite straightforward, even with highly nonlinear and/or time-varying systems. Our main tool is a newly developed numerical differentiation. Differential algebra provides the theoretical framework. Our approach is validated by several numerical experiments.
Modified Differential Evolution for Constrained Optimization
In this paper, we present a Differential-Evolution based approach to solve constrained optimization problems. The aim of the approach is to increase the probability of each parent to generate a better offspring. This is done by allowing each solution to generate more than one offspring but using a different mutation operator which combines information of the best solution in the population and also information of the current parent to find new search directions. Three selection criteria based on feasibility are used to deal with the constraints of the problem and also a diversity mechanism is added to maintain infeasible solutions located in promising areas of the search space. The approach is tested in a set of test problems proposed for the special session on Constrained Real Parameter Optimization. The results obtained are discussed and some conclusions are established.
Performance Evaluation of Piggyback Requests in IEEE 802.16
WiMAX (Worldwide Interoperability for Microwave Access) is a wireless access technology that aims to provide last mile wireless broadband access for fixed and mobile users as an alternative to the wired DSL and cable access. It is specified in the IEEE 802.16 standard. The standard defines several possible bandwidth request methods that can be implemented in an actual deployment of a WiMAX network. In this paper, we will study the performance of two different bandwidth request mechanisms, namely piggyback and broadcast requests and will show in which situations piggybacking performs better than the contention based broadcast bandwidth requests.
Use of coded infrared light as artificial landmarks for mobile robot localization
This paper presents mobile robot localization using coded infrared light as artificial landmarks. Different from RFID, identification using infrared light has highly deterministic characteristics. IRID(infrared identification) is implemented with IR LEDs and photo transistors. By putting several infrared LEDs on the ceiling, the floor is divided into several sectors and each sector is set to have a unique identification. The coded infrared light tells which sector the robot is in, but the size of the uncertainty is still too large if the sector size is large, which usually occur. Dead-reckoning provides the estimated robot configuration but the error is getting accumulated as the robot travels. This paper presents an algorithm which fuses both the encoder and the IRID information so that the size of the uncertainty becomes smaller. It also introduces a framework which can be used with other types of the artificial landmarks. The characteristics of the developed IRID and the proposed algorithm are verified from the experiments.
The growth of international collaboration in East European scholarly communities: a bibliometric analysis of journal articles published between 1989 and 2009
In the last two decades international collaboration in the Eastern European academic communities has strongly intensified. Scientists from developed countries within the European Union play a key role in stimulating the international collaboration of academics in this region. In addition, many of the research projects that engage East-European scholars are only possible in the framework of the large European programmes. The present study focuses on the role of EU and other developed nations as a partner of these countries and the analysis of the performance of collaborative research as reflected by the citation impact of internationally co-authored publications.
INEXACT KLEINMAN-NEWTON METHOD FOR RICCATI EQUATIONS
In this paper we consider the numerical solution of the algebraic Riccati equation using Newton's method. We propose an inexact variant which allows one control the number of the inner iterates used in an iterative solver for each Newton step. Conditions are given under which the monotonicity and global convergence result of Kleinman also hold for the inexact Newton iterates. Numerical results illustrate the efficiency of this method.
Dimensional synthesis of a 3-DOF parallel manipulator
Kinematics analysis and dimensional synthesis are two important problems of a parallel manipulator. Dimensional synthesis is optimization of the kinematic parameters according to desired workspace or other design requirements. In this paper, the dimensional synthesis of a 3-DOF parallel manipulator, which mimics DELTA robots, is studied considering maximum inscribed workspace and reciprocal of the condition number on the workspace section based on the concept. The kinematic model of the 3-DOF parallel manipulator is given at first. The golden section search is used to search the workspace of the manipulator and mesh the boundary. Then the algorithm for calculating the inscribed workspace and dimensional synthesis considering the inscribed workspace are presented. Thirdly, the distribution of dexterity index on the workspace section and dimensional synthesis considering the reciprocal of condition number is studied. The two dimensional synthesis results are compared at the end of the paper. While the synthesis methodology of the manipulator is studied. It is helpful to improve design efficiency of the 3-DOF parallel manipulator. The quickly design for the manipulator can be performed through the proposed methods of dimensional synthesis.
Kinematics analysis for obstacle-climbing performance of a rescue robot
A tracked robot is designed for destroyed mine search and rescue. The mechanical system is introduced from reconfigurable structure, suspension system and anti- explosive and waterproof. The sensors include CCD camera, CO, CH4, temperature and air speed are equipped on the robot. Two pairs of swing arms are equipped on the robot. Their motions help robot climb up obstacle. Because the center of gravity (CG) plays an important role in the process of climbing up an obstacle, the CG kinematics model is built. Using this model, the CG change situation is obtained, and the maximum height of the obstacle which can be climbed up is obtained, and the stability angle margin is obtained too. The relationship between the robot pitch angle and the height of the obstacle is obtained. Using this relationship, the geometry parameter of the uncertain environment can be known. These analysis help to design and control the robot.
Dynamic software updates: a VM-centric approach
Software evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of J volve , a DSU-enhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, just-in-time compilation, scheduling, return barriers, on-stack replacement, and garbage collection. J volve , is  safe : its use of bytecode verification and VM thread synchronization ensures that an update will always produce type-correct executions. Jvolve is  flexible : it can support 20 of 22 updates to three open-source programs--Jetty web server, JavaEmailServer, and CrossFTP server--based on actual releases occurring over 1 to 2 years. Jvolve is  efficient : performance experiments show that incurs  no overhead  during steady-state execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.
Creating Rule Ensembles from Automatically-Evolved Rule Induction Algorithms
Ensembles are a set of classification models that, when combined, produce better predictions than when used by themselves. This chapter proposes a new evolutionary algorithm-based method for creating an ensemble of rule sets consisting of two stages. First, an evolutionary algorithm (more precisely, a genetic programming algorithm) is used to automatically create complete rule induction algorithms. Secondly, the automatically-evolved rule induction algo- rithms are used to produce rule sets that are then combined into an ensemble. Concerning this second stage, we investigate the effectiveness of two different approaches for combining the votes of all rule sets in the ensemble and two dif- ferent approaches for selecting which subset of evolved rule induction algorithms (out of all evolved algorithms) should be used to produce the rule sets that will be combined into an ensemble.
XML and Meta Data Based EDI for Small Enterprises
Today in many cases electronic data interchange (EDI) is limited to large scale industry connected to their own value added networks. Small-scale enterprises are not yet integrated in the communication flow, because actual EDI solutions are to complex, to inflexible or to expensive.#R##N##R##N#The approach presented in this paper separates knowledge about data structures and data formats from the process of generation of destination files. This knowledge is transformed into a meta data structure represented by XML document type definitions (DTD) which itself are stored within a database system. If any changes of the data interchange specification are necessary, it is sufficient to update the corresponding meta data information within the XML DTDs. The implementation of the data interchange processor remains unchanged. This type of adaptation does not require a software specialist and therefore it meets an important requirement of small-scale enterprises. Data transmission is done using the advantages of XML and Internet technology.
An FPGA-Based Embedded System for Fingerprint Matching Using Phase-Only Correlation Algorithm
Biometric identification systems are defined as systems exploiting automated methods of personal recognition based on physiological or behavioural characteristics. Among these, fingerprints are very reliable biometric identifiers. Trying to fasten the image processing step makes the recognition process more efficient, especially concerning embedded systems for real-time authentication. In this paper we propose an FPGA-based architecture that efficiently implements the high computationally demanding core of a matching algorithm based on phase-only spatial correlation. Moreover, we show how it is possible to use COTS components to embed an entire AFIS on chip and so reducing cost, space and energy used.
Classification of breast masses in mammograms using neural networks with shape, edge sharpness, and texture features
We propose an approach using artificial neural networks to classify masses in mammograms as malignant or benign. Single- layer and multilayer perceptron networks are used in a study on perceptron topologies and training procedures for pattern classifica- tion of breast masses. The contours of a set of 111 regions on mam- mograms related to breast masses and tumors are manually delin- eated and represented by polygonal models for shape analysis. Ribbons of pixels are extracted around the boundaries of a subset of 57 masses by dilating and eroding the contours. Three shape fac- tors, three measures of edge sharpness, and 14 texture features based on gray-level co-occurrence matrices of the pixels in the rib- bons are computed. Several combinations of the features are used with perceptrons of varying topology and training procedures for the classification of benign masses and malignant tumors. The results are compared in terms of the area Az under the receiver operating characteristics curve. Values of Az up to 0.99 are obtained with the shape factors and texture features. However, only feature sets that included at least one shape factor provide consistently high perfor- mance with respect to variations in network topology and training.
The analysis of the performance of multi-beamforming in memory nonlinear power amplifier
With the increasingly diverse and complex requirements of radar systems and communication systems, the application of multifunction-phased array radar has become a trend, and the digital multi-beamforming technology plays a crucial role in it. In practice, power amplifier (PA) is an essential component in radar systems and communication systems. Unfortunately, it is always nonlinear to provide a high output power. With the purpose of a high output power and efficiency, it is necessary to study the influence of PA nonlinear characteristics on the digital multi-beamforming. In this paper, a form of the multi-beamforming signal and a nonlinear model with memory for PA are given. The output signal via the PA model has been analyzed subsequently. As the result of analysis, it can be found that the output signal is divided into the original signal and the interferential signal. The power ratio of original signal to interference signal can reflect the influence of PA nonlinear characteristics on the digital multi-beamforming. Finally, according to the ratio, the results of computer simulation show that the memory effect plays a key role for the small power signal, while the nonlinearity plays an important role for the large power signal.
Relaxed multiple routing configurations for IP fast reroute
Multi-topology routing is an increasingly popular IP network management concept that allows transport of different traffic types over disjoint network paths. The concept is of particular interest for implementation of IP fast reroute (IP FRR). First, it can support guaranteed, instantaneous recovery from any link or node failure. Second, different failures result in routing over different network topologies, which augments the parameter space for load distribution optimizations. Multiple routing configurations (MRC) is the state-of-the-art IP FRR scheme based on multi-topology routing today. In this paper we present a new, enhanced IP FRR scheme which we call ldquorelaxed MRCrdquo (rMRC). rMRC simplifies the topology construction and increases the routing flexibility in each topology. According to our experimental evaluation, rMRC has several benefits compared to MRC. The number of backup topologies required to provide protection against the same set of failures is reduced, hence reducing state in routers. In addition, the backup paths are shorter, and the link utilization is significantly better.
Similarity measures between intuitionistic fuzzy (vague) sets: A comparative analysis
Existing similarity measures between intuitionistic fuzzy sets/vague sets are analyzed, compared and summarized by their counter-intuitive examples in pattern recognition. The positive aspects of each similarity measure are demonstrated, along with counter cases and discussion of the conditions under which each may not work as desired. The research presented here could benefit selection and applications of similarity measures for intuitionistic fuzzy sets and vague sets in practice.
Distributed approximation of capacitated dominating sets
We study local, distributed algorithms for the capacitated minimum dominating set (CapMDS) problem, which arises in various distributed network applications. Given a network graph  G  = ( V,E ), and a capacity  cap(v)  ∈  N  for each node  v  ∈  V  , the CapMDS problem asks for a subset  S  ⊆  V  of minimal cardinality, such that every network node not in  S  is covered by at least one neighbor in  S , and every node  v  ∈  S  covers at most  cap(v)  of its neighbors. We prove that in general graphs and even with uniform capacities, the problem is inherently  non-local , i.e., every distributed algorithm achieving a non-trivial approximation ratio must have a time complexity that essentially grows linearly with the network diameter. On the other hand, if for some parameter e > 0, capacities can be violated by a factor of 1 + e, CapMDS becomes much more local. Particularly, based on a novel distributed randomized rounding technique, we present a distributed bi-criteria algorithm that achieves an O(log Δ)-approximation in time O(log 3  n  + log( n )/e), where  n  and Δ denote the number of nodes and the maximal degree in  G , respectively. Finally, we prove that in geometric network graphs typically arising in wireless settings, the uniform problem can be approximated within a constant factor in logarithmic time, whereas the non-uniform problem remains entirely non-local.
Using wordnet hypernyms and dependency features for phrasal-level event recognition and type classification
The goal of this research is to devise a method for recognizing and classifying TimeML events in a more effective way. TimeML is the most recent annotation scheme for processing the event and temporal expressions in natural language processing fields. In this paper, we argue and demonstrate that unit feature dependency information and deep-level WordNet hypernyms are useful for event recognition and type classification. The proposed method utilizes various features including lexical semantic and dependency-based combined features. The experimental results show that our proposed method outperforms a state-of-the-art approach, mainly due to the new strategies. Especially, the performance of noun and adjective events, which have been largely ignored and yet significant, is significantly improved.
Three-dimensional focusing with multipass SAR data
Deals with the use of multipass synthetic aperture radar (SAR) data in order to achieve three-dimensional tomography reconstruction in presence of volumetric scattering. Starting from azimuth- and range-focused SAR data relative to the same area, neglecting any mutual interaction between the targets, and assuming the propagation in homogeneous media, we investigate the possibility to focus the data also in the elevation direction. The problem is formulated in the framework of linear inverse problem and the solution makes use of the singular value decomposition of the relevant operator. This allows us to properly take into account nonuniform orbit separation and to exploit a priori knowledge regarding the size of the volume interested by the scattering mechanism, thus leading to superresolution in the elevation direction. Results obtained on simulated data demonstrate the feasibility of the proposed processing technique.
Software component independence
Independence is a fundamental requirement for calculating system reliability from component reliabilities, whether in hardware or software systems. Markov analysis is often used in such calculation; however, procedures as conventionally used do not qualify as nodes in a Markov system. We outline the requirements for several classes of component independence and use the CPS (continuation passing style) transformation to convert conventional procedures into fragments that are appropriate to Markov analysis.
Interpolated Allpass Fractional-Delay Filters Using Root Displacement
Fractional-delay filter is the general name given to filters modelling non-integer delays. Such filters have a flat phase delay for a wide frequency band, with the value of the phase delay approximating the fractional delay. A maximally-flat delay IIR fractional-delay filter can be obtained by the Thiran approximation. A simple and efficient method for obtaining filters modelling intermediate fractional delays from two Thiran fractional-delay filters is proposed. The proposed method allows continuously modifying the fractional delay. Computational complexity of the proposed method is discussed. A practical application of the method in model-based sound synthesis is given as an example.
MMsINC: a large-scale chemoinformatics database
MMsINC (http://mms.dsfarm.unipd.it/MMsINC/search) is a database of non-redundant, richly annotated and biomedically relevant chemical structures. A primary goal of MMsINC is to guarantee the highest quality and the uniqueness of each entry. MMsINC then adds value to these entries by including the analysis of crucial chemical properties, such as ionization and tautomerization processes, and the in silico prediction of 24 important molecular properties in the biochemical profile of each structure. MMsINC is consequently a natural input for different chemoinformatics and virtual screening applications. In addition, MMsINC supports various types of queries, including substructure queries and the novel ‘molecular scissoring’ query. MMsINC is interfaced with other primary data collectors, such as PubChem, Protein Data Bank (PDB), the Food and Drug Administration database of approved drugs and ZINC.
IBM Solves a Mixed-Integer Program to Optimize Its Semiconductor Supply Chain
IBM Systems and Technology Group uses operations research models and methods extensively for solving large-scale supply chain optimization (SCO) problems for planning its extended enterprise semiconductor supply chain. The large-scale nature of these problems necessitates the use of computationally efficient solution methods. However, the complexity of the models makes developing robust solution methods a challenge. We developed a mixed-integer programming (MIP) model and supporting heuristics for optimizing IBM's semiconductor supply chain. We designed three heuristics, driven by practical applications, for capturing the discrete aspects of the MIP. We leverage the model structure to overcome computational hurdles resulting from the large-scale problem. IBM uses the model and method daily for operational and strategic planning decisions and has saved substantial costs.
On passivity based control of stochastic port-Hamiltonian systems
This paper introduces Stochastic Port-Hamiltonian Systems (SPHS's), whose dynamics are described by Ito stochastic differential equations. SPHS's are extension of the deterministic port-Hamiltonian systems which are used to express various passive systems. First, we show a necessary and sufficient condition to preserve the stochastic port-Hamiltonian structure of the system under a class of coordinate transformations. Second, we derive a condition for the system to be stochastic passive. Third, we equip Stochastic Generalized Canonical Transformations (SGCT's), which are pairs of coordinate and feedback transformations preserving the stochastic port-Hamiltonian structure. Finally, we propose a stochastic stabilization framework based on stochastic passivity and SGCT's.
A survey of single and multi-hop link schedulers for mmWave wireless systems
Wireless communication at 60GHz, aka mmWave, provides extremely high data rates, i.e., several Gb/s. Moreover, devices have a much shorter transmission range as compared to those operating in the 2.4 and 5GHz bands. Indeed, links can be treated as pseudo-wires with minimal interference leakage. As a result, future 60GHz systems will have very high spatial reuse. This, however, is at the expense of high propagation loss, which can be overcome using directional or smart antennas. Another promising solution is to employ relays to boost the signal of weak links. In particular, if relays are properly selected, they are able to offer higher data rates than direct links, and also help circumvent obstacles. To this end, we review state-of-the-art schedulers that take advantage of the high spatial re-use afforded by 60GHz wireless systems to activate multiple links within a channel time allocation. Moreover, we survey works that use passive and active relays to overcome obstacles and to facilitate novel applications. We also survey those that maximize both spatial reuse and throughput of both direct and indirect (relay) links simultaneously.
A data-driven approach for building macroeconomic decision support system
More and more economic and financial data have been collected by the governmental departments in China since China started its “socialist market economy” in late 1980s. These government departments, in particular the departments in charge of economic development, pay a great attention to the economic information in their decision-making. There is an urgent demand for efficient decision support systems in macroeconomic decision-making. In this paper we present a data-driven approach for building macroeconomic decision support system. We first give a comprehensive discussion about the basic elements and their data-processing methods for Macroeconomic Decision Support Systems according to China's situation. These elements include: leading indicator system about business cycle, state identification of economic movement, forecasting of economic trend, promotion of successful cases, choice of regulation instruments, evaluation method of macroeconomic policies. Based on the discussion, we put forward to a general structure of macroeconomic decision support system. Premier implementation shows that the structure can not only satisfy the governmental departments' demand fairly, but also reflect the future trend.
TCP Performance in IEEE 802.11-Based Ad Hoc Networks with Multiple Wireless Lossy Links
We propose a packet-level model to investigate the impact of channel error on the transmission control protocol (TCP) performance over IEEE-802.11-based multihop wireless networks. A Markov renewal approach is used to analyze the behavior of TCP Reno and TCP Impatient NewReno. Compared to previous work, our main contributions are listed as follows: 1) modeling multiple lossy links, 2) investigating the interactions among TCP, Internet Protocol (IP), and media access control (MAC) protocol layers, specifically the impact of 802.11 MAC protocol and dynamic source routing (DSR) protocol on TCP throughput performance, 3) considering the spatial reuse property of the wireless channel, the model takes into account the different proportions between the interference range and transmission range, and 4) adopting more accurate and realistic analysis to the fast recovery process and showing the dependency of throughput and the risk of experiencing successive fast retransmits and timeouts on the packet error probability. The analytical results are validated against simulation results by using GloMoSim. The results show that the impact of the channel error is reduced significantly due to the packet retransmissions on a per-hop basis and a small bandwidth delay product of ad hoc networks. The TCP throughput always deteriorates less than ~ 10 percent, with a packet error rate ranging from 0 to 0.1. Our model also provides a theoretical basis for designing an optimum long retry limit for IEEE 802.11 in ad hoc networks.
Toward Secure Multikeyword Top-k Retrieval over Encrypted Cloud Data
Cloud computing has emerging as a promising pattern for data outsourcing and high-quality data services. However, concerns of sensitive information on cloud potentially causes privacy problems. Data encryption protects data security to some extent, but at the cost of compromised efficiency. Searchable symmetric encryption (SSE) allows retrieval of encrypted data over cloud. In this paper, we focus on addressing data privacy issues using SSE. For the first time, we formulate the privacy issue from the aspect of similarity relevance and scheme robustness. We observe that server-side ranking based on order-preserving encryption (OPE) inevitably leaks data privacy. To eliminate the leakage, we propose a two-round searchable encryption (TRSE) scheme that supports top-k multikeyword retrieval. In TRSE, we employ a vector space model and homomorphic encryption. The vector space model helps to provide sufficient search accuracy, and the homomorphic encryption enables users to involve in the ranking while the majority of computing work is done on the server side by operations only on ciphertext. As a result, information leakage can be eliminated and data security is ensured. Thorough security and performance analysis show that the proposed scheme guarantees high security and practical efficiency.
Improved 16-QAM constellation labeling for BI-STCM-ID with the Alamouti scheme
We design constellation labeling maps for bit-interleaved space-time coded modulation with iterative decoding (BI-STCM-ID) over Rayleigh block-fading channels using the Alamouti scheme and N/sub r/ receive antennas. To achieve the largest asymptotic coding gain from the constellation labeling, we propose a new design criterion that maximizes the (-2N/sub r/)-th power mean of the squared Euclidean distances associated with all "error-free feedback" events in the constellation. Based on this power mean criterion, we show that the labeling optimization problem falls into the category of quadratic assignment problems. We propose two novel 16-QAM labeling maps that are particularly designed for N/sub r/=1 and N/sub r/=2, respectively. Numerical results show that both labeling maps achieve about 1 dB coding gain over the conventional 16-QAM modified set partitioning labeling.
Perfect Output Feedback in the Two-User Decentralized Interference Channel
In this paper, the    $\eta $   -Nash equilibrium (   $\eta $   -NE) region of the two-user Gaussian interference channel (IC) with perfect output feedback is approximated to within 1 bit/s/Hz and    $\eta $    arbitrarily close to 1 bit/s/Hz. The relevance of the    $\eta $   -NE region is that it provides the set of rate pairs that are achievable and stable in the IC when both transmitter–receiver pairs autonomously tune their own transmit–receive configurations seeking an    $\eta $   -optimal individual transmission rate. Therefore, any rate tuple outside the    $\eta $   -NE region is not stable as there always exists one link able to increase by at least    $\eta $    bits/s/Hz its own transmission rate by updating its own transmit–receive configuration. The main insights that arise from this paper are as follows. First, the    $\eta $   -NE region achieved with feedback is larger than or equal to the    $\eta $   -NE region without feedback. More importantly, for each rate pair achievable at an    $\eta $   -NE without feedback, there exists at least one rate pair achievable at an    $\eta $   -NE with feedback that is weakly Pareto superior. Second, there always exists an    $\eta $   -NE transmit–receive configuration that achieves a rate pair that is at most 1 bit/s/Hz per user away from the outer bound of the capacity region.
A Design and Control Environment for Internet-Based Telerobotics
This paper describes an environment for the design, simulation, and control of Internet-based force-reflecting telerobotic systems. We define these systems as using a segment of the computer network to connect the master to the slave. Computer networks introduce a time delay that is best described by a time-varying random process. Thus, known techniques for controlling time-delay telerobots are not directly applicable, and an environment for iterative designing and testing is necessary. The underlying software architecture sup ports tools for modeling the delay of the computer network, design ing a stable controller, simulating the performance of a telerobotic system, and testing the control algorithms using a force-reflecting input device. Furthermore, this setup provides data about including the Internet into more general telerobotic control architectures. To demonstrate the features of this environment, the complete proce dure for the design of a telerobotic controller is discussed. First, the delay pa...
No-flow underfill flip chip assembly--an experimental and modeling analysis
In the flip-chip assembly process, no-flow underfill materials have a particular advantage over traditional underfill: the application and curing of the former can be undertaken before and during the reflow process. This advantage can be exploited to increase the flip-chip manufacturing throughput. However, adopting a no-flow underfill process may introduce reliability issues such as underfill entrapment, delamination at interfaces between underfill and other materials, and lower solder joint fatigue life. This paper presents an analysis on the assembly and the reliability of flip-chips with no-flow underfill. The methodology adopted in the work is a combination of experimental and computer-modeling methods. Two types of no-flow underfill materials have been used for the flip chips. The samples have been inspected with X-ray and scanning acoustic microscope inspection systems to find voids and other defects. Eleven samples for each type of underfill material have been subjected to thermal shock test and the number of cycles to failure for these flip chips have been found. In the computer modeling part of the work, a comprehensive parametric study has provided details on the relationship between the material properties and reliability, and on how underfill entrapment may affect the thermal–mechanical fatigue life of flip chips with no-flow underfill.
Reflections on designing networked exertion games
Research in human-computer interaction has begun to acknowledge the benefits of physicality in the way people interact with computers. However, the role of physicality is often understood in terms of the characteristics of physical smart objects and their digital augmentation. We are stressing that the physicality lies within the interaction, not the object, and use a subset of bodily actions, exertion interactions, as an example to demonstrate our point. Emerging game designs have shown that supporting such exertion interactions can enable beneficial experiences between geographically distant participants. Based on several designs from our own work as well as others in this area we articulate reflections for the design of systems that support and facilitate bodily aspects of physicality in networked environments. We believe our work can serve as guidance for designers who are interested in creating future systems that support networked exertion interactions.
Stack-free process-oriented simulation
The process interaction world view is widely used in the general simulation community for its expressive power, and is supported by most modern simulation languages. In parallel discrete event simulation, however, its use remains comparatively rare due to the perceived inefficiency (and difficulty) of parallel implementations.We present a new implementation strategy for parallel process-oriented simulation languages. This innovative, semantics-based approach directly addresses two common concerns of such languages. By concentrating on the intrinsic threads of control, we avoid the proliferation of simulation objects (and their associated costs) that might result from a naive translation. More fundamentally, the primary costs associated with process-oriented languages -- those of context switching between stacks and, in an optimistic setting, of saving the state of these stacks -- are entirely eliminated since our explicit use of continuations avoids the need for stacks in the first place. We similarly obtain cheap and natural thread preemption.
Network congestion control with Markovian multipath routing
In this paper we consider an integrated model for TCP/IP protocols with multipath routing. The model combines a Network Utility Maximization for rate control based on end-to-end queuing delays, with a Markovian Traffic Equilibrium for routing based on total expected delays. We prove the existence of a unique equilibrium state which is characterized as the solution of an unconstrained strictly convex program. A distributed algorithm for solving this optimization problem is proposed, with a brief discussion of how it can be implemented by adapting the current Internet protocols.
Motion compensated lossy-to-lossless compression of 4-D medical images using integer wavelet transforms
This paper proposes a method for progressive lossy-to-lossless compression of four-dimensional (4-D) medical images (sequences of volumetric images over time) by using a combination of three-dimensional (3-D) integer wavelet transform (IWT) and 3-D motion compensation. A 3-D extension of the set-partitioning in hierarchical trees (SPIHT) algorithm is employed for coding the wavelet coefficients. To effectively exploit the redundancy between consecutive 3-D images, the concepts of key and residual frames from video coding is used. A fast 3-D cube matching algorithm is employed to do motion estimation. The key and the residual volumes are then coded using 3-D IWT and the modified 3-D SPIHT. The experimental results presented in this paper show that our proposed compression scheme achieves better lossy and lossless compression performance on 4-D medical images when compared with JPEG-2000 and volumetric compression based on 3-D SPIHT.
A multiuser receiver for code division multiple access communications over multipath channels
A multiuser communication system is considered where K users share a channel with multipath propagation by using code division for multiple access. Data modulation is carried out by binary phase shift keying and direct sequence spread spectrum signaling. The micro-cellular communication media is modeled as a frequency selective fading channel with multipath propagation. The multipath diversity of the received signals from the K users is exploited by a bank of K RAKE correlators. Algorithms based on the maximum likelihood rule have been developed for estimating the complex channel coefficients as well as for detection of the desired data packets from the sufficient statistics provided by the RAKE correlators. The performance of the resulting multiuser detector is evaluated analytically and via Monte Carlo simulations. The results indicate that the estimator of the channel coefficients has a variance close to the Cramer-Rao lower bound, and that the proposed multiuser detector is capable of eliminating the near-far effect as well as processing the signals propagated through multiple paths. >
Spatial Synchronization Using Watermark Key Structure
Recently, we proposed a method for constructing a template for efficient temporal synchronization in video watermarking. 1 Our temporal synchronization method uses a state machine key generator for producing the watermark embedded in successive frames of video. A feature extractor allows the watermark key schedule to be content dependent, increasing the difficulty of copy and ownership attacks. It was shown that efficient synchronization can be achieved by adding temporal redundancy into the key schedule. In this paper, we explore and extend the concepts of our temporal synchronization method to spatial synchronization. The key generator is used to construct the embedded watermark of non-overlapping blocks of the video, creating a tiled structure. 2–4 The autocorrelation of the tiled watermark contains local maxima or peaks with a grid-like structure, where the distance between the peaks indicates the scale of the watermark and the orientation of the peaks indicate the watermark rotation. Experimental results are obtained using digital image watermarks. Scaling and rotation attacks are investigated.
SDG model-based analysis of fault propagation in control systems
In the area of fault analysis, SDG (Signed Directed Graph) models can be used to describe the system states and the fault propagation paths which are the composition of qualitative deviation from the normal state. In control systems, besides the natural relations caused by the physical properties, the forced control actions determine the dynamic properties of the systems, which cause the particularity of SDG model-based analysis. In this paper, the SDG description and the analysis methods of fault propagation in control systems are presented, and the typical cases like PID control, feedforward control, split-range control, cascade control etc are illustrated. A graphical analysis method is proposed to substitute the algebraic methods based on equations. These results can be expanded to various control systems and even be applied to large-scale industrial systems by the combination and connection of several basic elements.
The distribution of citations from nation to nation on a field by field basis — A computer calculation of the parameters
Following the methodology established byPrice, this paper analyzes the empirical evidence of citation matrices. Using the data cleaned and tabulated by Computer Horizons, Inc. from the Science Citation Index data banks, it is shown that the non-diagonal elements of the square citation matrices can be accounted for very satisfactorily by assigning each nation a characteristic output and input coefficient in each field measured; the ratio of these coefficients provides a measure of quality. Deviations from this simple model give measures of particular linkage strengths between nations showing some evidence of preferences and avoidances that exist for reason of language, social structure, etc. It is also shown that the diagonal data can be accounted for by the measurable phenomenon that each nation seems to publish partly for the international knowledge system and party for its own domestic purposes. Thus, three parameters and a cluster map can parsimoniously describe the citation data within the limits of random error.
An approach to object-oriented discrete-event simulation of manufacturing systems
It is shown how the object-oriented approach can be applied to discrete-event simulation and, in particular, to discrete-event simulation of manufacturing systems. A hierarchical structure of object classes is proposed, consisting of three class libraries: base classes, simulation support object classes, and manufacturing systems simulation object classes. The definition of each class and how the class objects interact with one another are discussed. An example of a discrete-event simulation model developed using the object classes is presented. The example illustrates the basic nature, merits, and drawbacks of this approach. >
Free subcarrier optimization for peak-to-average power ratio minimization in OFDM systems
Peak-to-average power ratio (PAR) reduction techniques are often employed to increase the power efficiency of orthogonal frequency division multiplexing (OFDM) systems. A recently proposed PAR optimization method demonstrates how the PAR can be minimized when free subcarriers and a certain distortion allowance on the error vector magnitude (EVM) are available. In this paper, we derive the lower bound on the capacity for such a system and investigate the capacity- maximizing number of free subcarriers that should be used.
A spatial mapping algorithm for heterogeneous coarse-grained reconfigurable architectures
In this work, we investigate the problem of automatically mapping applications onto a coarse-grained reconfigurable architecture and propose an efficient algorithm to solve the problem. We formalize the mapping problem and show that it is NP-complete. To solve the problem within a reasonable amount of time, we divide it into three subproblems: covering, partitioning and layout. Our empirical results demonstrate that our technique produces nearly as good performance as hand-optimized outputs for many kernels.
Market equilibrium via a primal--dual algorithm for a convex program
We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal--dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.
Enhancement of an Optical Fiber Sensor: Source Separation Based on Brillouin Spectrum
Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolution.
Intra dynamic scenario relations and a dynamic decision making algorithm
In this paper we present the composition of a general dynamic scenario, the relations of its components and a dynamic making algorithm called ECA (Evaluation and Correction Algorithm). The paper proposes three relations, namely, stable relation, controllable relation and uncontrollable relation. Numerical results are given by some simple dynamic scenarios.
PGMRA: a web server for (phenotype × genotype) many-to-many relation analysis in GWAS
It has been proposed that single nucleotide polymorphisms (SNPs) discovered by genome-wide association studies (GWAS) account for only a small fraction of the genetic variation of complex traits in human population. The remaining unexplained variance or missing heritability is thought to be due to marginal effects of many loci with small effects and has eluded attempts to identify its sources. Combination of different studies appears to resolve in part this problem. However, neither individual GWAS nor meta-analytic combinations thereof are helpful for disclosing which genetic variants contribute to explain a particular phenotype. Here, we propose that most of the missing heritability is latent in the GWAS data, which conceals intermediate phenotypes. To uncover such latent information, we propose the PGMRA server that introduces phenomics—the full set of phenotype features of an individual—to identify SNP-set structures in a broader sense, i.e. causally cohesive genotype‐ phenotype relations. These relations are agnostically identified (without considering disease status of the subjects) and organized in an interpretable fashion. Then, by incorporating a posteriori the subject status within each relation, we can establish the risk surface of a disease in an unbiased mode. This approach complements—instead of replaces— current analysis methods. The server is publically available at http://phop.ugr.es/fenogeno.
Parallel Lanczos bidiagonalization for total least squares filter in robot navigation
In the robot navigation problem, noisy sensor data must be filtered to obtain the best estimate of the robot position. The discrete Kalman filter, which usually is used for prediction and detection of signals in communication and control problems has become a commonly used method to reduce the effect of uncertainty from the sensor data. However, due to the special domain of robot navigation, the Kalman approach is very limited. The use of total least squares filter has been proposed (Boley and Sutherland, 1993) which is capable of converging with many fewer readings and achieving greater accuracy than the classical Kalman filter. The main disadvantage of those approaches is that they can not deal with the case where the noise subspace is of dimension higher than one. Here a parallel Krylov subspace method on parallel distributed memory computers which uses the Lanczos bidiagonalization process with updating techniques is proposed which is more computationally attractive to solve the total least squares problems. The parallel algorithm is derived such that all inner products of a single iteration step are independent. Therefore, the cost of global communication which represents the bottleneck of the parallel performance on parallel distributed memory computers can be significantly reduced. This filter is very promising for very large data information and from our very preliminary experiments we can obtain more precise accuracy and better speedup.
Bit and power loading for the multiband impulse radio UWB architecture
In this paper we present two different bit and power loading algorithms for the non-coherent multiband impulse UWB architecture. The first one is a very simple threshold based bit loading algorithm and an extension of the detect and avoid (DAA) algorithm presented in [1]. The second one is a more powerful algorithm, enabling also power loading. These algorithms allow a more efficient and flexible spectrum use, higher data rates and use less transmission power. The bit error rate performance is significantly improved. Both algorithms support inherent powerful DAA.
Intelligent decision system and its application in business innovation self assessment
In this paper, it is described how a multiple criteria decision analysis software tool, the Intelligent Decision System (IDS), can be used to help business self-assessment. Following a brief outline of a model for assessing business innovation capability and the IDS software, the process of using IDS to implement different types of assessment questions is discussed. It is demonstrated that IDS is a flexible tool capable of handling different types of data in self-assessment, including uncertain and incomplete data, and providing a wide range of information including scores, performance diversity, strength and weakness profile and graphics.
Cold delay defect screening
Delay defects can escape detection during the normal production test flow; particularly if they do not affect any of the long paths included in the test flow. Some delay defects can have their delay increased, making them easier to detect, by carrying out the test with a very low supply voltage (VLV testing). However, VLV testing is not effective for delay defects caused by high resistance interconnects. This paper presents a screening technique for such defects. This technique, cold testing, relies on carrying out the test at low temperature. One particular type of defect, silicide open, is analyzed and experimental data are presented to demonstrate the effectiveness of cold testing.
Knowledge representation for conceptual simulation modeling
Simulation is a powerful tool that helps decision makers in business and industry to solve difficult and complex problems, reduce cost, improve quality and productivity, and shorten time-to-market. However the technology is still underutilized in many applications due to several reasons. In this study we address these issues using a knowledge engineering approach, i.e. develop efficient and robust models and formats to capture, represent and organize the knowledge for developing conceptual simulation models that can be generalized and interfaced with different applications and implementation tools. The research fits into a larger project effort that aims to create a sustained research program on knowledge-based simulation.
An application-level implementation of causal timestamps and causal ordering
Maintenance of causality information in distributed systems has previously been implemented in the communications infrastructure with the focus on providing reliability and availability for distributed services. While this approach has a number of advantages, moving causality information up into the view and control of the application programmer is useful, and in some cases, preferable. In an experiment at the University of Queensland, libraries to support application-level maintenance of causality information have been implemented. The libraries allow the collection and use of causality information under programmer control, supplying a basis for making causal dependency information available for application management and troubleshooting. The libraries are also unique in supporting existing distributed systems based on the remote procedure call paradigm. This paper describes the underlying theory of causality, and the design and implementation of the libraries. An event reporting service example is used to motivate the approach, and a number of previously unresolved practical problems are addressed in the design process.
A high-quality multirate real-time CELP coder
The design and implementation of a real-time CELP coder for mobile communication applications are discussed. To realize a single-chip implementation, several tradeoffs were made without compromising speech quality. In addition, techniques that make the coder more robust under a variety of channel conditions are discussed. The real-time coder can be operated at different bit rates (8, 6.8, 4.6 kb/s) by simply changing the frame update rates. The speech quality was evaluated through a formal listening test, and it was found that this coder compares favorably with other (standardized) coders operating at similar or higher rates. >
Weighted least squares training of support vector classifiers leading to compact and adaptive schemes
An iterative block training method for support vector classifiers (SVCs) based on weighted least squares (WLS) optimization is presented. The algorithm, which minimizes structural risk in the primal space, is applicable to both linear and nonlinear machines. In some nonlinear cases, it is necessary to previously find a projection of data onto an intermediate-dimensional space by means of either principal component analysis or clustering techniques. The proposed approach yields very compact machines, the complexity reduction with respect to the SVC solution is especially notable in problems with highly overlapped classes. Furthermore, the formulation in terms of WLS minimization makes the development of adaptive SVCs straightforward, opening up new fields of application for this type of model, mainly online processing of large amounts of (static/stationary) data, as well as online update in nonstationary scenarios (adaptive solutions). The performance of this new type of algorithm is analyzed by means of several simulations.
A distributed parallel approach for BGP routing table partitioning in next generation routers
The rapid growth of routing tables represents a major challenge facing the scalability of BGP and indeed the whole Internet infrastructure. In this paper, we introduce a novel distributed algorithmic scheme for partitioning the BGP routing table on multiple controller cards, where we exploit parallelism to enhance both the lookup speed and the scalability of the RIB (Routing Information Base). The proposed scheme increases the lookup performance by letting unrelated tasks, such as the Best Match Prefix (BMP) lookup and the BGP decision process to be executed in parallel at different controller cards. Simulations show that our proposal outperforms classical central lookup mechanisms with a reasonably acceptable cost, while it increases considerably the space scalability of the BGP routing table.
Fast committee machines for regression and classification
In many data mining applications we are given a set of training examples and asked to construct a regression machine or a classifier that has low prediction error or low error rate on new examples, respectively. An important issue is speed especially when there are large amounts of data. We show how both classification and prediction error can be reduced by using boosting techniques to implement committee machines. In our implementation of committees using either classification trees or regression trees, we show how we can trade off speed against either error rate or prediction error.
Maximum-likelihood detection of nonlinearly distorted multicarrier symbols by iterative decoding
This paper proposes a new method for decoding multicarrier symbols with severe nonlinear distortion. The first part evaluates mutual information expressions for practical nonlinear models and shows the performance bounds for commonly used receiver structures. Then, we derive the maximum-likelihood (ML) sequence estimator, which unfortunately has an exponential complexity due to the nonlinear distortion. This extremely large complexity can be reduced with a simple algorithm that iteratively estimates the nonlinear distortion, thereby reducing the exponential ML to the standard ML without nonlinear distortion. The proposed method can be used to reduce the peak-to-average power ratio of multicarrier signals by clipping the transmit sequence. It can also be used to correct any nonlinear distortion present in transmitter/receiver amplifiers that are operating close to saturation.
Pricing Longevity Bonds Based on Stochastic Mortality Forecasting by Panel Data Procedures
In order to hedge the longevity risk, longevity bonds are designed, whose payoff structure depends on the changes in mortality. To forecast the mortality more precisely, we use a time-dynamic stochastic model by utilizing a panel data approach to forecast the mortality rates and get a survival index. Empirical study is conducted with the data in China. Then we apply these forecasting mortality rates to evaluate one kind of longevity bond. It turns out that it is reliable for the social security systems and the life insurance industry.
Partitions and Edge Colourings of Multigraphs
Erdős and Lovasz conjectured in 1968 that for every graph $G$ with $\chi(G)>\omega(G)$ and any two integers $s,t\geq 2$ with $s+t=\chi(G)+1$, there is a partition $(S,T)$ of the vertex set $V(G)$ such that $\chi(G[S])\geq s$ and $\chi(G[T])\geq t$. Except for a few cases, this conjecture is still unsolved. In this note we prove the conjecture for line graphs of multigraphs.
Robust Adaptive Fuzzy Sliding Mode Control for a Class of Uncertain Nonlinear Systems with Unknown Dead-Zone
In this paper, a robust adaptive fuzzy sliding mode control scheme is presented for a class of uncertain nonlinear systems preceded by an unknown dead-zone. Dead-zone characteristics are quite commonly encountered in actuators, such as hydraulic and pneumatic valves, electric servomotors, and electronic circuits, etc. Therefore, by using a description of a dead-zone and exploring the properties of this dead-zone model intuitively and mathematically, a robust adaptive fuzzy sliding control method is presented without constructing the dead-zone inverse. The unknown nonlinear functions of the plant are approximated by the fuzzy logic system according to some adaptive laws. Based on Lyapunov stability theorem and the theory of variable structure control, the proposed robust adaptive fuzzy sliding mode control scheme can guarantee the robust stability of the whole closed-loop system with an unknown dead-zone in the actuator and obtain good tracking performance as well. Finally, an example and simulation results are provided to illustrate the effectiveness of the proposed method.
Tracer kinetic modeling by morales-smith hypothesis in hepatic perfusion CT
Most of the existing tracer kinetic models for dynamic contrast-enhanced CT or MRI do not fully describe the principles of intra- and transcapillary transport of tracers. One point is to disregard the concentration profiles between the inlets and outlets of capillaries, which may cause a biased estimation of tissue parameters by a systematic error. The Morales-Smith hypothesis enables one to resolve this ambiguity by assuming that the difference between arterial and venous concentrations is proportional to the difference between the arterial and capillary concentrations. If the backflow of administered tracer into the plasma compartment is negligible compared to its outflow into the interstitial compartment during the initial enhancement phase after tracer administration, the capillary concentration can be considered to fall exponentially along the capillary from the arterial concentration to the venous concentration by the Renkin-Crone model, i.e., unidirectional extraction fraction, which can be incorporated in the concept of the Morales-Smith hypothesis. In this study, we reformed the mass-balance equations and mathematical solutions of several representative and well-known tracer kinetic models so that the Morales-Smith hypothesis could be incorporated into their compartment tracer kinetics, considering a tissue-specific factor independent of time as proposed by Brix et al. [5]. The tissue-specific factor was applied to a liver tumor case study in perfusion CT to illustrate the potential effectiveness of the Morales-Smith hypothesis. The proposed scheme was shown to be potentially useful for more consistent and reliable estimation of physiologic tissue parameters.
Algorithm for decomposing an analytic signal into AM and positive FM components
An analytic signal permits unambiguous characterization of the phase and envelope of a real signal. But the analytic signal's phase-derivative, i.e. the instantaneous frequency (IF) is typically a wild function and can take on values ranging from negative infinity to positive infinity. Fortunately, any analytic signal can be decomposed into a minimum phase (MinP) signal component and an all-phase (AllP) signal component. While the MinP signal's log-envelope and its phase form a Hilbert transform pair, the AllP signal has a positive definite instantaneous frequency (PIF) unlike that of the original analytic signal. We propose an elegant computational algorithm that separates the MinP and AllP components of the analytic signal. The envelope of the MinP component corresponds to the AM and the PIF of the AllP component corresponds to the positive FM.
Processing nested complex sequence pattern queries over event streams
Complex event processing (CEP) has become increasingly important for tracking and monitoring applications ranging from health care, supply chain management to surveillance. These monitoring applications submit complex event queries to track sequences of events that match a given pattern. As these systems mature the need for increasingly complex nested sequence queries arises, while the state-of-the-art CEP systems mostly focus on the execution of flat sequence queries only. In this paper, we now introduce an iterative execution strategy for nested CEP queries composed of sequence, negation, AND and OR operators. Lastly we have introduced the promising direction of applying selective caching of intermediate results to optimize the execution. Our experimental study using real-world stock trades evaluates the performance of our proposed iterative execution strategy for different query types.
Learning Top-Down Grouping of Compositional Hierarchies for Recognition
The complexity of real world image categorization and scene analysis requires compositional strategies for object representation. This contribution establishes a compositional hierarchy by first performing a perceptual bottom-up grouping of edge pixels to generate salient contour curves. A subsequent recursive top-down grouping yields a hierarchy of compositions. All entities in the compositional hierarchy are incorporated in a Bayesian network that couples them together by means of a shape model. The probabilistic model underlying top-down grouping as well as the shape model is learned automatically from a set of training images for the given categories. As a consequence, compositionality simplifies the learning of complex category models by building them from simple, frequently used compositions. The architecture is evaluated on the highly challenging Caltech 101 database1 which exhibits large intra-category variations. The proposed compositional approach shows competitive retrieval rates in the range of 53 .0 ± 0 .49%.
On the energy-efficiency of speculative hardware
Microprocessor trends are moving towards wider architectures and more aggressive speculation. With the increasing transistor budgets, energy consumption has become a critical design constraint. To address this problem, several researchers have proposed and evaluated energy-efficient variants of speculation mechanisms. However, such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor, for example, due to wrong-path executions, is ignored. Moreover, the available metrics that would provide a thorough evaluation of an architectural optimization employ somewhat complicated formulas with hard-to-measure parametersIn this paper, we introduce a simple method to accurately compare the energy-efficiency of speculative architectures. Our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities. We demonstrate the usefulness of our metric on the example of value speculation, where we found some proposed value predictors, including low-power designs, not to be energy-efficient
A new clutter rejection algorithm for Doppler ultrasound
Several strategies, known as clutter or wall Doppler filtering, were proposed to remove the strong echoes produced by stationary or slow moving tissue structures from the Doppler blood flow signal. In this study, the matching pursuit (MP) method is proposed to remove clutter components. The MP method decomposes the Doppler signal into wavelet atoms that are selected in a decreasing energy order. Thus, the high-energy clutter components are extracted first. In the present study, the pulsatile Doppler signal s(n) was simulated by a sum of random-phase sinusoids. Two types of high-amplitude clutter signals were then superimposed on s(n): time-varying low-frequency components, covering systole and early diastole, and short transient clutter signals, distributed within the whole cardiac cycle. The Doppler signals were modeled with the MP method and the most dominant atoms were subtracted from the time-domain signal s(n) until the signal-to-clutter (S/C) ratio reached a maximum. For the low-frequency clutter signal, the improvement in S/C ratio was 19.0 /spl plusmn/ 0.6 dB, and 72.0 /spl plusmn/ 4.5 atoms were required to reach this performance. For the transient clutter signal, ten atoms were required and the maximum improvement in S/C ratio was 5.5 /spl plusmn/ 0.5 dB. The performance of the MP method was also tested on real data recorded over the common carotid artery of a normal subject. Removing 15 atoms significantly improved the appearance of the Doppler sonogram contaminated with low-frequency clutter. Many more atoms (over 200) were required to remove transient clutter components. These results suggest the possibility of using this signal processing approach to implement clutter rejection filters on ultrasound commercial instruments.
Causal time series analysis of functional magnetic resonance imaging data
This review focuses on dynamic causal analysis of functional magnetic resonance (fMRI) data to infer brain connectivity from a time series analysis and dynamical systems perspective. Causal influence is expressed in the Wiener-Akaike-Granger-Schweder (WAGS) tradition and dynamical systems are treated in a state space modeling framework. The nature of the fMRI signal is reviewed with emphasis on the involved neuronal, physiological and physical processes and their modeling as dynamical systems. In this context, two streams of development in modeling causal brain connectivity using fMRI are discussed: time series approaches to causality in a discrete time tradition and dynamic systems and control theory approaches in a continuous time tradition. This review closes with discussion of ongoing work and future perspectives on the integration of the two approaches.
Medium access control in ad hoc networks with MIMO links: optimization considerations and algorithms
we present a medium access control (MAC) protocol for ad hoc networks with multiple input multiple output (MIMO) links. MIMO links provide extremely high spectral efficiencies in multipath channels by simultaneously transmitting multiple independent data streams in the same channel. MAC protocols have been proposed in related work for ad hoc networks with other classes of smart antennas such as switched beam antennas. However, as we substantiate in the paper, the unique characteristics of MIMO links coupled with several key optimization considerations, necessitate an entirely new MAC protocol. We identify several advantages of MIMO links, and discuss key optimization considerations that can help in realizing an effective MAC protocol for such an environment. We present a centralized algorithm called stream-controlled medium access (SCMA) that has the key optimization considerations incorporated in its design. Finally, we present a distributed SCMA protocol that approximates the centralized algorithm and compare its performance against that of baseline protocols that are CSMA/CA variants.
Design and Implementation of Physical Layer Private Key Setting for Wireless Networks
Due to the enormous spreading of applied wireless networks, security is actually one of the most important issues for telecommunications. One of the main issue in the field of securing wireless information exchanging is the initial common knowledge between source and destination. A shared secret is normally mandatory in order to decide the encryption (algorithm or code or key) of the information stream. It is usual to exchange this common a priori knowledge by using a "secure" channel. Now a days a secure wireless channel is not possible. In fact normally the common a priori knowledge is already established (but this is not secure) or by using a non-radio channel (that implies a waste of time and resource). This contribution deals with the proposal of a new modulation technique ensuring secure communication in a full wireless environment. The information is modulated, at physical layer, by the thermal noise experienced by the link between two terminals. A loop scheme is designed for unique recovering of mutual information. The probability of error/detection is analytically derived for the legal users and for the third unwanted listener. The proposed scheme has also been implemented in a Xilinx Virtex II FPGA.#R##N##R##N#All the results show that the performance of the proposed scheme yields the advantage of intrinsic security, i.e., the mutual information cannot be physically demodulated (passive attack) or denied (active attack) by a third terminal, leading us to conclude that the proposed technique is really useful for private key distribution in every wireless network.
Circular Acoustic Vector-Sensor Array for Mode Beamforming
Undersea warfare relies heavily on acoustic means to detect a submerged vessel. The frequency of the acoustic signal radiated by the vessel is typically very low, thus requires a large array aperture to achieve acceptable angular resolution. In this paper, we present a novel approach for low-frequency direction-of-arrival (DOA) estimation using miniature circular vector-sensor array mounted on the perimeter of a cylinder. Under this approach, we conduct beamforming using decomposition in the acoustic mode domain rather than frequency domain, to avoid the long wavelength constraints. We first introduce a multi-layer acoustic gradient scattering model to provide a guideline and performance predication tool for the mode beamformer design and algorithm. We optimize the array gain and frequency response with this model. We further develop the adaptive DOA estimation algorithm based on this model. We formulate the Capon spectra of the mode beamformer which is independent of the frequency band after the mode decomposition. Numerical simulations are conducted to quantify the performance and evaluate the theoretical results developed in this study.
On the expressive power of KLAIM-based calculi
We study the expressive power of variants of KLAIM, an experimental language with programming primitives for network-aware programming that combines the process algebra approach with the coordination-oriented one. KLAIM has proved to be suitable for programming a wide range of distributed applications with agents and code mobility, and has been implemented on the top of a runtime system written in Java. In this paper, the expressivity of its constructs is tested by distilling from it a few, more and more foundational, languages and by studying the encoding of each of them into a simpler one. The expressive power of the considered calculi is finally tested by comparing one of them with asynchronous π-calculus.
Fast Principal Component Analysis using Eigenspace Merging
In this paper, we propose a fast algorithm for principal component analysis (PCA) dealing with large high-dimensional data sets. A large data set is firstly divided into several small data sets. Then, the traditional PCA method is applied on each small data set and several eigenspace models are obtained, where each eigenspace model is computed from a small data set. At last, these eigenspace models are merged into one eigenspace model which contains the PCA result of the original data set. Experiments on the FERET data set show that this algorithm is much faster than the traditional PCA method, while the principal components and the reconstruction errors are almost the same as that given by the traditional method.
Towards the Automated Generation of Hard Disk Models through Physical Geometry Discovery
As the High Performance Computing industry moves towards the exascale era of computing, parallel scientific and engineering applications are becoming increasingly complex. The use of simulation allows us to predict how an application's performance will change with the adoption of new hardware or software, helping to inform procurement decisions. In this paper, we present a disk simulator designed to predict the performance of read and write operations to a single hard disk drive (HDD). Our simulator uses a geometry discovery benchmark (Diskovery) in order to estimate the data layout of the HDD, as well as the time spent moving the read/write head. We validate our simulator against two different HDDs, using a benchmark designed to simulate common disk read and write patterns, demonstrating accuracy to within 5% of the observed I/O time for sequential operations, and to within 10% of the observed time for seek-heavy workloads.
A Novel Approach for Binarization of Overlay Text
In this paper, we presents a new binarization approach to extract text pixels from complex background in video frames. The binarization computation is a crucial step for, video text recognition, which can greatly increase the recognition, accuracy of an OCR software. The proposed approach consists, of four phases. First, the text polarity is determined, i.e. light text with dark background or dark text with light background., Then the pixels in the given image are clustered into K clusters, using the K-means algorithm in the RGB color space and the, text cluster is selected based on the text polarity. Further, the, MRF Model is exploited to get the binarization result. Finally, the, result is further refined by the Log-Gabor filter. The Experimental, results on a large dataset show that the significant gains have been, obtained according to the segmentation performance on the pixel, level as well as the OCR accuracy.
An efficient architecture for JPEG2000 coprocessor
JPEG2000 is a new international standard for still image compression. It provides various functions in one single coding stream and the better compression quality than the traditional JPEG, especially in the high compression ratio. However, the heavy computation and large internal memory requirement still restrict the consumer electronics applications. In this paper, we propose a QCB (quad code block)-based DWT method to achieve the higher parallelism than the traditional DWT approach of JPEG2000 coding process. Based on the QCB-based DWT engine, three code blocks can be completely generated after every fixed time slice recursively. Thus, the DWT and EBCOT processors can process simultaneously and the high computational EBCOT then has the higher parallelism of the JPEG2000 encoding system. By changing the output timing of the DWT process and parallelizing with EBCOT, the internal tile memory size can be reduced by a factor of 4. The memory access cycles between the internal tile memory and the code block memory also decrease with the smooth encoding flow.
Predicting Billboard Success Using Data-Mining in P2P Networks
Peer to Peer networks are the leading cause for music piracy but also used for music sampling prior to purchase. In this paper we investigate the relations between music file sharing and sales (both physical and digital)using large Peer-to-Peer query database information. We compare file sharing information on songs to their popularity on the Billboard Hot 100 and the Billboard Digital Songs charts, and show that popularity trends of songs on the Billboard have very strong correlation (0.88-0.89) to their popularity on a Peer-to-Peer network. We then show how this correlation can be utilized by common data mining algorithms to predict a song's success in the Billboard in advance, using Peer-to-Peer information.
Effects of Popular Exemplars in Television News
Common people that are apparently randomly selected by journalists to illustrate a news story (popular exemplars) have a substantial effect on what the audience think about the issue. This effect may be partly due to the mere fact that popular exemplars attract attention and act as attention commanders just like many other speaking sources in the news. Yet, popular exemplars’ effects extend well beyond that of other talking sources. Due to their similarity, trustworthiness, and the vividness of their account, popular exemplars have significantly more impact than experts that are being interviewed or, in particular, than politicians that are quoted in the news. We show this drawing on an internet-based experiment that uses fake television news items as stimuli and that systematically compares the effect of these talking sources in the news. We also find that taking into account preexisting attitudes changes the findings substantially. The effects are more robust and yield a more nuanced picture of what typ...
On second-order statistics of log-periodogram with correlated components
We derive an explicit expression for the covariance of the log-periodogram power spectral density estimator for a zero mean Gaussian process. We do not make the assumption that the spectral components of the process are uncorrelated. Applications to spectral estimation and to cepstral modeling in automatic speech recognition are discussed.
An additive exponential noise channel with a transmission deadline
We derive the maximum mutual information for an additive exponential noise (AEN) channel with a peak input constraint. We find that the optimizing input density is mixed (with singularities) similar to previous results for AEN channels with a mean input constraint. Likewise, the maximum mutual information takes a similar form, though obviously the maximum for the peak constraint is smaller than for the corresponding mean-constrained channel. This model is inspired by multiple biological phenomena and processes which can be abstracted as follows: inscribed matter is sent by an emitter, moves through a medium, and arrives eventually at its destination receptor. The inscribed matter can convey information in a variety of ways such as the number of signaling quanta - molecules, macromolecular complexes, organelles, cells and tissues - that are emitted as well as the detailed pattern of their release. However, rather than focus on a general class of emitter-receptor systems or a particular exemplar of biomedical importance, our ultimate goal is to provide bounds on the potential efficacy of timed-release signaling for any system which emits identical signaling quanta. That is, we seek to apply one of the most potent aspects of information theory to biological signaling - mechanism blindness - in the hopes of gaining insights applicable to diverse systems that span a wide range of spatiotemporal scales.
Exhaustive search for small fully absorbing sets and the corresponding low error-floor decoder
This work provides an exhaustive search algorithm for finding small fully absorbing sets (FASs) of arbitrary low-density parity-check (LDPC) codes. In particular, given any LDPC code, the problem of finding all FASs of size less than t is formulated as an integer programming problem, for which a new branch-&-bound algorithm is devised. New node selection and the tree-trimming mechanisms are designed to further enhance the efficiency of the algorithm. The proposed algorithm is capable of finding all FASs of size ≤ 11 with no larger than 2 induced odd-degree check nodes for LDPC codes of length ≤ 1000. The resulting exhaustive list of small FASs is then used to devise a new post-processing decoder. Numerical results show that by taking advantage of the exhaustive list of small FASs, the proposed decoder significantly lowers the error floor for codes of practical lengths and outperforms the state-of-the-art low-error-floor decoders.
A multi-objective artificial immune algorithm for parameter optimization in support vector machine
Support vector machine (SVM) is a classification method based on the structured risk minimization principle. Penalize, C; and kernel, @s parameters of SVM must be carefully selected in establishing an efficient SVM model. These parameters are selected by trial and error or man's experience. Artificial immune system (AIS) can be defined as a soft computing method inspired by theoretical immune system in order to solve science and engineering problems. A multi-objective artificial immune algorithm has been used to optimize the kernel and penalize parameters of SVM in this paper. In training stage of SVM, multiple solutions are found by using multi-objective artificial immune algorithm and then these parameters are evaluated in test stage. The proposed algorithm is applied to fault diagnosis of induction motors and anomaly detection problems and successful results are obtained.
A novel hidden station detection mechanism in IEEE 802.11 WLAN
The popular IEEE 802.11 wireless local area network (WLAN) is based on a carrier sense multiple access with collision avoidance (CSMA/CA), where a station listens to the medium before transmission in order to avoid collision. If there exist stations which can not hear each other, i.e., hidden stations, the potential collision probability increases, thus dramatically degrading the network throughput. The RTS/CTS (request-to-send/clear-to-send) frame exchange is a solution for the hidden station problem, but the RTS/CTS exchange itself consumes the network resources by transmitting the control frames. In order to maximize the network throughput, we need to use the RTS/CTS exchange adaptively only when hidden stations exist in the network. In this letter, a simple but very effective hidden station detection mechanism is proposed. Once a station detects the hidden stations via the proposed detection mechanism, it can trigger the usage of the RTS/CTS exchange. The simulation results demonstrate that the proposed mechanism can provide the maximum system throughput performance
Low complexity algorithm for robust video frame rate up-conversion (FRUC) technique
Two challenging situations for video frame rate up-conversion (FRUC) are first identified and analyzed; namely, when the input video has abrupt illumination change and/or a low frame rate. Then, a low-complexity processing technique and robust FRUC algorithm are proposed to address these two issues. The proposed algorithm utilizes a translational motion vector model of the first- and the second-order and detects the continuity of these motion vectors. Additionally, in order to improve perceptual quality of interpolated frame, spatial smoothness criterion is employed. The superior performance of the proposed algorithm has been tested extensively and representative examples are given in this work.
Timestamping schemes for MPEG-2 systems layer and their effect on receiver clock recovery
We propose and analyze several strategies for performing timestamping of an MPEG-2 Transport Stream transmitted over a packet-switched network using the PCR-unaware encapsulation scheme, and analyze their effect on the quality of the recovered clock at the MPEG-2 Systems decoder. When the timestamping scheme is based on a timer with a fixed period, the PCR values in the packet stream may switch polarity deterministically, at a frequency determined by the timer period and the transport rate of the MPEG signal. This, in turn, can degrade the duality of the recovered clock at the receiver beyond acceptable limits. We consider three timestamping schemes for solving this problem: (1) selecting a deterministic timer period to avoid the phase difference in PCR values altogether, (2) fine-tuning the deterministic timer period to maximize the frequency of PCR polarity changes, and (3) selecting the timer period randomly to eliminate the deterministic PCR polarity changes. For the case of deterministic timer period, we derive the frequency of the PCR polarity changes as a function of the timer period and the transport rate, and use it to find ranges of the timer period for acceptable quality of the recovered clock. We also analyze a random timestamping procedure based on a random telegraph process and obtain lower bounds on the rate of PCR polarity changes such that the recovered clock does not violate the PAL/NTSC clock specifications. The analytical results are verified by simulations with both synthetic and actual MPEG-2 Transport Streams sent to a simulation model of an MPEG-2 Systems decoder.
Pyramid transform and scale-space analysis in image analysis
The pyramid transform compresses images while preserving global features such as edges and segments. The pyramid transform is efficiently used in optical flow computation starting from planar images captured by pinhole camera systems, since the propagation of features from coarse sampling to fine sampling allows the computation of both large displacements in low-resolution images sampled by a coarse grid and small displacements in high-resolution images sampled by a fine grid.#R##N##R##N#The image pyramid transform involves the resizing of an image by downsampling after convolution with the Gaussian kernel. Since the convolution with the Gaussian kernel for smoothing is derived as the solution of a linear diffusion equation, the pyramid transform is performed by applying a downsampling operation to the solution of the linear diffusion equation.
Adaptive modelling of digital hearing aids using a subband affine projection algorithm
Adaptive modeling of digital hearing aids is useful in characterizing the hearing aid behavior in response to “real world” stimuli such as speech and music. Most modern hearing aids employ amplitude compression in different frequency bands for effective mapping of the wide dynamic range audio signals into the reduced dynamic range of the hearing impaired listeners. Due to the presence of independent compression channels, the conventional fullband adaptive model might not adequately characterize the performance of a multichannel compression hearing aid (MCHA). In this paper, we propose a subband adaptive modeling approach to characterize the electroacoustic performance of a MCHA. The proposed structure employs uniform, oversampled DFT filterbanks for analysis and synthesis, and the affine projection algorithm for adaptive modeling in each subband. Experiments with simulated MCHAs showed that the subband structure outperforms the fullband structure under a variety of operating conditions.
A Cost Effective Centralized Adaptive Routing for Networks-on-Chip
As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) encounters unpredictable, heterogeneous and time dependent traffic loads. This motivates the introduction of adaptive routing mechanisms that balance the NoC's loads and achieve higher throughput compared with traditional oblivious routing schemes. An effective adaptive routing scheme should be based on a global view of the network state. However, most current adaptive routing schemes, following off-chip networks, are based on distributed reactions to local congestion. In this paper we leverage the unique on-chip capabilities and introduce a novel paradigm of NoC centralized adaptive routing. Our scheme continuously monitors the global traffic load in the network and modifies the routing of packets to improve load balancing accordingly. We present a specific design for the case of mesh topology, where XY or YX routes are adaptively selected for each source-destination pair. We show that while our implementation is lightweight and scalable in hardware costs, it outperforms oblivious and distributed adaptive routing schemes in terms of load balancing and average packet delay.
Robust directional features for wordspotting in degraded Syriac manuscripts
This paper presents a contribution to Word Spotting applied for digitized Syriac manuscripts. The Syriac language was wrongfully accused of being a dead language and has been set aside by the domain of handwriting recognition. Yet it is a very fascinating handwriting that combines the word structure and calligraphy of the Arabic handwriting with the particularity of being intentionally written tilted by an angle of approximately 45deg. For the spotting process, we developed a method that should find all occurrences of a certain query word image, based on a selective sliding window technique, from which we extract directional features and afterwards perform a matching using Euclidean distance correspondence between features. The proposed method does not require any prior information, and does not depend of a word to character segmentation algorithm which would be extremely complex to realize due to the tilted nature of the handwriting.
Statistical semantics for enhancing document clustering
Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. This paper proposes new models for document representation that capture semantic similarity between documents based on measures of correlations between their terms. The paper uses the proposed models to enhance the effectiveness of different algorithms for document clustering. The proposed representation models define a corpus-specific semantic similarity by estimating measures of term–term correlations from the documents to be clustered. The corpus of documents accordingly defines a context in which semantic similarity is calculated. Experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to VSM and other well-known models for capturing semantic similarity.
Transcript mapping for historic handwritten document images
There is a large number of scanned historical documents that need to be indexed for archival and retrieval purposes. A visual word spotting scheme that would serve these purposes is a challenging task even when the transcription of the document image is available. We propose a framework for mapping each word in the transcript to the associated word image in the document. Coarse word mapping based on document constraints is used for lexicon reduction. Then, word mappings are refined using word recognition results by a dynamic programming algorithm that finds the best match while satisfying the constraints.
Less Is More: Mixed-Initiative Model-Predictive Control With Human Inputs
This paper presents a new method for injecting human inputs into mixed-initiative interactions between humans and robots. The method is based on a model-predictive control (MPC) formulation, which inevitably involves predicting the system (robot dynamics as well as human input) into the future. These predictions are complicated by the fact that the human is interacting with the robot, causing the prediction method itself to have an effect on future human inputs. We investigate and develop different prediction schemes, including fixed and variable horizon MPCs and human input estimators of different orders. Through a search-and-rescue-inspired human operator study, we arrive at the conclusion that the simplest prediction methods outperform the more complex ones, i.e., in this particular case, less is indeed more.
Enhanced Color-Theory-Based Dynamic Localization in Mobile Wireless Sensor Networks
There are few localization schemes targeted at mobile wireless sensor networks. This paper proposed an enhanced color-theory-based dynamic localization (E-CDL) which is based on the CDL algorithm (Shee, 2005). However, the location accuracy of this algorithm depends on the accuracy of the average hop distance derivation. Therefore, the authors present two novel schemes to estimate the average hop distance. The authors analyzed the behavior of sensor nodes communication, and computed the expected value of the average hop distance, which is 7r/9 where r is the radio range. In addition, since CDL is based on the DV-hop scheme, the derived shortest path length is usually larger than the corresponding Euclidean distance. With this observation, the derived shortest path length can be adjusted by the ratio of the Euclidean distance and the shortest path distance to further enhance the location accuracy. Finally, in mobile wireless sensor networks, sensor nodes may become isolated. By employing mobile anchor nodes, the isolation problem can be relieved and hence the location accuracy can be improved. Simulation results have shown that the location accuracy of E-CDL is 50%-55% better than that of CDL, and 75%-80% better than that of MCL (Monte Carlo localization) (Lingxuan and David, 2004), In addition, the authors have implemented and verified our algorithm on the MICAz Mote developer's kit.
Biometric binary string generation with detection rate optimized bit allocation
Extracting binary strings from real-valued templates has been a fundamental issue in many biometric template protection systems. In this paper, we present an optimal bit allocation method (OBA). By means of it, a binary string at a pre-defined length with maximized overall detection rate is generated. Experiments with the binary strings and a Hamming distance classifier on FRGC and FERET databases show promising performance in terms of FAR and FRR.
Dynamic identification of a 6 dof robot without joint position data
Off-line robot dynamic identification methods are mostly based on the use of the inverse dynamic model, which is linear with respect to the dynamic parameters. This model is calculated with torque and position sampled data while the robot is tracking reference trajectories that excite the system dynamics. This allows using linear least-squares techniques to estimate the parameters. This method requires the joint force/torque and position measurements and the estimate of the joint velocity and acceleration, through the bandpass filtering of the joint position at high sampling rates. A new method called DIDIM (Direct and Inverse Dynamic Identification Models) has been proposed and validated on a 2 degree-of-freedom robot [1]. DIDIM method requires only the joint force/torque measurement. It is based on a closed-loop simulation of the robot using the direct dynamic model, the same structure of the control law, and the same reference trajectory for both the actual and the simulated robot. The optimal parameters minimize the 2-norm of the error between the actual force/torque and the simulated force/torque. A validation experiment on a 6 dof Staubli TX40 robot shows that DIDIM method is very efficient on industrial robots.
Bit-Stream Switching in Multiple Bit-Rate Video Streaming using Wyner-Ziv Coding
It has been commonly recognized that multiple bit-rate (MBR) encoding provides a concise method for video streaming over bandwidth-fluctuant networks. The key problem of the MBR technique lies in how to seamlessly switch one bit-stream to another one. To tackle this problem, we propose a bit-stream switching framework based on the Wyner-Ziv coding. Within the propose framework, the multiple bit-streams can be individually encoded without data exchange, which also supports the random switching at any desired frame without affecting the original coding efficiency of the regular bit-stream. In particular, two different implementation schemes under the same framework are presented. Different from the traditional switching schemes, the proposed method can use the same switching frame for the switching from any other bit-stream to the current one, which means less storage and less encoding efforts. Simulation results and comparison between the proposed method and the traditional switching method in H.264 are also presented.
An architecture for linking medical decision-support applications to clinical databases and its evaluation
We describe and evaluate a framework, the Medical Database Adaptor (MEIDA), for linking knowledge-based medical decision-support systems (MDSSs) to multiple clinical databases, using standard medical schemata and vocabularies. Our solution involves a set of tools for embedding standard terms and units within knowledge bases (KBs) of MDSSs; a set of methods and tools for mapping the local database (DB) schema and the terms and units relevant to the KB of the MDSS into standardized schema, terms and units, using three heuristics (choice of a vocabulary, choice of a key term, and choice of a measurement unit); and a set of tools which, at runtime, automatically map standard term queries originating from the KB, to queries formulated using the local DB's schema, terms and units. The methodology was successfully evaluated by mapping three KBs to three DBs. Using a unit-domain matching heuristic reduced the number of term-mapping candidates by a mean of 71% even after other heuristics were used. Runtime access of 10,000 records required one second. We conclude that mapping MDSSs to different local clinical DBs, using the three-phase methodology and several term-mapping heuristics, is both feasible and efficient.
Animation Metaphors for Object-Oriented Concepts
Program visualization and animation has traditionally been done at the level of the programming language and its implementation in a computer. However, novices do not know these concepts and visualizations that build upon programming language implementation may easily fail in helping novices to learn programming concepts. Metaphor, on the contrary, involves the presentation of a new idea in terms of a more familiar one and can facilitate active learning. This paper applies a metaphor approach to object-oriented programming by presenting new metaphors for such concepts as class, object, object instantiation, method invocation, parameter passing, object reference, and garbage collection. The use of these metaphors in introductory programming education is also discussed.
Performance of recovery time improvement algorithms for software RAIDs
A software RAID is a RAID implemented purely in software running on a host computer. One problem with software RAIDs is that they do not have access to special hardware such as NVRAM. Thus, software RAIDs may need to check every parity group of an array for consistency following a host crash or power failure. This process of checking parity groups is called recovery, and results in long delays when the software RAID is restarted. The authors review two algorithms to reduce this recovery time for software RAIDs: the PGS bitmap algorithm and the list algorithm. They compare the performance of these two algorithms using trace-driven simulations. Their results show that the PGS bitmap algorithm can reduce recovery time by a factor of 12 with a response time penalty of less than 1%, or by a factor of 50 with a response time penalty of less than 2%, and a memory requirement of around 9 Kbytes. The list algorithm can reduce recovery time by a factor of 50 but cannot achieve a response time penalty of less than 16%.
Independent gate SRAM based on asymmetric gate to source/drain overlap-underlap device FinFET
The read-write ability of SRAM cells is one of the major concern in nanometer regime. This paper analyzes the stability and performance of asymmetric FinFET based different schematic of 6T SRAM cells. The proposed structure exploits asymmetrical behavior of current to improve read-write stability of SRAM. By exploiting the asymmetricity in proposed structure, contradiction between read and write noise margin (RNM and WNM) is relaxed. The overall improvements in static, read and write noise margins for proposed asymmetric FinFET based independent gate SRAM (IGSRAM) are 28%, 71%, and 31% respectively.
Distributed, scalable, and static parallel arc consistency algorithms on private memory machines
Several arc consistency algorithms for sequential and parallel processing computers are reviewed. Three distributed parallel arc consistency algorithms-DSPAC-1, DSPAC-2, and DSPAC-3-are introduced and compared with existing algorithms. Through actual machine experimentation the time required for the DSPAC algorithms was measured and compared with that for existing sequential algorithms. Results indicate that the parallel arc consistency algorithms are very effective and that scalability can be efficiently maintained. >
Provisioning On-Chip Networks under Buffered RC Interconnect Delay Variations
A network-on-chip (NoC) replaces on-chip communication implemented by point-to-point interconnects in a multi-core environment by a set of shared interconnects connected through programmable crosspoints. Since an NoC may provide a number of paths between a given source and destination, manufacturing or runtime faults on one interconnect does not necessarily render the chip useless. It is partly because of this fault tolerance that NoCs have emerged as a viable alternative for implementing communication between functional units of a chip in the nanometer regime, where high defect rates are prevalent. In this paper, the authors quantify the fault tolerance offered by an NoC against process variations. Specifically, the authors develop an analytical model for the probability of failure in buffered global NoC links due to interconnect dishing, and effective channel length variation. Using the developed probability model, the authors study the impact of link failure on the number of cycles required to establish communications in NoC applications
Accurate Distributed Range-Based Positioning Algorithm for Wireless Sensor Networks
Localization of sensor nodes is a fundamental and important problem in wireless sensor networks. In this correspondence, a recursive distributed positioning algorithm is devised with the use of range measurements. Computer simulations are included to contrast the performance of the proposed approach with the conventional semi-definite relaxation positioning method as well as Crameacuter-Rao lower bound.
Prospective multi-centre Voxel Based Morphometry study employing scanner specific segmentations : procedure development using CaliBrain structural MRI data
Background#R##N#Structural Magnetic Resonance Imaging (sMRI) of the brain is employed in the assessment of a wide range of neuropsychiatric disorders. In order to improve statistical power in such studies it is desirable to pool scanning resources from multiple centres. The CaliBrain project was designed to provide for an assessment of scanner differences at three centres in Scotland, and to assess the practicality of pooling scans from multiple-centres.
Improving the scalability of cloud-based resilient database servers
Many rely now on public cloud infrastructure-as-a-service for database servers, mainly, by pushing the limits of existing pooling and replication software to operate large shared-nothing virtual server clusters. Yet, it is unclear whether this is still the best architectural choice, namely, when cloud infrastructure provides seamless virtual shared storage and bills clients on actual disk usage.   This paper addresses this challenge with Resilient Asynchronous Commit (RAsC), an improvement to awell-known shared-nothing design based on the assumption that a much larger number of servers is required for scale than for resilience. Then we compare this proposal to other database server architectures using an analytical model focused on peak throughput and conclude that it provides the best performance/cost trade-off while at the same time addressing a wide range of fault scenarios.
A neural network-based image processing system for detection of vandal acts in unmanned railway environments
Lately, the interest in advanced video-based surveillance applications has been increasing. This is especially true in the field of urban railway transport where video-based surveillance can be exploited to face many relevant security aspects (e.g. vandalism, overcrowding, abandoned object detection etc.). This paper aims at investigating an open problem in the implementation of video-based surveillance systems for transport applications, i.e., the implementation of reliable image understanding modules in order to recognize dangerous situations with reduced false alarm and misdetection rates. We considered the use of a neural network-based classifier for detecting vandal behavior in metro stations. The achieved results show that the classifier achieves very good performance even in the presence of high scene complexity.
Recursive binary dilation and erosion using digital line structuring elements in arbitrary orientations
Performing morphological operations such as dilation and erosion of binary images, using very long line structuring elements is computationally expensive when performed brute-force following definitions. We present two-pass algorithms that run at constant time for obtaining binary dilations and erosions with all possible length line structuring elements, simultaneously. The algorithms run at constant time for any orientation of the line structuring element. Another contribution of this paper is the use of the concept of orientation error between a continuous line and its discrete counterpart. The orientation error is used in determining the minimum length of the basic digital line structuring element used in obtaining what we call dilation and erosion transforms. The transforms are then thresholded by the length of the desired structuring element to obtain the dilation and erosion results. The algorithms require only one maximum operation for erosion transform and only one minimum operation for dilation transform, and one thresholding step and one translation step per result pixel. We tested the algorithms on Sun Sparc Station 10, on a set of 240/spl times/250 salt and pepper noise images with probability of a pixel being a 1-pixel set to 0.25, for orientations of the normals of the structuring elements in the range [/spl pi//2,3/spl pi//2] and lengths, in pixels, in the range [5,145]. We achieved a speed up of about 50 (and for special orientations /spl theta/ /spl isin/ {(/spl pi//2), (3/spl pi//4), /spl pi/, (5/spl pi//4), (3/spl pi//2)} a speed up of about 100) when the structuring elements had lengths of 145 pixels, over the brute-force methods in these experiments. We compared the results of our dilation algorithm with those of the algorithm discussed by Soille et al. (see IEEE Trans. Pattern Anal. Machine Intell., vol.18, p.562-67, 1996) and showed that for binary dilation (and erosion since it is just the dilation of the background with the reflected structuring element) our algorithm performed better and achieved a speed up of about four when dilation or erosion transform alone is obtained.
Using assignment examples to infer weights for ELECTRE TRI method: Some experimental results
Given a finite set of alternatives A, the sorting (or assignment) problem consists in the assignment of each alternative to one of the pre-defined categories. In this paper, we are interested in multiple criteria sorting problems and, more precisely, in the existing method ELECTRE TRI. This method requires the elicitation of preferential parameters (weights, thresholds, category limits,…) in order to construct a preference model which the decision maker (DM) accepts as a working hypothesis in the decision aid study. A direct elicitation of these parameters requiring a high cognitive effort from the DM (V. Mosseau, R. Slowinski, Journal of Global Optimization 12 (2) (1998) 174), proposed an interactive aggregation–disaggregation approach that infers ELECTRE TRI parameters indirectly from holistic information, i.e., assignment examples. In this approach, the determination of ELECTRE TRI parameters that best restore the assignment examples is formulated through a nonlinear optimization program.#R##N##R##N#In this paper, we consider the subproblem of the determination of the weights only (the thresholds and category limits being fixed). This subproblem leads to solve a linear program (rather than nonlinear in the global inference model). Numerical experiments were conducted so as to check the behaviour of this disaggregation tool. Results showed that this tool is able to infer weights that restores in a stable way the assignment examples and that it is able to identify “inconsistencies” in the assignment examples.
A 250 mV 8 kb 40 nm Ultra-Low Power 9T Supply Feedback SRAM (SF-SRAM)
Low voltage operation of digital circuits continues to be an attractive option for aggressive power reduction. As standard SRAM bitcells are limited to operation in the strong-inversion regimes due to process variations and local mismatch, the development of specially designed SRAMs for low voltage operation has become popular in recent years. In this paper, we present a novel 9T bitcell, implementing a Supply Feedback concept to internally weaken the pull-up current during write cycles and thus enable low-voltage write operations. As opposed to the majority of existing solutions, this is achieved without the need for additional peripheral circuits and techniques. The proposed bitcell is fully functional under global and local variations at voltages from 250 mV to 1.1 V. In addition, the proposed cell presents a low-leakage state reducing power up to 60%, as compared to an identically supplied 8T bitcell. An 8 kbit SF-SRAM array was implemented and fabricated in a low-power 40 nm process, showing full functionality and ultra-low power.
Map-Aided Evidential Grids for Driving Scene Understanding
Evidential grids have recently been shown to have interesting properties for mobile object perception. Possessing only partial information is a frequent situation when driving in complex urban areas, and by making use of the Dempster-Shafer framework, evidential grids are able to handle partial information efficiently. This article deals with a lidar perception scheme that is enhanced by geo-referenced maps used as an additional source of information in a multi-grid fusion framework. The paper looks at the key stages of such a data fusion process and presents an adaptation of the conjunctive combination rule for refining the analysis of conflicting information. This method relies on temporal accumulation to distinguish between stationary and moving objects, and applies contextual discounting for modeling information obsolescence. As a result, the method is able to better characterize the state of the occupied cells by differentiating moving objects, parked cars, urban infrastructure and buildings. Another advantage of this approach is its ability to separate the drivable from the non-drivable free space. Experiments carried out in real traffic conditions with a specially equipped car illustrate the performance of this approach.
Interferential Packet Detection Scheme for a Solution to Overlapping BSS Issues in IEEE 802.11 WLANs
In this manuscript, an interferential packet detection scheme in IEEE 802.11 WLANs is proposed. If another Basic Service Set (BSS) is overlapping domestic BSS, some Stations (STAs) may suffer from interference from a hidden terminal in overlapping BSS (OBSS). One of the best ways to avoid this undesirable situation is channel switching after detecting OBSS. However there are some difficulties to recognize existence of OBSS. One difficulty is that STAs in domestic BSS can't receive frames from OBSS correctly and can't check BSSID of frames if traffic in domestic BSS is heavy and if transmission rates of frames from OBSS are higher. Another difficulty is that if the cell radius of domestic BSS is smaller than that of OBSS, some STAs in OBSS may transmit frames asynchronously and interfere with transmissions in domestic BSS. If interference causes frame error, domestic STA can?t distinguish interference from degradation of channel condition. This paper proposes a method whereby STAs can detect interferential packet even while STAs receive frames from domestic BSS. If STAs detect interference, channel switching is performed dynamically. The probability of detecting interferential packets is evaluated by computer simulation, and the results confirm the effectiveness of the proposed method.
Optimal error estimates for finite element discretization of elliptic optimal control problems with finitely many pointwise state constraints
In this paper we consider a model elliptic optimal control problem with finitely many state constraints in two and three dimensions. Such problems are challenging due to low regularity of the adjoint variable. For the discretization of the problem we consider continuous linear elements on quasi-uniform and graded meshes separately. Our main result establishes optimal a priori error estimates for the state, adjoint, and the Lagrange multiplier on the two types of meshes. In particular, in three dimensions the optimal second order convergence rate for all three variables is possible only on properly refined meshes. Numerical examples at the end of the paper support our theoretical results.
Taxonomy of trust: Categorizing P2P reputation systems
The field of peer-to-peer reputation systems has exploded in the last few years. Our goal is to organize existing ideas and work to facilitate system design. We present a taxonomy of reputation system components, their properties, and discuss how user behavior and technical constraints can conflict. In our discussion, we describe research that exemplifies compromises made to deliver a useable, implementable system.
Performance of a 60-GHz DCM-OFDM and BPSK-Impulse Ultra-Wideband System with Radio-Over-Fiber and Wireless Transmission Employing a Directly-Modulated VCSEL
The performance of radio-over-fiber optical transmission employing vertical-cavity surface-emitting lasers (VCSELs), and further wireless transmission, of the two major ultra-wideband (UWB) implementations is reported when operating in the 60-GHz radio band. Performance is evaluated at 1.44 Gbit/s bitrate. The two UWB implementations considered employ dual-carrier modulation orthogonal frequency-division multiplexing (DCM-OFDM) and binary phase-shift keying impulse radio (BPSK-IR) modulation respectively. Optical transmission distances up to 40 km in standard single-mode fiber and up to 500 m in bend-insensitive single-mode fiber with wireless transmission up to 5 m in both cases is demonstrated with no penalty. A simulation analysis has also been performed in order to investigate the operational limits. The analysis results are in excellent agreement with the experimental work and indicate good tolerance to chromatic dispersion due to the chirp characteristics of electro-optical conversion when a directly-modulated VCSEL is employed. The performance comparison indicates that BPSK-IR UWB exhibits better tolerance to optical transmission impairments requiring lower received optical power than its DCM-OFDM UWB counterpart when operating in the 60-GHz band.
DPLL bit synchronizer with rapid acquisition using adaptive Kalman filtering techniques
A second-order DPLL with time-varying loop gains is applied to the symbol synchronization of burst mode data signals. An algorithm to control the DPLL loop gains is derived from adaptive Kalman filtering theory. Simulation results for the variable gain DPLL compared to a fixed gain DPLL demonstrate the improved acquisition performance. >
A 65 nm CMOS Quad-Band SAW-Less Receiver SoC for GSM/GPRS/EDGE
A quad-band 2.5G receiver is designed to replace the front-end SAW filters with on-chip bandpass filters and to integrate the LNA matching components, as well as the RF baluns. The receiver achieves a typical sensitivity of -110 dBm or better, while saving a considerable amount of BOM. Utilizing an arrangement of four baseband capacitors and MOS switches driven by 4-phase 25% duty-cycle clocks, high-Q BPF's are realized to attenuate the 0 dBm out-of-band blocker. The 65 nm CMOS SAW-less receiver integrated as a part of a 2.5G SoC, draws 55 mA from the battery, and measures an out-of-band 1 dB-compression of greater than +2 dBm. Measured as a stand-alone, as well as the baseband running in call mode in the platform level, the receiver passes the 3GPP specifications with margin.
Automated evaluation of HER-2/neu immunohistochemical expression in breast cancer using digital microscopy
HER-2/neu (HER2) has been shown to be a valuable biomarker for breast cancer. However, inter-observer variability has been reported in the evaluation of HER2 with immunohistochemistry. It has been suggested that automated computer-based evaluation can provide a consistent and objective measure of HER2 expression. In this manuscript, we present an automated method for the quantitative assessment of HER2 using digital microscopy. The method employs imaging algorithms on whole slide images of tissue specimens for the extraction of two features describing HER2 membrane staining, namely membrane staining completeness and membrane staining intensity. A classifier was trained to merge the extracted features into an overall slide assessment score. Preliminary results showed good agreement with the provided truth. The developed automated method has the potential to be used as a computer aid for the immunohistochemical evaluation of HER2 expression with the objective of increasing observer reproducibility.
Adding expressiveness to musical messages
A system to add expressiveness to musical messages has been developed, starting from the results of acoustic and perceptual analyses. The system allows to obtain different performances, by modifying the acoustic parameters of a given neutral performance. The modification of the input performance is performed by a model that uses the hierarchical segmentation of the musical organization. For every hierarchical level, opportune curves are applied to the principal acoustic parameters. Level's self-similarity is the main criteria to construct the curves. The modular structure of the system defines an open architecture, where the rendering steps can be realized both with synthesis and post-processing techniques. Different synthesis techniques, like FM, physical models or wavetable have been explored.
Observations on using empirical studies on developing a knowledge-based software engineering tool
There exist a wide variety of techniques for performing empirical studies which researchers in human-computer interaction have adapted from fields of cognitive psychology, sociology and anthropology. An analysis of several of these techniques is presented through an approach that balances empirical study with tool development. The analysis is based on, and illustrated with, a several-year experience of consulting in a scientific software environment and in building an evaluating a prototype knowledge-based tool to capture aspects of that experience. Guidelines for applying specific techniques and cautions about potential pitfalls are discussed. Many additional examples of using the techniques are cited from the literature. >
Fire Detection by Microwave Radiometric Sensors: Modeling a Scenario in the Presence of Obstacles
This paper deals with the problem of fire detection in the presence of obstacles that are nontransparent to visible or infrared wavelengths. Exploiting the obstacle penetration capability of microwaves, a solution based on passive microwave radiometry has been proposed. To investigate such a solution, a theoretical model of the scene sensed by a microwave radiometer is developed, accounting for the presence of both fire spot and wall-like obstacles. By reversing the model's equations, it is possible to directly relate the obstacle emissivity, reflectivity, and transmissivity to the antenna noise temperatures measured in several conditions. These temperatures have been sensed with a portable low-cost instrument. The selected 12.65-GHz operation frequency features good wall penetration capability to be balanced with a reasonable antenna size. In order to verify the aforementioned model, several fire experiments have been carried out, resulting in an overall good agreement between measurements and developed theory. In particular, a 2-cm-thick plasterboard wall, typically used for indoor building construction, shows a transmissivity equal to 0.86 and can easily be penetrated by a microwave radiometer in the X-band.
Efficient evaluation of queries with mining predicates
Modern relational database systems are beginning to support ad-hoc queries on data mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models: decision trees, naive Bayes, and clustering. Our experiments on a Microsoft SQL Server 2000 demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries.
Stochastic processes via the pathway model
After collecting data from observations or experiments, the next step is to analyze the data to build an appropriate mathematical or stochastic model to describe the data so that further studies can be done with the help of the model. In this article, the input-output type mechanism is considered first, where reaction, diffusion, reaction-diffusion, and production-destruction type physical situations can fit in. Then techniques are described to produce thicker or thinner tails (power law behavior) in stochastic models. Then the pathway idea is described where one can switch to different functional forms of the probability density function through a parameter called the pathway parameter. The paper is a continuation of related solar neutrino research published previously in this journal.
Predictive Metamorphic Control
Model Predictive Control (MPC) has become widely accepted in industry. The reason for its success are manifold including easy implementation, ability to handle constraints, capacity to deal with nonlinearities, etc. However, the method does have drawbacks including tuning difficulties. In this paper, we propose an embellishment to the basic MPC strategy by incorporating a tuning parameter such that one can move continuously from an existing controller to a new MPC strategy. The continuous change of this tuning parameter leads to a continuously varying stabilizing control law. Since the proposed strategy allows one to slowly move from an existing control law to a new and better one, we term the strategy Predictive Metamorphic Control. For the case of an infinite horizon problem without constraints and for the general case with state and input constraints, stability results are established. The merits of the proposed method are illustrated by examples.
Efficient network flow based min-cut balanced partitioning
We consider the problem of bipartitioning a circuit into two balanced components that minimizes the number of crossing nets. Previously, the Kernighan and Lin type (K&L) heuristics, the simulated annealing approach, and the spectral method were given to solve the problem. However, network flow techniques were overlooked as a viable approach to min-cut balanced bipartition to due its high complexity. In this paper we propose a balanced bipartition heuristic based on repeated max-flow min-cut techniques, and give an efficient implementation that has the same asymptotic time complexity as that of one max-flow computation. We implemented our heuristic algorithm in a package called FBB. The experimental results demonstrate that FBB outperforms the K&L heuristics and the spectral method in terms of the number of crossing nets, and the efficient implementation makes it possible to partition large, circuit instances with reasonable runtime. For example, the average elapsed time for bipartitioning a circuit S35932 of almost 20K gates is less than 20 minutes.
Reconstruction of polynomial systems from noisy time-series measurements using genetic programming
The problem of functional reconstruction of a polynomial system from its noisy time-series measurement is addressed in this paper. The reconstruction requires the determination of the embedding dimension and the unknown polynomial structure. The authors propose the use of genetic programming (GP) to find the exact functional form and embedding dimension of an unknown polynomial system from its time-series measurement. Using functional operators of addition, multiplication and time delay, they use GP to reconstruct the exact polynomial system and its embedding dimension. The proposed GP approach uses an improved least-squares (ILS) method to determine the parameters of a polynomial system. The ILS method is based on the orthogonal Euclidean distance to obtain an accurate parameter estimate when the series is corrupted by measurement noise. Simulations show that the proposed ILS-GP method can successfully reconstruct a polynomial system from its noisy time-series measurements.
Grid Service for Environmental Data Retrieval and Disasters Detection Based on Satellite Image Analysis
Considering that one of today's biggest global concerns is related to the climate change and its imminent undesired effects, we present the approach of creating and offering a public Web service to provide real-time access to environmental data and information. One of service's direct usages is natural disasters detection, but it could be further used for developing complex statistics and prediction generators, or for other environment related applications. The data is extracted from a satellite imagery repository implemented on a Grid infrastructure. For testing the capabilities of the service for different type of users, a visualization and interaction Web application has been developed. The service is integrated in the MedioGRID system.
An Evaluation Framework for Energy Aware Buildings using Statistical Model Checking
Cyber-physical systems are to be found in numerous applications throughout society. The principal barrier to develop trustworthy cyber-physical systems is the lack of expressive modelling and specification formalisms supported by efficient tools and methodologies. To overcome this barrier, we extend in this paper the modelling formalism of the tool UPPAAL-SMC to stochastic hybrid automata, thus providing the expressive power required for modelling complex cyber-physical systems. The application of Statistical Model Checking provides a highly scalable technique for analyzing performance properties of this formalisms.
ML-Flex: a flexible toolbox for performing classification analyses in parallel
Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.
Pacemaker interference and low-frequency electric induction in humans by external fields and electrodes
The possibility of interference by low-frequency external electric fields with cardiac pacemakers is a matter of practical concern. For pragmatic reasons, experimental investigations into such interference have used contact electrode current sources. However, the applicability to the external electric field problem remains unclear. The recent development of anatomically based electromagnetic models of the human body, together with progress in computational electromagnetics, enable the use of numerical modeling to quantify the relationship between external field and contact electrode excitation. This paper presents a comparison between the computed fields induced in a 3.6-mm-resolution conductivity model of the human body by an external electric field and by several electrode source configurations involving the feet and either the head or shoulders. The application to cardiac pacemaker interference is also indicated.
A Structurally Stable Globally Adaptive Internal Model Regulator for MIMO Linear Systems
The problem of compensating an uncertain disturbance and/or tracking some reference signals for a general linear MIMO system is studied in this work using the robust regulation theory frame. The disturbances are assumed to be composed by a known number of distinct sinusoidal signals with unknown phases, amplitude and frequencies. Under suitable assumptions, an exponentially convergent estimator of the unknown disturbance parameters is proposed and introduced into the classical robust regulator design to obtain an adaptive controller. This controller guarantees that the closed-loop robust regulation is attained in some neighborhood of the nominal values of the parameters of system. A simulated example shows the validity of the proposed approach.
Developing discriminate model and comparative analysis of differentially expressed genes and pathways for bloodstream samples of diabetes mellitus type 2
Background#R##N#Diabetes mellitus of type 2 (T2D), also known as noninsulin-dependent diabetes mellitus (NIDDM) or adult-onset diabetes, is a common disease. It is estimated that more than 300 million people worldwide suffer from T2D. In this study, we investigated the T2D, pre-diabetic and healthy human (no diabetes) bloodstream samples using genomic, genealogical, and phonemic information. We identified differentially expressed genes and pathways. The study has provided deeper insights into the development of T2D, and provided useful information for further effective prevention and treatment of the disease.
An autonomous sewer robots navigation based on stereo camera information
In this paper, we propose a method for autonomous sewer robots to navigate through a sewer pipe system based on stereo camera information. In this method, local features such as manholes and pipe joints are extracting as a feature pixels in the region of interest (ROI) of left image. Then, an accurate and fast stereo matching measure named linear computation is implemented in this ROI image to compute the distance between the robots and local features. Finally, the distance data can be used for navigation map in sewer pipe system. The experimental results show that our method can provide sufficient information for autonomous sewer robots navigation
Introducing a geographic information system as computer tool to apply the problem-based learning process in public buildings indoor routing
Abstract#R##N##R##N#A geographic information system (GIS) is presented in this work with the aim of helping the application of problem-based learning process to show the students how to adopt the appropriate decisions for the adaptation of architectural barriers to ensure the universal accessibility in public buildings. The GIS developed here consists of three layers based on vector maps corresponding to buildings, potential routes and architectural barriers. Hyperlinks in the last layer allow access to some relevant information about each barrier, such as type, description and adaptation cost. Several tests have been carried out to show the capability of the implemented GIS to locate indoor barriers, determine suitable indoor routes by considering criteria such as paths lengths or the total cost of barrier elimination, and update the information corresponding to each architectural barrier. In addition, the application of the proposed GIS has also been explored for indoor route guidance with promising results. This investigation has been carried out with the aim of being combined with the problem-based learning process. © 2010 Wiley Periodicals, Inc. Comput Appl Eng Educ 21: 573–580, 2013
A Practical Location-aided Energy-aware Routing Method for UWB-based Sensor Networks
In this paper, a practical location-aided routing method for sensor networks based on ultra-wideband (UWB) technique is proposed and evaluated. This method makes use of the positioning function of UWB and takes into account the energy consumption in the network. By modeling the property of energy consumption, we find that energy and quality-of-service (QOS) issues are greatly influenced by the route selected. Accordingly, a new routing algorithm is derived to search for energy-efficient routes that can support adequate QOS requirements. The simulation results have proved the advantages of this routing scheme.
Waveband switching networks with limited wavelength conversion
We study reconfigurable multi-granular optical cross-connects (MG-OXCs) in waveband switching networks with limited wavelength conversion and propose a heuristic algorithm to minimize the number of used wavelength converters while reducing the blocking probability.
An identity-based identification scheme based on discrete logarithms modulo a composite number
We first describe a modification of Schnorr's identification scheme, in which the modulus is composite (instead of prime). This modification has some similarity with Brickell-McCurky's one, presented at the same conference. Then, by establishing a new set-up, we derive the first identity-based identification scheme based on discrete logarithms. More precisely, it is based on discrete logarithm modulo a composite number, a problem known to be harder than factorization problem. This scheme has interesting and somewhat paradoxical features. In particular, any user can choose his own secret, and, provided the parameters have convenient sizes, even the trusted center is unable to retrieve it from the public key (contrary to any identity-based scheme known until now).
Control of Artificial Pneumatic Muscle for Robot Application
Pneumatic muscle has many advantages such as elasticity, high power and structural similarity to a living thing's muscle. There has been many researches to control robot actuated by pneumatic muscles, but conventional theories are hard to apply on real robot plants because of their assumptions and disregards of pneumatic muscle's physical aspects like size of pneumatic muscle and its controller. Here, the new method for saving space which is occupied by many controllers to operate robot actuated by pneumatic muscles is proposed. Actually there is easy way to control pneumatic muscle using the commercial proportional pressure regulator, but its size is not suitable to be embedded on stand alone robot. So, new method using the pressure switches of compact size and encoders is suggested. This new method is tested on a robot link with ball joints, actuated by four pneumatic muscles.
The role of EDM in information management within SMEs
Electronic document management (EDM) is a new form of information management. EDM is described to have certain business values in organizations, but no research has been found about EDM and Small and Medium sized Enterprises (SME). In this paper we present an ongoing investigation in two SMEs guided by the following research questions: "How are electronic documents used in the SMEs? "and "What are the business needs of the SMEs, and how do they correspond with stated EDM business values?". The study was carried out as two qualitative case studies in two SMEs in the north of Sweden. The results show that the business need for an SME corresponds with the business values of EDM. Yet is management of electronic document too dependent on individuals' competence, very complex when many systems are involved, and the context where the document is created is not preserved. There is also an emergent need for an organization-specific classification scheme to enable information sharing between systems.
Forensic acquisition and analysis of magnetic tapes
Recovering evidential data from magnetic tapes in a forensically sound manner is a difficult task. There are many different tape technologies in existence today and an even greater number of archive formats used. This paper discusses the issues and challenges involved in the forensic acquisition and analysis of magnetic tapes. It identifies areas of slack space on tapes and discusses the challenges of low level acquisition of an entire length of tape. It suggests a basic methodology for determining the contents of a tape, acquiring tape files, and preparing them for forensic analysis.
BIFURCATION, CHAOS AND THEIR CONTROL IN A TIME-DELAY DIGITAL TANLOCK LOOP
This paper reports the detailed parameter space study of the nonlinear dynamical behaviors and their control in a time-delay digital tanlock loop (TDTL). At first, we explore the nonlinear dynamics of the TDTL in parameter space and show that beyond a certain value of loop gain parameter the system manifests bifurcation and chaos. Next, we consider two variants of the delayed feedback control (DFC) technique, namely, the time-delayed feedback control (TDFC) technique, and its modified version, the extended time-delayed feedback control (ETDFC) technique. Stability analyses are carried out to find out the stable phase-locked zone of the system for both the controlled cases. We employ two-parameter bifurcation diagrams and the Lyapunov exponent spectrum to explore the dynamics of the system in the global parameter space. We establish that the control techniques can extend the stable phase-locked region of operation by controlling the occurrence of bifurcation and chaos. We also derive an estimate of the optimum parameter values for which the controlled system has the fastest convergence time even for a larger acquisition range. The present study provides a necessary detailed parameter space study that will enable one to design an improved TDTL system.
Contribution to the Determination of In Vivo Mechanical Characteristics of Human Skin by Indentation Test
This paper proposes a triphasic model of intact skin in vivo based on a general phenomenological thermohydromechanical and physicochemical (THMPC) approach of heterogeneous media. The skin is seen here as a deforming stratified medium composed of four layers and made out of different fluid-saturated materials which contain also an ionic component. All the layers are treated as linear, isotropic materials described by their own behaviour law. The numerical simulations of in vivo indentation test performed on human skin are given. The numerical results correlate reasonably well with the typical observations of indented human skin. The discussion shows the versatility of this approach to obtain a better understanding on the mechanical behaviour of human skin layers separately.
Transmit power adaptation for multiuser OFDM systems
In this paper, we develop a transmit power adaptation method that maximizes the total data rate of multiuser orthogonal frequency division multiplexing (OFDM) systems in a downlink transmission. We generally formulate the data rate maximization problem by allowing that a subcarrier could be shared by multiple users. The transmit power adaptation scheme is derived by solving the maximization problem via two steps: subcarrier assignment for users and power allocation for subcarriers. We have found that the data rate of a multiuser OFDM system is maximized when each subcarrier is assigned to only one user with the best channel gain for that subcarrier and the transmit power is distributed over the subcarriers by the water-filling policy. In order to reduce the computational complexity in calculating water-filling level in the proposed transmit power adaptation method, we also propose a simple method where users with the best channel gain for each subcarrier are selected and then the transmit power is equally distributed among the subcarriers. Results show that the total data rate for the proposed transmit power adaptation methods significantly increases with the number of users owing to the multiuser diversity effects and is greater than that for the conventional frequency-division multiple access (FDMA)-like transmit power adaptation schemes. Furthermore, we have found that the total data rate of the multiuser OFDM system with the proposed transmit power adaptation methods becomes even higher than the capacity of the AWGN channel when the number of users is large enough.
Fine grain associative feature reasoning in collaborative engineering
This paper explores the vast domain of systematic collaborative engineering with reference to product lifecycle management approach from the angle of feature-level collaboration among partners. A new method of fine grain feature association modelling and reasoning is proposed. The original contribution is on the explicit modelling and reasoning of collaborative feature relations within a dynamic context. A case study has been carried out to illustrate the interweaving feature relations in collaborative oil rig space management and the effective application of such relations modelled in design solution optimisation.
Multiple ant tracking with global foreground maximization and variable target proposal distribution
Motion and behavior analysis of social insects such as ants requires tracking many ants over time. This process is highly labor-intensive and tedious. Automatic tracking is challenging as ants often interact with one another, resulting in frequent occlusions that cause drifts in tracking. In addition, tracking many objects is computationally expensive. In this paper, we present a robust and efficient method for tracking multiple ants. We first prevent drifts by maximizing the coverage of foreground pixels at at global scale. Secondly, we improve speed by reducing markov chain length through dynamically changing the target proposal distribution for perturbed ant selection. Using a real dataset with ground truth, we demonstrate that our algorithm was able to improve the accuracy by 15% (resulting in 98% tracking accuracy) and the speed by 76%.
A Framework for Discrete Modeling of Juxtacrine Signaling Systems
Juxtacrine signaling is intercellular communication, in which the receptor of the signal (typically a protein) as well as the ligand (also typically a protein, responsible for the activation of the receptor) are anchored in the plasma membranes, so that in this type of signaling the activation of the receptor depends on direct contact between the membranes of the cells involved. Juxtacrine signaling is present in many important cellular events of several organisms, especially in the development process. We propose a generic formal model (a modeling framework) for juxtacrine signaling systems that is a class of dynamic discrete systems. It possesses desirable characteristics in a good modeling framework, such as: a) structural similarity with biological models, b) capacity of operating in different scales of time and c) capacity of explicitly treating both the events and molecular elements that occur in the membrane, and those that occur in the intracellular environment and are involved in the juxtacrine signaling process. We implemented this framework and used to develop a new discrete model for the neurogenic network and its participation in neuroblast segregation
DIY interface for enhanced service customization of remote IoT devices: a CoAP based prototype
DIY vision for the design of a smart and customizable world in the form of IoT demands the involvement of general public in its development process. General public lacks the technical depths for programming state-of-the-art prototyping and development kits. Latest IoT kits, for example, Intel Edison, are revolutionizing the DIY paradigm for IoT and more than ever a DIY intuitive programming interface is required to enable masses to interact with and customize the behavior of remote IoT devices on the Internet. This paper presents the novel implementation of such a system enabling general public to customize the behavior of remote IoT devices through a visual interface. The interface enables the visualization of the resources exposed by a remote CoAP device in the form of graphical virtual objects. The VOs are used to create service design through simple operations like drag-and-drop and properties settings. The design is maintained as an XML document, thus being easily distributable and recognizable. CoAP proxy acts as an operation client for the remote device and also provides communication link between the designer and the device. The paper presents the architecture, detailed design, and prototype implementation of the system using state-of-the-art technologies.
The Application and Research of Ontology Construction Technology
In the field of search, the application of ontology is an important research topic. Introduction of ontology technology in the retrieval system with massive data can make the searching results more comprehensive. However, now days the ontology is constructed by domain experts, and there are a lot of shortcomings, such as complex process, long time for the project, and difficulty to update. Thereby, in this paper, a method of semiautomaticly building ontology is proposed, after synthetically analyzing a variety of methods and techniques about it. The building process which is based on user interests, mines not only the concepts but also the potential relationships between concepts from the texts by the method of concepts clustering. On the basis of such research, an unique patent information retrieval system based on ontology has been completed.
Intelligent systems in accounting, finance and management: ISI journal and proceeding citations, and research issues from most-cited papers
This paper analyses the citations from Intelligent Systems in Accounting, Finance and Management that have occurred in ISI's Web of Knowledge in February 2010. I found roughly 1000 citations to the journal under 10 different journal name abbreviations, with roughly 25p of the citations occurring during 2008–2009, associated with 27 of the more frequently cited papers. Using that citation data, the H-index and the 40 (42 with ties) most-cited papers are presented. I found that ISI's new proceedings data appear to have a different citation pattern than ISI's journal citation data, resulting in citations to more sources, but fewer citations per source. I also examine the research methodologies and applications of the most-cited papers in an attempt to determine what areas have been cited most and where there are potential gaps in the research. Copyright © 2010 John Wiley & Sons, Ltd.
Anomalous Network Packet Detection Using Data Stream Mining
In recent years, significant research has been devoted to the development of Intrusion Detection Systems (IDS) able to detect anomalous computer network traffic indicative of malicious activity. While signature-based IDS have proven effective in discovering known attacks, anomaly-based IDS hold the even greater promise of being able to automatically detect previously undocumented threats. Traditional IDS are generally trained in batch mode, and therefore cannot adapt to evolving network data streams in real time. To resolve this limitation, data stream mining techniques can be utilized to create a new type of IDS able to dynamically model a stream of network traffic. In this paper, we present two methods for anomalous network packet detection based on the data stream mining paradigm. The first of these is an adapted version of the DenStream algorithm for stream clustering specifically tailored to evaluate network traffic. In this algorithm, individual packets are treated as points and are flagged as normal or abnormal based on their belonging to either normal or outlier clusters. The second algorithm utilizes a histogram to create a model of the evolving network traffic to which incoming traffic can be compared using Pearson correlation. Both of these algorithms were tested using the first week of data from the DARPA ’99 dataset with Generic HTTP, Shell-code and Polymorphic attacks inserted. We were able to achieve reasonably high detection rates with moderately low false positive percentages for different types of attacks, though detection rates varied between the two algorithms. Overall, the histogram-based detection algorithm achieved slightly superior results, but required more parameters than the clustering-based algorithm. As a result of its fewer parameter requirements, the clustering approach can be more easily generalized to different types of network traffic streams.
Machine learning in automated text categorization
The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.
Intelligent energy management agent for a parallel hybrid vehicle-part II: torque distribution, charge sustenance strategies, and performance results
This paper represents the second part of a two-part paper on development of an intelligent energy management agent (IEMA) for parallel hybrid vehicles. In this part, energy management strategies for the torque distribution and charge sustenance tasks are established and implemented. Driving situation awareness-based fuzzy rule bases are developed to make intelligent decisions on the power split function. A charge sustenance strategy is developed in parallel to maintain adequate reserves of energy in the storage device for supporting an extended range of driving. Simulation study is conducted for the proposed IEMA and performance results are analyzed to evaluate its viability as a possible solution to and an extendable framework for energy management for parallel hybrid electric vehicles.
Evolution of Adaptive Synapses: Robots with Fast Adaptive Behavior in New Environments
This paper is concerned with adaptation capabilities of evolved neural controllers. We propose to evolve mechanisms for parameter self-organization instead of evolving the parameters themselves. The method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment. In the experiments presented here, the performance of the robot is measured in environments that are different in significant ways from those used during evolution. The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that evolutionary adaptive controllers can adapt to environmental changes that involve new sensory characteristics (including transfer from simulation to reality and across different robotic platforms) and new spatial relationships.
Set-valued cooperative games with fuzzy payoffs. The fuzzy assignment game
In this paper we study cooperative games with fuzzy payoffs. The main advantage of the approach presented is the incorporation into the analysis of the problem of ambiguity inherent in many real-world collective decision situations. We propose extensions of core concepts which maintain the fuzzy nature of allocations, and lead to a more satisfactory study of the problem within the fuzzy context. Finally, we illustrate the extended core concepts and the approach to obtain the corresponding allocations through the analysis of assignment games with uncertain profits.
