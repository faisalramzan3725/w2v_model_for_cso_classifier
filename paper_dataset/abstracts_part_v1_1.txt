A new approach of 3D watermarking based on image segmentation
In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.
Attractor neural networks with activity-dependent synapses: The role of synaptic facilitation
We studied an autoassociative neural network with dynamic synapses which include a facilitating mechanism. We have developed a general mean-field framework to study the relevance of the different parameters defining the dynamics of the synapses and their influence on the collective properties of the network. Depending on these parameters, the network shows different types of behaviour including a retrieval phase, an oscillatory regime, and a non-retrieval phase. In the oscillatory phase, the network activity continously jumps between the stored patterns. Compared with other activity-dependent mechanisms such as synaptic depression, synaptic facilitation enhances the network ability to switch among the stored patterns and, therefore, its adaptation to external stimuli. A detailed analysis of our system reflects an efficient-more rapid and with lesser errors-network access to the stored information with stronger facilitation. We also present a set of Monte Carlo simulations confirming our analytical results.
A characterization of balanced episturmian sequences
It is well-known that Sturmian sequences are the non ultimately periodic sequences that are balanced over a 2-letter alphabet. They are also characterized by their complexity: they have exactly $(n+1)$ distinct factors of length $n$. A natural generalization of Sturmian sequences is the set of infinite episturmian sequences. These sequences are not necessarily balanced over a $k$-letter alphabet, nor are they necessarily aperiodic. In this paper, we characterize balanced episturmian sequences, periodic or not, and prove Fraenkel's conjecture for the special case of episturmian sequences. It appears that balanced episturmian sequences are all ultimately periodic and they can be classified in 3 families.
Exploring the space of a human action
One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions. In this paper, we explicitly identify three important sources of variability: (1) viewpoint, (2) execution rate, and (3) anthropometry of actors, and propose a model of human actions that allows us to investigate all three. Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space. We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data. A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action. The algorithm is applied to recognize several actions, and promising results have been obtained.
Generalized upper bounds on the minimum distance of PSK block codes
This paper generalizes previous optimal upper bounds on the minimum Euclidean distance for phase shift keying (PSK) block codes, that are explicit in three parameters: alphabet size, block length a ...
Applying BCMP multi-class queueing networks for the performance evaluation of hierarchical and modular software systems
Queueing networks with multiple classes of customers play a fundamental role for evaluating the performance of both software and hardware architectures. The main strength of product–form models, in particular of BCMP queueing networks, is that they combine a flexible formalism with efficient analysis techniques and solution algorithms. In this paper we provide an algorithm that starting from a high–level description of a system, and from the definition of its components in terms of interacting sub–systems, computes a multiple–class and multiple–chain BCMP queueing network. We believe that the strength of this approach is twofold. First, the modeller deals with simplified models, which are defined in a modular and hierarchical way. Hence, we can carry on sensitivity analysis that may easily include structural changes (and not only on the time parameters). Second, maintaining the product–form property allows one to derive the average system performance indices very efficiently. The paper also discusses the ...
A Push–Pull Class-C CMOS VCO
A CMOS oscillator employing differential transistor pairs working in Class-C in push-pull configuration is presented. The oscillator exhibits the same advantages enjoyed by complementary topologies on oscillators based on a single differential pair, while yielding a substantial power consumption reduction thanks to the Class-C operation. The phase-noise performance and the fundamental conditions required to keep the transistors working in Class-C are analyzed in detail. It is shown that, for an optimal performance, both nMOS and pMOS transistors should not be pushed into the deep triode region by the instantaneous resonator voltage, and a simple circuit solution is proposed to accommodate a large oscillation swing. A 0.18- μm CMOS prototype of the (voltage-controlled) oscillator displays an oscillation frequency from 6.09 to 7.50 GHz. The phase noise at 2-MHz offset is below -120 dBc/Hz with a power dissipation of 2.2 mW, for a state-of-the-art figure-of-merit ranging from 189 to 191 dBc/Hz.
On computability of pattern recognition problems
In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if consider only computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.#R##N##R##N#The task of pattern recognition is considered in conjunction with another learning problem — data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.
Manipulating biological and mechanical micro-objects using LIGA-microfabricated end-effectors
We first discuss some general aspects of micromanipulation and possible different approaches. Then, we present new results in the micromanipulation of mechanical and biological objects. The apparatus we use is a purposely developed workstation comprising macro- and micro-manipulators. The most innovative component of the workstation is a micro-gripper fabricated using LIGA technology and actuated by piezoelectric actuators. We describe the design, fabrication and performance of a few prototypes of LIGA micro-grippers. Results are presented which demonstrate the ability of the system to manipulate effectively both micro-mechanical and biological micro-objects.
An abundance of invariant polynomials satisfying the Riemann hypothesis
In 1999, Iwan Duursma defined the zeta function for a linear code as a generating function of its Hamming weight enumerator. It can also be defined for other homogeneous polynomials not corresponding to existing codes. If the homogeneous polynomial is invariant under the MacWilliams transform, then its zeta function satisfies a functional equation and we can formulate an analogue of the Riemann hypothesis. As far as existing codes are concerned, the Riemann hypothesis is believed to be closely related to the extremal property. In this article, we show there are abundant polynomials invariant by the MacWilliams transform which satisfy the Riemann hypothesis. The proof is carried out by explicit construction of such polynomials. To prove the Riemann hypothesis for a certain class of invariant polynomials, we establish an analogue of the Enestrom-Kakeya theorem.
Entity resolution with iterative blocking
Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an  iterative blocking framework  where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets.
Evaluating the accuracy of Java profilers
Performance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonly-used Java profilers ( xprof , hprof , jprofile, and yourkit ) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement.   This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly.   We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.
Robust group-of-picture architecture for video transmission over error-prone channels
In motion-compensated video-coding schemes, such as MPEG, an I frame is normally followed by several P frames and possibly B frames in a group-of-picture (GOP). In error-prone environments, errors happening in the previous frames in a GOP may propagate to all the following frames until the next I frame, which is the beginning of the next GOP. In this paper, we propose a novel GOP structure for robust transmission of MPEG video bitstream. By selecting the optimal position of the I frame in a GOP, robustness can be achieved without reducing any coding efficiency. Experimental results demonstrate the robustness of the proposed GOP structure.
Useful computations need useful numbers
Most of us have taken the exact rational and approximate numbers in our computer algebra systems for granted for a long time, not thinking to ask if they could be significantly better. With exact rational arithmetic and adjustable-precision floating-point arithmetic to precision limited only by the total computer memory or our patience, what more could we want for such numbers? It turns out that there is much more that can be done that permits us to obtain exact results more often, more intelligible results, approximate results guaranteed to have requested error bounds, and recovery of exact results from approximate ones.
Fast Content Aware Image Retargeting
This paper addresses the problem of retargeting, namely adapting large source images for effective viewing at a smaller size with possible applications to PDAs, or dynamic page layouts. Instead of extracting regions of interest for retargeting, the uninteresting parts are removed from the scene in Shai Avidan and Shamir, A. (2007). This is done by computing the RGB variance within non-overlapping 3times3 blocks and removing the block path with minimal variance cost using dynamic programming. It is shown that transformation to CIELAB space is more effective for visual interpretation of image content. The implementations are shown to be much faster than the seam carving approach of Shai Avidan and Shamir, A. (2007). Schemes are also presented for speeding up the seam carving scheme itself.
SCADDAR: an efficient randomized technique to reorganize continuous media blocks
Scalable storage architectures allow for the addition of disks to increase storage capacity and/or bandwidth. In its general form, disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired. Assuming random placement of blocks on multiple nodes of a continuous media server, our optimization objective is to redistribute a minimum number of media blocks after disk scaling. This objective should be met under two restrictions. First, uniform distribution and hence a balanced load should be ensured after redistribution. Second, the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation. We propose a technique that meets the objective, while we prove that it also satisfies both restrictions. The SCADDAR approach is based on using a series of REMAP functions which can derive the location of a new block using only its original location as a basis.
Importance sampling in Markovian settings
Rare event simulation for stochastic models of complex systems is still a great challenge even for Markovian models. We review results in importance sampling for Markov chains, provide new viewpoints and insights, and we pose some future research directions.
Identification of cold-induced genes in cereal crops and arabidopsis through comparative analysis of multiple EST sets
Freezing tolerance in plants is obtained during a period of low nonfreezing temperatures before the winter sets on, through a biological process known as cold acclimation. Cold is one of the major stress factors that limits the growth, productivity and distribution of plants, and understanding the mechanism of cold tolerance is therefore important for crop improvement. Expressed sequence tags (EST) analysis is a powerful, economical and time-efficient way of assembling information on the transcriptome. To date, several EST sets have been generated from cold-induced cDNA libraries from several different plant species. In this study we utilize the variation in the frequency of ESTs sampled from different cold-stressed plant libraries, in order to identify genes preferentially expressed in cold in comparison to a number of control sets. The species included in the comparative study are oat (Avena sativa), barley (Hordeum vulgare), wheat (Triticum aestivum), rice (Oryza sativa) and Arabidopsis thaliana. However, in order to get comparable gene expression estimates across multiple species and data sets, we choose to compare the expression of tentative ortholog groups (TOGs) instead of single genes, as in the normal procedure. We consider TOGs as preferentially expressed if they are detected as differentially expressed by a test statistic and up-regulated in comparison to all control sets, and/or uniquely expressed during cold stress, i.e., not present in any of the control sets. The result of this analysis revealed a diverse representation of genes in the different species. In addition, the derived TOGs mainly represent genes that are long-term highly or moderately expressed in response to cold and/or other stresses.
Real-time distributed computing
This position paper concerns itself with real-time safety critical distributed systems. It presents a computational model that is appropriate for this type of application and architecture. It then defines a resource allocations scheme based upon fixed priority scheduling. Such a scheme has the advantage (over purely static schedules) of supporting greater levels of flexibility and non-determinism, whilst still providing static guarantees of necessary timing behaviour (i.e. end-to-end deadlines through the systems). Priority based communication protocols are investigated, with possible future techniques reviewed.
On the impact of using volume as an independent variable for the solution of P-T fluid-phase equilibrium with equations of state
a b s t r a c t The constant pressure–temperature (P–T) flash plays an important role in the modelling of fluid-phase behaviour, and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure. We explore the relative merits of solving the P–T flash in two ensembles: mole numbers, pressure and temperature, in which each free-energy evaluation requires the use of a numerical solver; and mole numbers, volume and temperature, in which a direct evaluation of the free-energy is possible. We examine the performance of two algorithms, HELD (Helmholtz free energy Lagrangian dual), introduced in Pereira et al. (2012), and GILD (Gibbs free energy Lagrangian dual), introduced here, for the fluid-phase equilibria of 8 mixtures comprising up to 10 components, using two equations of state. While the reliability of both algorithms is comparable, the computational cost of HELD is consistently lower; this difference becomes increasingly pronounced as the number of components is increased. © 2014 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license
Semantic Processing of Natural Language Queries in the OntoNL Framework
The OntoNL Framework provides an architecture and re-usable components for automating as much as possible the building of natural language interfaces to information systems. In addition to the syntactic analysis components, OntoNL has semantic analysis components which exploit domain ontologies to provide better disambiguation of the user input. We present in this paper the algorithms used for semantic processing of the natural language queries, as well as an ontology-driven semantic relatedness measure developed for this purpose. We also present extensive evaluation results with different ontologies using human subjects.
A Comparison of Linear Keyword and Restricted Natural Language Data Base Interfaces for Novice Users
This study compares a linear keyword language interface and a restricted natural language interface for data retrieval by a novice user. The comparison focuses on the effect of different data base interfaces on user performance as measured by query correctness and query writing time in a query writing task across varying query types and training levels. To accomplish this objective, a laboratory experiment was conducted using a split-plot factorial design using two between-subjects factors and one within-subjects factor. The results indicate that the restricted natural language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time.
Reactive Virtual Position-Based Routing in Wireless Sensor Networks
Virtual position-based routing protocols have many attractive characteristics for wireless sensor networks. Typically, such protocols use a proactive scheme for updating routing tables. Because sensor networks can have very low data rate, sending periodic beacons to update routing tables can be very expensive. Instead, reactive approaches might be more appropriate in such scenarios. MANET-inspired reactive routing protocols do not scale well because of the effort in the order of O(n) for each routing information update. In this paper, we present Reactive Virtual Cord Protocol (RVCP), a data-centric reactive virtual position based routing protocol for use in sensor networks. Route discovery is directed towards the destination and hence there is no need to flood the entire network to discover a route. Our approach is based on Virtual Cord Protocol (VCP), an efficient, virtual relative position based routing protocol that also provides support for data management as known from typical Distributed Hash Table (DHT) services. To minimize the end-to-end delay and energy consumption, we used adaptive techniques for the development of RVCP.
A note on the complexity of scheduling coupled tasks on a single processor
This paper considers a problem of coupled task scheduling on one processor, where all processing times are equal to 1, the gap has exact length h, precedence constraints are strict and the criterion is to minimise the schedule length. This problem is introduced e.g. in systems controlling radar operations. We show that the general problem is NP-hard.
Novel Weighting-Delay-Based Stability Criteria for Recurrent Neural Networks With Time-Varying Delay
In this paper, a weighting-delay-based method is developed for the study of the stability problem of a class of recurrent neural networks (RNNs) with time-varying delay. Different from previous results, the delay interval [0, d(t)] is divided into some variable subintervals by employing weighting delays. Thus, new delay-dependent stability criteria for RNNs with time-varying delay are derived by applying this weighting-delay method, which are less conservative than previous results. The proposed stability criteria depend on the positions of weighting delays in the interval [0, d(t)], which can be denoted by the weighting-delay parameters. Different weighting-delay parameters lead to different stability margins for a given system. Thus, a solution based on optimization methods is further given to calculate the optimal weighting-delay parameters. Several examples are provided to verify the effectiveness of the proposed criteria.
Cross-Language Information Retrieval
Search for information is no longer exclusively limited within the native language of the user, but is more and more extended to other languages. This gives rise to the problem of cross-language information retrieval (CLIR), whose goal is to find relevant information written in a different language to a query. In addition to the problems of monolingual information retrieval (IR), translation is the key problem in CLIR: one should translate either the query or the documents from a language to another. However, this translation problem is not identical to full-text machine translation (MT): the goal is not to produce a human-readable translation, but a translation suitable for finding relevant documents. Specific translation methods are thus required. The goal of this book is to provide a comprehensive description of the specifi c problems arising in CLIR, the solutions proposed in this area, as well as the remaining problems. The book starts with a general description of the monolingual IR and CLIR problems. Different classes of approaches to translation are then presented: approaches using an MT system, dictionary-based translation and approaches based on parallel and comparable corpora. In addition, the typical retrieval effectiveness using different approaches is compared. It will be shown that translation approaches specifically designed for CLIR can rival and outperform high-quality MT systems. Finally, the book offers a look into the future that draws a strong parallel between query expansion in monolingual IR and query translation in CLIR, suggesting that many approaches developed in monolingual IR can be adapted to CLIR. The book can be used as an introduction to CLIR. Advanced readers can also find more technical details and discussions about the remaining research challenges in the future. It is suitable to new researchers who intend to carry out research on CLIR.
Best case energy analysis of localized euclidean minimum spanning tree based multicasting in ad hoc and sensor networks
I consider the known localized multicast protocol MSTEAM and derive the energy consumed by the multicast tree constructed by this protocol in the best case. Moreover, I show that the length of multicast links connecting into a multicast branch can not be bounded from above. For typical wireless networks where links have a limited communication range, however, I can show that asymptotically the relation between the derived best case energy consumption of MSTEAM and a known lower bound on multicast energy consumption is limited by a factor of 2.
DQAINF: an algorithm for automatic integration of infinite oscillating tails
We describe an automatic quadrature routine which is specifically designed for real functions having a certain type of infinite oscillating tails. The algorithm is designed to integrate a vector function over an infinite interval. A FORTRAN implementation of the algorithm is included.
An Interoperability Framework for Pan-European E-Government Services (PEGS)
Interoperability between public administrations receives nowadays a lot of attention. Also in the European Union interworking is high on the priority list, but the challenges to achieve the European administrative space is enormous. Many research projects are undertaken, especially in the domain of semantic interoperability. Many of these efforts seem to start from a technical solution rather than from an actual business problem. By taking a narrow view on the problem space, they only promise limited support for the many challenges in the domain of interoperability and innovation of e-government services. In this paper we present a business driven approach that looks promising in enabling entire classes of interoperability solutions
Performance optimization of interference-limited multihop networks
The performance of a multihop wireless network is typically affected by the interference caused by transmissions in the same network. In a statistical fading environment, the interference effects become harder to predict. Information sources in a multihop wireless network can improve throughput and delay performance of data streams by implementing interference-aware packet injection mechanisms. Forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference. In this paper, throughput and delay performance in interference-limited multihop networks is analyzed. Using nonlinear probabilistic hopping models, waiting times which jointly optimize throughput and delay performances are derived. Optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations. The resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network.
Action Reaction Learning: Automatic Visual Analysis and Synthesis of Interactive Behaviour
We propose Action-Reaction Learning as an approach for analyzing and synthesizing human behaviour. This paradigm uncovers causal mappings between past and future events or between an action and its reaction by observing time sequences. We apply this method to analyze human interaction and to subsequently synthesize human behaviour. Using a time series of perceptual measurements, a system automatically discovers correlations between past gestures from one human participant (action) and a subsequent gesture (reaction) from another participant. A probabilistic model is trained from data of the human interaction using a novel estimation technique, Conditional Expectation Maximization (CEM). The estimation uses general bounding and maximization to monotonically find the maximum conditional likelihood solution. The learning system drives a graphical interactive character which probabilistically predicts a likely response to a user's behaviour and performs it interactively. Thus, after analyzing human interaction in a pair of participants, the system is able to replace one of them and interact with a single remaining user.
Circuit Delay Models and Their Exact Computation Using Timed Boolean Functions
We propose a general circuit delay model that unifies all previous delay models, e.g. floating, viability, and transition delays, and models introduced in this paper, e.g. delays by sequences of vectors and minimum delays. Then, we formulate the computation of the exact circuit delays, under both bounded and unbounded gate delay models, as a mixed Boolean linear programming using a new formulation technique, called Timed Boolean Function. Next, we compute the exact delays of combinational circuits for transition delay and delay by sequences of vectors. We show that delays by sequences of vectors and floating (or viability) delays are invariant under both bounded and unbounded gate delay models. Finally, we address the effect of gate delay lower bounds on delays of circuits. We demonstrate the effectiveness of the method by giving exact delay results for all ISCAS benchmark circuits (except C6188).
An anisotropic evolution formulation applied in 2-D unwrapping of discontinuous phase surfaces
In this paper, a new method to reconstruct piecewise continuous phase estimates using inphase and quadrature components acquired from interferometry measurements is derived and discussed. The method, based on the concept of anisotropic evolution formulations, is shown to be far less noise sensitive than similar methods operating on modulo-mapped data (i.e., traditional phase unwrapping methods). The method is able to produce reliable phase estimates from data containing complex sheared structures in combination with high noise content without relying on user-defined weights.
A score function of splitting band for two-band speech model
Two-band speech model which assumes lower band is a quasi-periodic component and upper band is a non-periodic component is widely used due to its natural and simple framework. In this paper, a score function is defined for splitting lower and upper band of two-band speech model and estimation method of band-splitting frequency which is the boundary of the two bands is proposed. The score function is calculated for each harmonic frequency using the normalized autocorrelation function of the time signal corresponding to the each sub-band divided by the given frequency. By using the score function, tracking technique is applied to the band-splitting frequency estimation procedure to reflect the continuity between neighboring frames. Experimental tests confirm that the proposed score function is effective for estimation of the band-splitting frequency and produces better results compared with the previous other methods.
The Reliability Study of the Single Hydraulic Prop Based on Finite Element Analysis
In allusion to the reliability, which exists in the parametric design and optimizing process of the single hydraulic prop, this paper presents the new method in comparison with the traditional calculating and checking method for the reliability. The geometry model of the hydraulic prop is built firstly based on the 3D software, then analyzed and optimized by the finite element software-ANSYS. Results show that the method presented for the reliability is efficient and accurate.
Fuzzy Capacitated Location-allocation Problem with Minimum Risk Criteria
Based on credibility theory, a new class of two-stage minimum risk location-allocation model is first proposed. Then we deal with the approximation of the location and allocation problem after that, a hybrid algorithm, which integrates the approximation approach, neural network and simulated annealing, is designed to solve the proposed location-allocation problem, and a numerical example is provided to test the effectiveness of the hybrid algorithm
Towards End User Service Composition
The popularity of service oriented computing (SOC) brings a large number of distributed, well-encapsulated and reusable services all over Internet, and makes it possible to create value-added services by means of service composition. Current composition styles are too professional to those end users when building their own applications. Actually, the end user would prefer rapidly discovering the best-of-breed services to assemble as well as visually personalizing the presentation to enjoy rich experiences. We propose an end user service composition approach for reducing the composition complexity and difficulty from the end user perspective. In our approach, similar candidate services are aggregated together as a unified resource, whose wide QoS spectrum can be easily manipulated by the end users to satisfy their requirements. Then they can personalize the services and, the composition occurs only at the presentation layer. The main contributions of the approach are: (i) enabling the end users to personalize the composite application with more powerful presentation; (ii) supporting the end users to dynamically customize the service composition in terms of QoS; (iii) alleviating the end users from the time-consuming task of selecting service to compose.
Optimal detection of functional connectivity from high-dimensional EEG synchrony data.
article i nfo Computing phase-locking values between EEG signals is a popular method for quantifying functional connectivity. However, this method involves large-scale, high-resolution datasets, which impose a serious multiple testing problem. Standard multiple testing methods fail to exploit the information from the complex dependence structure that varies across hypotheses in spectral, temporal, and spatial dimensions and result in a severe loss of power. They tend to control the false positives at the cost of hiding true positives. We introduce a new approach, called optimal discovery procedure (ODP) for identifying synchrony that is statistically significant. ODP maximizes the number of true positives for a given number of false positives, and thus offers a theoretical optimum for detecting significant synchrony in a multiple testing situation. We demonstrate the utility of this method with PLV data obtained from a visual search study. We also present simulation analysis to confirm the validity and relevance of using ODP in comparison with the standard FDR method for given configurations of true synchrony. We also compare the effectiveness of ODP with our previously published investigation of hierarchical FDR method (Singh and Phillips, 2010).
starBase v2.0: decoding miRNA-ceRNA, miRNA-ncRNA and protein–RNA interaction networks from large-scale CLIP-Seq data
Although microRNAs (miRNAs), other non-coding RNAs (ncRNAs) (e.g. lncRNAs, pseudogenes and circRNAs) and competing endogenous RNAs (ceRNAs) have been implicated in cell-fate determination and in various human diseases, surprisingly little is known about the regulatory interaction networks among the multiple classes of RNAs. In this study, we developed starBase v2.0 (http://starbase.sysu. edu.cn/) to systematically identify the RNA–RNA and protein–RNA interaction networks from 108 CLIP-Seq (PAR-CLIP, HITS-CLIP, iCLIP, CLASH) data sets generated by 37 independent studies. By analyzing millions of RNA-binding protein binding sites, we identified � 9000 miRNA-circRNA, 16 000 miRNApseudogene and 285 000 protein–RNA regulatory relationships. Moreover, starBase v2.0 has been updated to provide the most comprehensive CLIP-Seq experimentally supported miRNA-mRNA and miRNAlncRNA interaction networks to date. We identified � 10 000 ceRNA pairs from CLIP-supported miRNA target sites. By combining 13 functional genomic annotations, we developed miRFunction and ceRNAFunction web servers to predict the function of miRNAs and other ncRNAs from the miRNAmediated regulatory networks. Finally, we developed interactive web implementations to provide visualization, analysis and downloading of the aforementioned large-scale data sets. This study will greatly expand our understanding of ncRNA functions and their coordinated regulatory networks.
A Low Phase Noise 100MHz Silicon BAW Reference Oscillator
The paper presents a temperature compensated 100MHz reference oscillator based on a capacitive silicon Bulk Acoustic Wave (BAW) resonator interfaced with a CMOS amplifier. The resonator is optimized for high quality factor (92000) and low impedance. The CMOS IC comprises of a trans-impedance amplifier to sustain oscillations and an oven control mechanism for temperature control. A phase noise floor of ?136dBc/Hz was measured for the oscillator and the temperature drift of frequency was measured to be 56ppm over 100°C.
Performance Bounds for MLSD Reception of OFDM Signals in Fast Fading
For OFDM systems in fast fading, it is difficult to obtain a closed form expression for the OFDM symbol error probability. Thus, tight analytical bounds on actual performance are extremely useful for performance prediction and verification. Additionally, the structure of the bound provides insight into system behavior. A new expurgated 2-dimensional union bound is proposed and applied to an OFDM system in fast fading. This bound becomes extremely tight as the rate of fading L increases. Performance comparison of the new bound to a lower bound on a known expurgated union bound demonstrates a gain of up to 1.5 dB at P(e) = 10 2 for channel implicit diversity order of L = 4. A new simulated upper bound that uses a reduced state vector in a modified trellis search algorithm and a simulated lower bound that assumes limited knowledge of the ICI are also presented. Both bounds are derived from a "time-varying" finite state machine model of the received signal. Performance results for these bounds are extremely tight for small to large values of N, where N is the number of OFDM signal tones. Also, the reduction in computational complexity achieved for N = 512 is from 2 512  to 2 6  for both bounds.
Creating GIS-based spatial interaction models for retail centres in Jeddah City
Spatial interaction models are used today in facilities planning research for predicting and for allocating flows of demand between origin and destination areas based on the attractiveness of each facility and based on the distance between facilities and demand areas. These models have been adapted to a wide range of application areas including predicting flows of people to shops, offices, schools, and hospitals. The aim of this paper is to use GIS for producing spatial interaction models for two retail centres Jeddah City, Saudi Arabia. These models are created using ArcGIS software and using the interaction function which is available within the network analysis module. To produce these models, detailed geo-database was created that covers location of retail centres, the capacity of each centre, the size of centres demand at the study area, and road network coverage for Jeddah City. The created models can be used by city planners for identifying areas of the city that are poorly served by existing retail centres. In addition, these models can be used to define the impacts of expanding retail supply and or retail demand at the study area.
Surface models of tube trees
This paper describes a new method for generating surfaces of branching tubular structures with given center-lines and radii. As the centerlines are not straight lines, the cross-sections are not parallel and well-known algorithms for surface tiling from parallel cross-sections cannot be used. Nonparallel cross-sections can be tiled by means of the maximal-disc interpolation method; special methods for branching-structures modeling by means of convolution surfaces produce excellent results, but these methods are more complex than our approach. The proposed method tiles nonparallel circular cross-sections and constructs a topologically-correct surface mesh. The method is not artifact-free, but it is fast and simple. The surface mesh serves as a data representation of a vessel tree suitable for real-time virtual reality operation planning and operation support within a medical application. Proposed method extracts a "classical" polygonal representation, which can be used in common surface-oriented graphic accelerators
Node-to-Set Disjoint-path Routing in Metacube
The metacube interconnection network introduced a few years ago has some very interesting properties: it has a short diameter similar to the hypercube, and its degree is much lower than that of a hypercube of the same size. In this paper, we describe an efficient algorithm for finding disjoint paths between one source node and at most m+k target nodes in a metacube MC(k, m) excluding MC(*,1), MC(2,2), MC(3,2) and MC(3,3). We show that we can find m+k disjoint paths between the source node and the m+k targets of length at most metacube diameter plus (k+4) with time complexity of order of metacube degree times its diameter.
Systematic error in the organization of physical action
Current views of the control of complex, purposeful movements acknowledge that organizational processes must reconcile multiple concerns. The central priority is of course accomplishing the actor’s goal. But in specifying the manner in which this occurs, the action plan must accommodate such factors as the interaction of mechanical forces associated with the motion of a multilinked system (classical mechanics) and, in many cases, intrinsic bias toward preferred movement patterns, characterized by so-called “coordination dynamics.” The most familiar example of the latter is the symmetry constraint, where spatial trajectories and/or temporal landmarks (e.g., reversal points) of concurrentlymoving body segments (limbs, digits, etc.) exhibit mutual attraction. The natural coordination tendencies that emerge through these constraints can facilitate or hinder motor control, depending on the degree of congruency with the desired movement pattern. Motor control theorists have long recognized the role of classical mechanics in theories of movement organization, but an appreciation of the importance of intrinsic interlimb bias has been gained only recently. Although detailed descriptions of temporal coordination dynamics have been provided, systematic attempts to identify additional salient dimensions of interlimb constraint have been lacking. We develop and implement here a novel method for examining this problem by exploiting two robust principles of psychomotor behavior, the symmetry constraint and the Two-Thirds Power Law. Empirical evidence is provided that the relative spatial patterns of concurrently moving limbs are naturally constrained in much the same manner as previously identified temporal constraints and, further, that apparent velocity interference is an indirect, secondary consequence of primary spatial assimilation. The theoretical implications of spatial interference are elaborated with respect to movement organization and motor learning. The need to carefully consider the appropriate dimensions with which to characterize coordination dynamics is also discussed. © 2001 Cognitive Science
Modeling the performance of evolutionary algorithms on the root identification problem: A case study with pbil and chc algorithms
The availability of a model to measure the performance of evolutionary algorithms is very important, especially when these algorithms are applied to solve problems with high computational requirements. That model would compute an index of the quality of the solution reached by the algorithm as a function of run-time. Conversely, if we fix an index of quality for the solution, the model would give the number of iterations to be expected. In this work, we develop a statistical model to describe the performance of PBIL and CHC evolutionary algorithms applied to solve the root identification problem. This problem is basic in constraint-based, geometric parametric modeling, as an instance of general constraint-satisfaction problems. The performance model is empirically validated over a benchmark with very large search spaces.
Towards a Probabilistic Calculus for Mobile Ad Hoc Networks
In this paper we present a probabilistic calculus for formally modeling and reasoning about Mobile Ad Hoc Networks (MANETs) with unreliable connections and mobility of nodes. In our calculus, a MANET node can locally broadcast messages to a group of nodes within its physical transmission range. The group probability is also introduced since two distinct nodes within different groups should receive messages from the same sender with different possibilities. Our calculus naturally captures essential features of MANETs, i.e., local broadcast, mobility and probability. Moreover, we give a formal operational semantics of the calculus in terms of the labeled transition system and define the notion of open bisimulation. Finally, we illustrate our calculus with a toy example.
Slow motion replay of video sequences using fractal zooming
Slow motion replay is a special effect used in the video entertainment field. It consists in a presentation of a video scene at a rate display lower than the original. Already consolidated as a commercial feature of analog video players, today slow motion is likely to be extended to the digital environment. Purpose of this paper is to present a technique combining fractals (I. F. S.) and wavelets to obtain a subjectively pleasant zoom and slow motion of digital video sequences. Active scene detection and post processing techniques are used to reduce computational cost and improve visual quality respectively. This study shows that the proposed technique produces better results than the state of the art techniques based either on data replication or classical interpolation.
Joint Semiblind Frequency Offset and Channel Estimation for Multiuser MIMO–OFDM Uplink
A semiblind method is proposed for simultaneously estimating the carrier frequency offsets (CFOs) and channels of an uplink multiuser multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) system. By incorporating the CFOs into the transmitted symbols and channels, the MIMO-OFDM with CFO is remodeled into an MIMO-OFDM without CFO. The known blind method for channel estimation (Zeng and Ng in 2004) (Y. H. Zeng and T. S. Ng, ldquoA semi-blind channel estimation method for multi-user multi-antenna OFDM systems,rdquo IEEE Trans. Signal Process., vol. 52, no. 5, pp. 1419-1429, May 2004.) is then directly used for the remodeled system to obtain the shaped channels with an ambiguity matrix. A pilot OFDM block for each user is then exploited to resolve the CFOs and the ambiguity matrix. Two dedicated pilot designs, periodical and consecutive pilots, are discussed. Based on each pilot design and the estimated shaped channels, two methods are proposed to estimate the CFOs. As a result, based on the second-order statistics (SOS) of the received signal and one pilot OFDM block, the CFOs and channels are found simultaneously. Finally, a fast equalization method is given to recover the signals corrupted by the CFOs.
The cluster density of a distributed clustering algorithm in ad hoc networks
Given is a wireless multihop network whose nodes are randomly distributed according to a homogeneous Poisson point process of density /spl rho/ (in nodes per unit area). The network employs Basagni's distributed mobility-adaptive clustering (DMAC) algorithm to achieve a self-organizing network structure. We show that the cluster density, i.e., the expected number of cluster- heads per unit area, is /spl rho//sub c/= /spl rho//spl divide/(1+/spl mu//spl divide/2), where /spl mu/ denotes the expected number of neighbors of a node. Consequently, a clusterhead is expected to incorporate half of its neighboring nodes into its cluster. This result also holds in a scenario with mobile nodes and serves as a bound for inhomogeneous spatial node distributions.
Fault-tolerant ring embedding in faulty arrangement graphs
The arrangement graph A/sub n,k/, which is a generalization of the star graph (n-t=1), presents more flexibility than the star graph in adjusting the major design parameters: number of nodes, degree, and diameter. Previously the arrangement graph has proven hamiltonian. In this paper we further show that the arrangement graph remains hamiltonian even if it is faulty. Let |F/sub e/| and |F/sub v/| denote the numbers of edge faults and vertex faults, respectively. We show that A/sub n,k/ is hamiltonian when (1) (k=2 and n-k/spl ges/4, or k/spl ges/3 and n-k/spl ges/4+[k/2]), and |F/sub e/|/spl les/k(n-k-2)-1, or (2) k/spl ges/2, n-k/spl ges/2+[k/2], and |F/sub e/|/spl les/k(n-k-3)-1, or (3) k/spl ges/2, n-k/spl ges/3, and |F/sub 3/|/spl les/k.
The Local Structure of a Bipartite Distance-regular Graph
In this paper, we consider a bipartite distance-regular graph ?= (X, E) with diameter d? 3. We investigate the local structure of? , focusing on those vertices with distance at most 2 from a given vertex x. To do this, we consider a subalgebra R=R(x) ofMat0307a0x.gif X(C), where 0307a1x.gifX denotes the set of vertices in X at distance 2 from x. R is generated by matrices A, 0307a2x.gif J, and 0307a3x.gif D defined as follows. For all y, z? 0307a4x.gif X, the (y,z )-entry of A is 1 if y, z are at distance 2, and 0 otherwise. The (y, z)-entry of 0307a5x.gif J equals 1, and the (y,z )-entry of 0307a6x.gif D equals the number of vertices of X adjacent to each ofx , y, and z. We show that R is commutative and semisimple, with dimension at least 2. We assume thatdimR is one of 2, 3, or 4, and explore the combinatorial implications of this. We are motivated by the fact that if ? has a Q-polynomial structure, thendimR? 4.
Prerequesites for symbiotic brain-machine interfaces
Recent advancements in the neuroscience and engineering of Brain-Machine Interfaces are providing a blueprint for how new co-adaptive designs based on reinforcement learning change the nature of a user's ability to accomplish tasks that were not possible using static methodologies. By designing adaptive controls and artificial intelligence into the neural interface, computers can become active assistants in goal-directed behavior and further enhance human performance. This paper presents a set of minimal prerequisites that enable a cooperative symbiosis and dialogue between biological and artificial systems.
Computation and analysis of natural compliance in fixturing and grasping arrangements
This paper computes and analyzes the natural compliance of fixturing and grasping arrangements. Traditionally, linear-spring contact models have been used to determine the natural compliance of multiple contact arrangements. However, these models are not supported by experiments or elasticity theory. We derive a closed-form formula for the stiffness matrix of multiple contact arrangements that admits a variety of nonlinear contact models, including the well-justified Hertz model. The stiffness matrix formula depends on the geometrical and material properties of the contacting bodies and on the initial loading at the contacts. We use the formula to analyze the relative influence of first- and second-order geometrical effects on the stability of multiple contact arrangements. Second-order effects, i.e., curvature effects, are often practically beneficial and sometimes lead to significant grasp stabilization. However, in some contact arrangements, curvature has a dominant destabilizing influence. Such contact arrangements are deemed stable under an all-rigid body model but, in fact, are unstable when the natural compliance of the contacting bodies is taken into account. We also consider the combined influence of curvature and contact preloading on stability. Contrary to conventional wisdom, under certain curvature conditions, higher preloading can increase rather than decrease grasp stability. Finally, we use the stiffness matrix formula to investigate the impact of different choices of contact model on the assessment of the stability of multiple contact arrangements. While the linear-spring model and the more realistic Hertz model usually lead to the same stability conclusions, in some cases, the two models lead to different stability results.
Adapting Information Theoretic Clustering to Binary Images
We consider the problem of finding points of interest along local curves of binary images. Information theoretic vector quantization is a clustering algorithm that shifts cluster centers towards the modes of principal curves of a data set. Its runtime characteristics, however, do not allow for efficient processing of many data points. In this paper, we show how to solve this problem when dealing with data on a 2D lattice. Borrowing concepts from signal processing, we adapt information theoretic clustering to the quantization of binary images and gain significant speedup.
Construction of robust prognostic predictors by using projective adaptive resonance theory as a gene filtering method
Motivation: For establishing prognostic predictors of various diseases using DNA microarray analysis technology, it is desired to find selectively significant genes for constructing the prognostic model and it is also necessary to eliminate non-specific genes or genes with error before constructing the model.#R##N##R##N#Results: We applied projective adaptive resonance theory (PART) to gene screening for DNA microarray data. Genes selected by PART were subjected to our FNN-SWEEP modeling method for the construction of a cancer class prediction model. The model performance was evaluated through comparison with a conventional screening signal-to-noise (S2N) method or nearest shrunken centroids (NSC) method. The FNN-SWEEP predictor with PART screening could discriminate classes of acute leukemia in blinded data with 97.1% accuracy and classes of lung cancer with 90.0% accuracy, while the predictor with S2N was only 85.3 and 70.0% or the predictor with NSC was 88.2 and 90.0%, respectively. The results have proven that PART was superior for gene screening.#R##N##R##N#Availability: The software is available upon request from the authors.#R##N##R##N#Contact: honda@nubio.nagoya-u.ac.jp
The relationship between minimum entropy control and risk-sensitive control for time-varying systems
The connection between minimum entropy control and risk-sensitive control for linear time-varying systems is investigated. For time-invariant systems, the entropy functional and the linear exponential quadratic Gaussian cost are the same. In this paper, it is shown that this is not true for general time varying systems. It does hold, however, when the system admits a state-space representation.
A Novel Hybrid Parallel-Prefix Adder Architecture With Efficient Timing-Area Characteristic
Two-operand binary addition is the most widely used arithmetic operation in modern datapath designs. To improve the efficiency of this operation, it is desirable to use an adder with good performance and area tradeoff characteristics. This paper presents an efficient carry-lookahead adder architecture based on the parallel-prefix computation graph. In our proposed method, we define the notion of triple-carry-operator, which computes the generate and propagate signals for a merged block which combines three adjacent blocks. We use this in conjunction with the classic approach of the carry-operator to compute the generate and propagate signals for a merged block combining two adjacent blocks. The timing-driven nature of the proposed design reduces the depth of the adder. In addition, we use a ripple-carry type of structure in the nontiming critical portion of the parallel-prefix computation network. These techniques help produce a good timing-area tradeoff characteristic. The experimental results indicate that our proposed adder is significantly faster than the popular Brent-Kung adder with some area overhead. On the adder hand, the proposed adder also shows marginally faster performance than the fast Kogge-Stone adder with significant area savings.
Control of Doubly-Fed Induction Generator System Using PIDNNs
An intelligent control stand-alone doubly-fed induction generator (DFIG) system using proportional-integral-derivative neural network (PIDNN) is proposed in this study. This system can be applied as a stand-alone power supply system or as the emergency power system when the electricity grid fails for all sub-synchronous, synchronous and super-synchronous conditions. The rotor side converter is controlled using the field-oriented control to produce three-phase stator voltages with constant magnitude and frequency at different rotor speeds. Moreover, the stator side converter, which is also controlled using field-oriented control, is primarily implemented to maintain the magnitude of the DC-link voltage. Furthermore, the intelligent PIDNN controller is proposed for both the rotor and stator side converters to improve the transient and steady-state responses of the DFIG system for different operating conditions. Both the network structure and on-line learning algorithm are introduced in detail. Finally, the feasibility of the proposed control scheme is verified through experimentation.
Password-based tripartite key exchange protocol with forward secrecy
A tripartite authenticated key agreement protocol is designed for three entities to communicate securely over an open network particularly with a shared key. Password-authenticated key exchange (PAKE) allows the participants to share a session key using a human memorable password only. In this paper, A password-based authenticated tripartite key exchange protocol(3-PAKE) is presented in the standard model. The security of the protocol is reduced to theDecisional Bilinear Diffie-Hellman (DBDH) problem, and the protocol provides not only the properties of forward secrecy, but also resistance against known key attacks. The proposed protocol is more efficient than the similar protocols in terms of both communication and computation.
Evaluation of high-altitude balloons as a learning technology
The utility of high-altitude balloons as a learning technology to facilitate education in several disciplines is considered in this paper. The role that a high-altitude balloon can play as a learning technology is discussed and its utility in this role is considered. The need for a formal design framework for high-altitude ballooning is also discussed. A framework for the assessment of high-altitude ballooning in supporting an undergraduate-level course is evaluated and an assessment using this framework is conducted. The paper concludes with a discussion of techniques that can be used to broaden access to high-altitude ballooning in education.
Automatic Discovery of Action Taxonomies from Multiple Views
We present a new method for segmenting actions into primitives and classifying them into a hierarchy of action classes. Our scheme learns action classes in an unsupervised manner using examples recorded by multiple cameras. Segmentation and clustering of action classes is based on a recently proposed motion descriptor which can be extracted efficiently from reconstructed volume sequences. Because our representation is independent of viewpoint, it results in segmentation and classification methods which are surprisingly efficient and robust. Our new method can be used as the first step in a semi-supervised action recognition system that will automatically break down training examples of people performing sequences of actions into primitive actions that can be discriminatingly classified and assembled into high-level recognizers.
Dr.VIS: a database of human disease-related viral integration sites
Viral integration plays an important role in the development of malignant diseases. Viruses differ in preferred integration site and flanking sequence. Viral integration sites (VIS) have been found next to oncogenes and common fragile sites. Understanding the typical DNA features near VIS is useful for the identification of potential oncogenes, prediction of malignant disease development and assessing the probability of malignant transformation in gene therapy. Therefore, we have built a database of human disease-related VIS (Dr.VIS, http://www.scbit.org/dbmi/drvis) to collect and maintain human disease-related VIS data, including characteristics of the malignant disease, chromosome region, genomic position and viral–host junction sequence. The current build of Dr.VIS covers about 600 natural VIS of 5 oncogenic viruses representing 11 diseases. Among them, about 200 VIS have viral–host junction sequence.
Research on Internet marketing relationship model
The present models about Internet marketing, to a certain extent, have some limits, which cannot systematically reveals the process and characteristics of Internet marketing. Based on the theories of traditional marketing and Internet marketing, the paper builds the model of Internet marketing relationship that describes consumer's purchase decision process and firm's Internet marketing process, and correlation among consumer, firm, and bank and logistics firm. Through systematically analyzing the two processes, the model reveals intrinsic characteristics and essences of Internet marketing that are all different from traditional marketing. The model provides a researchful platform for the researchers, and a fundamental basis for further researching Internet marketing.
Expanded rectangles: a new VLSI data structure
A data structure derived from corner stitching which allows efficient representation of VLSI layouts is presented. While each entry in the expanded rectangle database is larger than the corresponding corner-stitched entry, generally fewer entries are required to represent the same VLSI layout. The data structure has two important features: first, the VLSI design is represented as a slicing structure in which each slice contains a portion of the solid material; and second, corner stitches are used to provide two-dimensional nearness information. Initial measurements indicate that expanded rectangles is a viable data structure for use in a complete VLSI layout system. >
Predicting opponent actions by observation
In competitive domains, the knowledge about the opponent can give players a clear advantage. This idea lead us in the past to propose an approach to acquire models of opponents, based only on the observation of their input-output behavior. If opponent outputs could be accessed directly, a model can be constructed by feeding a machine learning method with traces of the opponent. However, that is not the case in the Robocup domain. To overcome this problem, in this paper we present a three phases approach to model low-level behavior of individual opponent agents. First, we build a classifier to label opponent actions based on observation. Second, our agent observes an opponent and labels its actions using the previous classifier. From these observations, a model is constructed to predict the opponent actions. Finally, the agent uses the model to anticipate opponent reactions. In this paper, we have presented a proof-of-principle of our approach, termed OMBO (Opponent Modeling Based on Observation), so that a striker agent can anticipate a goalie. Results show that scores are significantly higher using the acquired opponent's model of actions.
Web accessibility compliance of government web sites in Korea
This paper introduces Korean web accessibility activities, such as relational laws, ordinances, policies, guidelines. It also presents analytical result of the investigation on web-contents accessibilities of the 39 Korean government agencies. The result shows that only one agency provides web contents satisfying all the minimum requirements, while 97% of the agencies does not satisfy all the minimum requirements. Unfortunately, 6 agencies do not satisfy any.
Free-Viewpoint Video Sequences: a New Challenge for Objective Quality Metrics
Free-viewpoint television is expected to create a more natural and interactive viewing experience by providing the ability to interactively change the viewpoint to enjoy a 3D scene. To render new virtual viewpoints, free-viewpoint systems rely on view synthesis. However, it is known that most objective metrics fail at predicting perceived quality of synthesized views. Therefore, it is legitimate to question the reliability of commonly used objective metrics to assess the quality of free-viewpoint video (FVV) sequences. In this paper, we analyze the performance of several commonly used objective quality metrics on FVV sequences, which were synthesized from decompressed depth data, using subjective scores as ground truth. Statistical analyses showed that commonly used metrics were not reliable predictors of perceived image quality when different contents and distortions were considered. However, the correlation improved when considering individual conditions, which indicates that the artifacts produced by some view synthesis algorithms might not be correctly handled by current metrics.
Dealing with Hardware in Embedded Software: A General Framework Based on the Devil Language
Writing code that talks to hardware is a crucial part of any embedded project. Both productivity and quality are needed, but some flaws in the traditional development process make these requirements difficult to meet.  We have recently introduced a new approach of dealing with hardware, based on the Devil language. Devil allows to write a high-level, formal definition of the programming interface of a peripheral circuit. A compiler automatically checks the consistency of a Devil specification, from which it generates the low-level, hardware-operating code.  In our original framework, the generated code is dependent of the host architecture (CPU, buses, and bridges). Consequently, any variation in the hardware environment requires a specific tuning of the compiler. Considering the variability of embedded architectures, this is a serious drawback. In addition, this prevents from mixing different buses in the same circuit interface.  In this paper, we remove those limitations by improving our framework in two ways. (i) We propose a better isolation between the Devil compiler and the host architecture. (ii) We introduce Trident, a language extension aimed at mapping one or several buses to each peripheral circuit.
Uniprocessor Scheduling Under Precedence Constraints
In this paper we present a novel approach to the constrained scheduling problem, while addressing a more general class of constraints that arise from the timing requirements on real-time embedded controllers and from the implementation of mixed data-flow/event-driven real-time systems. We provide general necessary and sufficient conditions for scheduling under precedence constraints and derive sufficient conditions for two well-known scheduling policies. We define mathematical problems that provide optimum priority and deadline assignments, while ensuring both precedence constraints and systems schedulability.We show how these problems can be relaxed to corresponding ILP formulations leveraging on available solvers.
Adaptive Multi-Layer Traffic Engineering with Shared Risk Group Protection
In this paper we propose a new traffic engineering scheme to be used jointly with protection in multi-layer, grooming-capable, optical-beared networks. To make the working and protection paths of demands better adapt to changing traffic and network conditions we propose the adaptive multi-layer traffic engineering (AMLTE) scheme that "tailors" i.e., fragments and de-fragments wavelength paths in a fully automatic distributed way.
Endowing Spoken Language Dialogue Systems with Emotional Intelligence
While most dialogue systems restrict themselves to the adjustment of the propositional contents, our work concentrates on the generation of stylistic va- riations in order to improve the user's perception of the interaction. To accomplish this goal, our approach integrates a social theory of politeness with a cognitive theory of emotions. We propose a hierarchical selection process for politeness behaviors in order to enable the refinement of decisions in case additional context information becomes available.
A multiple subset sum formulation for feedback implosion suppression over satellite networks
In this paper, we present a feedback implosion suppression (FIS) algorithm that reduces the volume of feedback information transmitted through the network without relying on any collaboration between users, or on any infrastructure other than the satellite network. Next generation satellite systems that utilize the Ka frequency band are likely to rely on various fade mitigation techniques, in order to guarantee a service quality that is comparable to other broadband technologies. User feedback would be a valuable input for a number of such components, however, collecting periodic feedback from a large number of users would result in the well-known feedback implosion problem. Feedback implosion is identified as a major problem when a large number of users try to transmit their feedback messages through the network, holding up a significant portion of the uplink resources and clogging the shared uplink medium. In this paper, we look at a system where uplink channel access is organized in time-slots. The goal of the FIS algorithm is to reduce the number of uplink time-slots hold up for the purpose of feedback transmission. Our analysis show that the FIS algorithm effectively suppresses the feedback messages of 95% of all active users, but still achieves acceptable performance results when the ratio of available time-slots to number of users is equal to or higher than 5%
Computing subgraph probability of random geometric graphs with applications in quantitative analysis of ad hoc networks
Random geometric graphs (RGG) contain vertices whose points are uniformly distributed in a given plane and an edge between two distinct nodes exists when their distance is less than a given positive value. RGGs are appropriate for modeling ad hoc networks consisting of n mobile devices that are independently and uniformly distributed randomly in an area. To the best of our knowledge, this work presents the first paradigm to compute the subgraph probability of RGGs in a systematical way. In contrast to previous asymptotic bounds or approximation, which always assume that the number of nodes in the network tends to infinity, the closed-form formulas we derived herein are fairly accurate and of practical value. Moreover, computing exact subgraph probability in RGGs is shown to be a useful tool for counting the number of induced subgraphs, which explores fairly accurate quantitative property on topology of ad hoc networks.
Unequal Error Protection for Video Streaming Over Wireless LANs using Content-Aware Packet Retry Limit
In this paper, we propose a content-aware retry limit adaptation scheme for video streaming over IEEE 802.11 wireless LANs (WLANs). Video packets of different importance are unequally protected with different retry limits at the MAC layer. The loss impact of each packet is estimated to guide the selection of its retry limit. More retry numbers are allocated to packets of higher loss impact to achieve unequal error protection. Experimental results show that the proposed adaptation scheme can effectively mitigate the error propagation due to packet loss and assure the on-time arrival of packets for presentation, thereby improving video quality significantly.
Linearization of ancestral multichromosomal genomes.
Background#R##N#Recovering the structure of ancestral genomes can be formalized in terms of properties of binary matrices such as the Consecutive-Ones Property (C1P). The Linearization Problem asks to extract, from a given binary matrix, a maximum weight subset of rows that satisfies such a property. This problem is in general intractable, and in particular if the ancestral genome is expected to contain only linear chromosomes or a unique circular chromosome. In the present work, we consider a relaxation of this problem, which allows ancestral genomes that can contain several chromosomes, each either linear or circular.
OdinTools--Model-Driven Development of Intelligent Mobile Services
Today's computationally able mobile devices are capable of acting as service providers as opposed to their traditional role as consumers. To address the challenges associated with the development of these mobile services, we have developed Odin, a middleware which masks complexity, allowing rapid development of mobile services. Odin, however, does not allow cross-platform development, which is an important concern with today's wide variety of mobile devices. To solve this problem, we have designed Odin Tools - a model-driven toolkit for cross-platform development of mobile services. Leveraging appropriate metamodels, a prototype has been implemented in Eclipse and Marama that allows developers to model mobile services in a platform-independent manner. We are currently working on transformations between levels of the model hierarchy which will allow full Odin-based service implementations to be generated automatically.
A socio political model of the relationship between IT investments and business performance
In recent years many studies have been published on the assessment of payoffs from investments in IT. The research has produced mainly mixed results. Different explanations can be given for these mixed results. One possible explanation might be the dominance of the rational perspective in previous research. The consequence of this overemphasis on rationality is that the social political nature of the IT investment process has largely been neglected in previous research on IT business value. This omission produces an incomplete picture and might contribute to the conflicting empirical results. In this research we studied whether and how the socio political perspective can be used to explain the mixed results. Case study research was used to test whether the attitude towards the value of IT, destructive conflict, and a low level of trust influence the relationship between IT investments and business performance.
A new metrics set for evaluating testing efforts for object-oriented programs
Software metrics proposed and used for procedural paradigm have been found inadequate for object oriented software products, mainly because of the distinguishing features of the object oriented paradigm such as inheritance and polymorphism. Several object oriented software metrics have been described in the literature. These metrics are goal driven in the sense that they are targeted towards specific software qualities. We propose a new set of metrics for object oriented programs; this set is targeted towards estimating the testing efforts for these programs. The definitions of these metrics are based on the concepts of object orientation and hence are independent of the object oriented programming languages. The new metrics set has been critically compared with three other metrics sets published in the literature.
Towards a universal client for grid monitoring systems: design and implementation of the Ovid browser
In this paper, we present the design and implementation of Ovid, a browser for grid-related information. The key goal of Ovid is to support the seamless navigation of users in the grid information space. Key aspects of Ovid are: (i) a set of navigational primitives, which are designed to cope with problems such as network disorientation and information overloading; (ii) a small set of Ovid views, which present the end-user with high-level, visual abstractions of grid information; these abstractions correspond to simple models that capture essential aspects of a grid infrastructure; (iii) support for embedding and implementing hyperlinks that connect related entities represented within different information views; (iv) a plug-in mechanism, which enables the seamless integration with Ovid of third-party software that retrieves and displays data from various grid information sources; and (v) a modular software design, which allows the easy integration of different visualization algorithms that support the graphical representation of large amounts of grid-related information in the context of Ovid's views.
A trainable, single-pass algorithm for column segmentation
Column segmentation logically precedes OCR in the document analysis process. The trainable algorithm XYCUT relies on horizontal and vertical binary profiles to produce an XY-tree representing the column structure of a page of a technical document in a single pass through the bit image. Training against ground truth adjusts a single, resolution independent, parameter using only local information and guided by an edit distance function. The algorithm correctly segments the page image for a (fairly) wide range of parameter values, although small, local and repairable errors may be made, an effect measured by a repair cost function.
Hybrid Wordlength Optimization Methods of Pipelined FFT Processors
Quickly and accurately predicting the performance based on the requirements for IP-based system implementations optimizes the design and reduces the design time and overall cost. This study describes a novel hybrid method for the word-length optimization of pipelined FFT processors that is the arithmetic kernel of OFDM-based systems. This methodology utilizes the rapid computing of statistical analysis and the accurate evaluation of simulation-based analysis to investigate a speedy optimization flow. A statistical error model for varying word-lengths of PE stages of an FFT processor was developed to support this optimization flow. Experimental results designate that the word-length optimization employing the speedy flow reduces the percentage of the total area of the FFT processor that increases with an increasing FFT length. Finally, the proposed hybrid method requires a shorter prediction time than the absolute simulation-based method does and achieves more accurate outcomes than a statistical calculation does.
Generating organic textures with controlled anisotropy and directionality
This article presents a method for generating organic textures by tessellating a region into a set of pseudo-Voronoi polygons using a particle model and then generating the detailed geometry each of the polygons with fractal noise.
Guidelines for reporting an fMRI study
In this editorial, we outline a set of guidelines for the reporting of methods and results in functional magnetic resonance imaging studies and provide a checklist to assist authors in preparing manuscripts that meet these guidelines.
On Load Regulated CSMA
In this paper, we derive throughput of a threshold-based transmission policy, namely load-regulated CSMA, taking into account the propagation delay of the medium and the offered load at different probability of the fading channel. In case of the saturated load regulated CSMA, a trivial relationship between deterministic offered load to the channel at a particular fading channel condition and the maximum possible offered load has been shown. We further extend the load regulation concept into multi-channel domain. Both single and multi-channel load regulated CSMA improves the throughput of the system compared to the existing CSMA system which does not consider channel fading to control the packet transmissions.
Component based design using constraint programming for module placement on FPGAs
Constraint satisfaction modeling is both an efficient, and an elegant approach to model and solve many real world problems. In this paper, we present a constraint solver targeting module placement in static and partial run-time reconfigurable systems. We use the constraint solver to compute feasible placement positions. Our placement model incorporates communication, implementation variants and device configuration granularity. In addition, we model heterogeneous resources such as embedded memory, multipliers and logic. Furthermore, we take into account that logic resources consist of different types including logic only LUTs, arithmetic LUTs with carry chains, and LUTs with distributed memory. Our work targets state of the art field-programmable gate arrays (FPGAs) in both design-time and run-time applications. In order to evaluate our placement model and module placer implementation, we have implemented a repository containing 200 fully functional, placed and routed relocatable modules. The modules are used to implement complete systems. This validates the feasibility of both the model and the module placer. Furthermore, we present simulated results for run-time applications, and compare this to other state of the art research. In run-time applications, the results point to improved resource utilization. This is a result of using a finer tile grid and complex module shapes.
E-BRAINSTORMING: OPTIMIZATION OF COLLABORATIVE LEARNING THANKS TO ONLINE QUESTIONNAIRES.
The purpose of this article is to present a methodology and tools allowing the use of online multiple-choice questionnaires to enhance collaborative work. The first goal is to allow the questionnaires generation and setting with a simple and ergonomic manner, but also to let questioned people making comments and proposing new questions to other contributors. The developed system provides a visualization of a synthesis of the questionnaire results that is also accessible by the mean of external applications through standard Web services. These principles were developed and tested on a sample of users.
Bioinformatics integration framework for metabolic pathway data-mining
A vast amount of bioinformatics information is continuously being introduced to different databases around the world. Handling the various applications used to study this information present a major data management and analysis challenge to researchers. The present work investigates the problem of integrating heterogeneous applications and databases towards providing a more efficient data-mining environment for bioinformatics research. A framework is proposed and GeXpert, an application using the framework towards metabolic pathway determination is introduced. Some sample implementation results are also presented.
Schedulability analysis of fixed priority real-time systems with offsets
For a number of years, work has been performed in collaboration with industry to establish improved techniques for achieving and proving the system timing constraints. The specific requirements encountered during the course of this work for both uniprocessor and distributed systems indicate a need for an efficient mechanism for handling the timing analysis of task sets which feature offsets. Little research has been performed on this subject. The paper describes a new technique tailored to a set of real world problems so that the results are effective and the complexity is manageable.
Change Impact Analysis for Generic Libraries
Since the Standard Template Library (STL), generic libraries in C++ rely on concepts to precisely specify the requirements of generic algorithms (function templates) on their parameters (template arguments). Modifying the definition of a concept even slightly, can have a potentially large impact on the (interfaces of the) entire library. In particular the non-local effects of a change, however, make its impact difficult to determine by hand. In this paper we propose a conceptual change impact analysis (CCIA), which determines the impact of changes of the conceptual specification of a generic library. The analysis is organized in a pipe-and-filter manner, where the first stage finds any kind of impact, the second stage various specific kinds of impact. Both stages describe reachability algorithms, which operate on a conceptual dependence graph. In a case study, we apply CCIA to a new proposal for STL iterator concepts, which is under review by the C++ standardization committee. The analysis shows a number of unexpected incompatibilities and, for certain STL algorithms, a loss of genericity.
A model and case study for efficient shelf usage and assortment analysis
In the rapidly changing environment of Fast Moving Consumer Goods sector where new product launches are frequent, retail channels need to reallocate their shelf spaces intelligently while keeping up their total profit margins, and to simultaneously avoid product pollution. In this paper we propose an optimization model which yields the optimal product mix on the shelf in terms of profitability, and thus helps the retailers to use their shelves more effectively. The model is applied to the shampoo product class at two regional supermarket chains. The results reveal not only a computationally viable model, but also substantial potential increases in the profitability after the reorganization of the product list.
Multiprocessors May Reduce System Dependability under File-Based Race Condition Attacks
Attacks exploiting race conditions have been considered rare and "low risk". However, the increasing popularity of multiprocessors has changed this situation: instead of waiting for the victim process to be suspended to carry out an attack, the attacker can now run on a dedicated processor and actively seek attack opportunities. This change from fortuitous encountering to active exploiting may greatly increase the success probability of race condition attacks. This point is exemplified by studying the TOCTTOU (Time-of- Check-to-Time-of-Use) race condition attacks in this paper. We first propose a probabilistic model for predicting TOCTTOU attack success rate on both uniprocessors and multiprocessors. Then we confirm the applicability of this model by carrying out TOCTTOU attacks against two widely used utility programs: vi and gedit. The success probability of attacking vi increases from low single digit percentage on a uniprocessor to almost 100% on a multiprocessor. Similarly, the success rate of attacking gedit jumps from almost zero to 83%. These case studies suggest that our model captures the sharply increased risks, and hence the decreased dependability of our systems, represented by race condition attacks such as TOCTTOU on the next generation multiprocessors.
Members of Random Closed Sets
The members of Martin-Lof random closed sets under a distribution studied by Barmpalias et al. are exactly the infinite paths through Martin-Lof random Galton-Watson trees with survival parameter $\frac{2}{3}$. To be such a member, a sufficient condition is to have effective Hausdorff dimension strictly greater than $\gamma=\log_2 \frac{3}{2}$, and a necessary condition is to have effective Hausdorff dimension greater than or equal to  ***  .
Information extraction from nanotoxicity related publications
High-quality experimental data are important when developing predictive models for studying nanomaterial environmental impact (NEI). Given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and small-scaled, extracting information from publications is an attractive alternative for collecting data. We developed an information extraction system that can extract useful information from full-text nanotoxicity related publications. This information extraction system consists of five components: raw data transformation into machine readable format, data preprocessing, ontology-based named entity recognition, rule-based numerical attribute extraction from both tables and unstructured text, and relation extraction among entities and attributes. The information extraction system is applied on a dataset made of 94 publications, and results in an acceptable accuracy. By storing extracted data into a table according to relations among the data, a dataset that can be used to predict nanomaterial environmental impact is obtained. Such a system is unique in current nanomaterial community, and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles.
CMOS body-enhanced cascode current mirror
A cascode current mirror with auxiliary body-driven feedback loop is proposed. Main performance parameters are analytically evaluated and compared to those of a conventional high-swing cascode and of a recently-proposed body-driven topology. Simulations are also provided confirming improvements in the achievable output resistance (important for short channel technologies), DC accuracy, and input dynamic range. Linearity, bandwidth, noise and voltage requirements are substantially the same of the conventional high-swing cascode solution.
Performance of the beacon-less routing protocol in realistic scenarios
The beacon-less routing protocol (BLR) is a position-based routing protocol for mobile ad-hoc networks that makes use of location information to reduce routing overhead. Unlike other position-based routing protocols, BLR does not require nodes to periodically broadcast hello messages. This avoids drawbacks such as extensive use of scarce battery-power, interferences with regular data transmission, and outdated position information in case of high mobility. This paper discusses the behavior and performance of BLR in realistic scenarios, in particular with irregular transmission ranges. BLR has been implemented using appropriate simulation models and in an out-door test-bed consisting of GNU/Linux laptops with wireless LAN network interfaces and GPS receivers.
Two-dimensional orthogonal tiling: from theory to practice
In pipelined parallel computations the inner loops are often implemented in a block fashion. In such programs, an important compiler optimization involves the need to statically determine the grain size. This paper presents extensions and experimental validation of the previous results of Andonov and Rajopadhye (1994) on optimal grain size determination.
A two-level ECN marking for fair bandwidth allocation between HSTCP and TCP Reno
Many versions of TCP have been proposed for transmitting data, and among them, TCP Reno is most widely used today. However, the problem, that it is difficult to use the wide bandwidth efficiently with TCP Reno, has been pointed out. HSTCP is one of the several new versions of TCP that are proposed to address this problem, but when its flows compete with TCP Reno flows at the same link, HSTCP gains most of the bandwidth and it is impossible to conduct fair transfer. In order to address this problem, we propose a two-level ECN marking to increase the frequency of congestion controls of HSTCP flows, holding its throughput. We evaluate our proposal through computer simulations, and the results show that our proposal mitigates bandwidth allocation to HSTCP, promoting fair transfer with TCP Reno.
A distributed simulation based monitoring
MSS, a computer-based monitoring system with integrated cooperative objects is proposed. MSS uses an object-based framework to interface with the user to guide a specific system evolution. MSS espouses a blackboard architecture and runs according a cooperating objects model. To achieve monitoring tasks, MSS selects the appropriate technique(s) within a set of a high performance algorithms. From the user viewpoint, MSS has been developed as a control assistant featuring different levels of interactivity, a hierarchical design style and fully embedded algorithmic tools. Virtually, MSS is able to design a monitoring board for any dynamic system.
Leakage power reduction in dual-Vdd and dual-Vth designs through probabilistic analysis of Vth variation
The noise sensitivity of low power circuits is rapidly increasing with the increasing levels of process variability and uncertainty. In this work, we study the problem of leakage power minimization in dual-Vdd and dual-Vth designs in the presence of significant Vth variation. The impact of the uncertainty in Vth on leakage power and timing are studied through probabilistic analytical models. We develop probabilistic models for timing slack and leakage power considering threshold variations with the objective of achieving an optimal selection of Vth. An analysis of the models indicate that, in the presence of variability, the value of the second Vth must be about 30mV higher than the Vth value obtained without considering variability. We show that our proposed method for the selection of Vth yields the lowest leakage power ratio of the dual-Vdd and dual-Vth versus the single-Vdd and single-Vth designs. In addition, the proposed models can be used to determine the ideal values for the second Vdd and Vth values in the context of variability for a variety of process conditions.
Probabilistic Cluster Signature for Modeling Motion Classes
In this paper, a novel 3-D motion trajectory signature is introduced to serve as an effective description to the raw trajectory. More importantly, based on the trajectory signature, a probabilistic model-based cluster signature is further developed for modeling a motion class. The cluster signature is a mixture model-based motion description that is useful for motion class perception, recognition and to benefit a generalized robot task representation. The signature modeling process is supported by integrating the EM and IPRA algorithms. The conducted experiments verified the cluster signature's effectiveness.
Secure cross-domain positioning architecture for autonomic systems
Positioning, as one of the prime components of context, has been a driving factor in the development of ubiquitous computing applications throughout the past two decades. Based on the redundant positioning architecture, this paper discusses the issues of exchanging positioning data between applications and between different administrative domains, with a focus on implementing security and privacy concepts in a self-learning and self-adapting autonomic systems environment
Refinement of medical knowledge bases: a neural network approach
One important issue in designing medical knowledge-based systems is the management of uncertainty. Among the schemes that have been developed for this purpose, probability and CF (certainty factor) are the most widely used. If rules are organized according to a connectionist model, then neural network learning suggests a promising solution to this problem. When most rules are correct, semantically incorrect rules can be recognized if their associated certainty factors are weakened or change signs after training with correct samples. The techniques for rule base refinement are examined under this approach. The concept has been implemented and tested in an actual medical expert system. >
An IT appliance for remote collaborative review of mechanisms of injury to children in motor vehicle crashes
This paper describes the architecture and implementation of a Java-based appliance for collaborative review of crashes involving injured children in order to determine mechanisms of injury. The multidisciplinary expertise needed for such reviews is not available at any one institution, resulting in the need for remote collaboration, while the sensitive nature of the information requires secure transmission and controlled access of data. The intended users of the appliance are researchers, engineers, medical doctors, government regulators, automobile and restraint manufacturers, insurance company representatives, and others who are interested in understanding the types and causes of injuries to children involved in motor vehicle crashes. The ultimate goal is to devise engineering solutions that prevent similar injuries from occurring in the future. The collaboration appliance (called Telecenter) enables the following activities: (1) the distributed asynchronous collection of digital content needed for each crash case review under a scheme that consistently organizes content across multiple cases; (2) the secure, Web-based remote participation of users in case-review meetings that involve viewing of case-specific content, live communication (written or verbal), multimedia access and sharing (slide presentations/ images), and use of Web resources; and (3) archival and post-review access of case reviews for follow-up activities and other functions (e.g., statistics, search, and networking). The Telecenter design supports audio conferencing, remote delivery and viewing of slide presentations, and other collaboration features also available in commercial and public-domain collaboration middleware products. However, it goes beyond existing solutions by also embedding a specific workflow and content organization suited for traffic injury reviews, supporting spatio-temporal role-based access control, distributed management of content and seamless integration of existing services. The current status and experience from using an early prototype of the Telecenter in actual case reviews are discussed, along with planned extensions to its functionality.
Strategic Design of the Purchase System Toward R&D Supply Chain Based on SNA
In order to make the strategy for research and development R&D purchase system better serve the personalization of material requirements in R&D process, the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously. Social network analysis is used to analyze the vertical and horizontal relationships among the project, department and enterprise layers. Through a case study, the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration. To disclose the restricted equilibrium mechanism of participants involved, changes under different strategies are compared, which can assist enterprises to enforce the decision making and to improve the R&D purchase system ability. The authors outline some of the managerial implications arising from the research findings at the end of this paper.
Exploring early usage patterns of mobile data services
In this paper we study the nature of factors that facilitate mobile data services use, as well as the characteristics of early adopters, to shed light into diffusion patterns and inform predictions for future growth. We advocate that the use of mobile data services can be associated with one's level of satisfaction with his/her life. Based on the findings of a questionnaire-based survey (N=388), we have found that users satisfied with their personal life use information, mobile e-mail, and stock broking services more frequently than dissatisfied ones, while users satisfied with their professional life tend to use financial, information, and mobile e-mail services more heavily. Furthermore, we identify early adopters' profiles in terms of their demographic characteristics (gender, age, education, and income) to inform the design of effective target marketing strategies.
A Fast Fixed Point Iteration Algorithm for Sparse Channel Estimation
Channels with a long but sparse impulse response arise in a variety of wireless communication applications, such as high definition television (HDTV) terrestrial transmission and underwater acoustic communications. By adopting the $\ell_1$-norm as the sparsity metric of the channel response, the channel estimation is formulated as a complex-valued convex optimization problem. A fast fixed point iteration algorithm is developed to solve the resultant complex-valued $\ell_1$-minimization problem. The proposed fast channel estimation algorithm is easy to implement and has a low computational complexity of $O(N\log N)$ per iteration with $N$ the signal length. Simulation results are provided to demonstrate the performance of the proposed fixed point algorithm.
Model-independent recovery of object orientations
A novel algorithm is presented for determining the orientation of road vehicles in traffic scenes using video images. The algorithm requires no specific 3-D vehicle models and only uses local image gradient values. It may easily be implemented in real-time. Experimental results with a variety of vehicles in routine traffic scenes are included to demonstrate the effectiveness of the algorithm.
Linear Scale and Rotation Invariant Matching
Matching visual patterns that appear scaled, rotated, and deformed with respect to each other is a challenging problem. We propose a linear formulation that simultaneously matches feature points and estimates global geometrical transformation in a constrained linear space. The linear scheme enables search space reduction based on the lower convex hull property so that the problem size is largely decoupled from the original hard combinatorial problem. Our method therefore can be used to solve large scale problems that involve a very large number of candidate feature points. Without using prepruning in the search, this method is more robust in dealing with weak features and clutter. We apply the proposed method to action detection and image matching. Our results on a variety of images and videos demonstrate that our method is accurate, efficient, and robust.
A built-in self-testing approach for minimizing hardware overhead
A built-in self-test (BIST) hardware insertion technique is addressed. Applying to register transfer level designs, this technique utilizes not only the circuit structure but also the module functionality in reducing test hardware overhead. Experimental results have shown up to 38% reduction in area overhead over other system level BIST techniques. >
Multi-Stage TR Scheme for PAPR Reduction in OFDM Signals
In the tone reservation (TR) scheme of the orthogonal frequency division multiplexing (OFDM) systems, there exists a trade-off between the peak to average power ratio (PAPR) reduction performance and the peak reduction tone (PRT) set size. In this paper, we propose a multi-stage TR scheme for PAPR reduction, which adaptively selects one of several PRT sets according to the PAPR of OFDM signal while the PRT set is fixed for the conventional TR scheme. It is shown that the PAPR reduction performance of the proposed scheme is better than that of the conventional TR scheme when the tone reservation rate (TRR) is the same.
Random key predistribution schemes for sensor networks
Key establishment in sensor networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor nodes, and also because the nodes could be physically compromised by an adversary. We present three new mechanisms for key establishment using the framework of pre-distributing a random set of keys to each node. First, in the q-composite keys scheme, we trade off the unlikeliness of a large-scale network attack in order to significantly strengthen random key predistribution's strength against smaller-scale attacks. Second, in the multipath-reinforcement scheme, we show how to strengthen the security between any two nodes by leveraging the security of other links. Finally, we present the random-pairwise keys scheme, which perfectly preserves the secrecy of the rest of the network when any node is captured, and also enables node-to-node authentication and quorum-based revocation.
Quantitative assessment of image noise and streak artifact on CT image: comparison of z-axis automatic tube current modulation technique with fixed tube current technique.
Abstract  The purpose of our study is to quantitatively assess the effects of  z -axis automatic tube current modulation technique on image noise and streak artifact, by comparing with fixed tube current technique. Standard deviation of CT-values was employed as a physical index for evaluating image noise, and streak artifact was quantitatively evaluated using our devised Gumbel evaluation method.  z -Axis automatic tube current modulation technique will improve image noise and streak artifact, compared with fixed tube current technique, and will make it possible to significantly reduce radiation doses at lung levels while maintaining the same image quality as fixed tube current technique.
Coordination policy for a two-stage supply chain considering quantity discounts and overlapped delivery with imperfect quality
Unlike the traditional integrated supplier-buyer coordination model, this research incorporates overlapped delivery and imperfect items into the production-distribution model. This model improves the observable fact that the system might experience shortage during the screening duration and also takes quantity discount into account. This approach has not been discussed in previous integrated supplier-buyer coordination models. The expected annual integrated total cost function is derived and properties and theorems are explored to help develop an algorithm. A solution procedure, free from the convexity associated with an algorithm is established to find the optimal solution. A numerical example is given to illustrate the proposed procedure and algorithm. A sensitivity analysis is made to investigate the effects of five important parameters (the inspect rate, the annual demand, the defective rate, the holding cost, and the receiving cost) on the optimal solution. Managerial insights are also discussed.
Monte Carlo modeling for implantable fluorescent analyte sensors
A Monte Carlo simulation of photon propagation through human skin and interaction with a subcutaneous fluorescent sensing layer is presented. The algorithm will facilitate design of an optical probe for an implantable fluorescent sensor, which holds potential for monitoring many parameters of biomedical interest. Results are analyzed with respect to output light intensity as a function of radial distance from source, angle of exit for escaping photons, and sensor fluorescence (SF) relative to tissue autofluorescence (AF). A sensitivity study was performed to elucidate the effects on the output due to changes in optical properties, thickness of tissue layers, thickness of the sensor layer, and both tissue and sensor quantum yields. The optical properties as well as the thickness of the stratum corneum, epidermis, (tissue layers through which photons must pass to reach the sensor) and the papillary dermis (tissue distal to sensor) are highly influential. The spatial emission profile of the SF is broad compared that of the tissue fluorescence and the ratio of sensor to tissue fluorescence increases with distance from the source. The angular distribution of escaping photons is more concentrated around the normal for SF than for tissue AF. The information gained from these simulations will he helpful in designing appropriate optics for collection of the signal of interest.
Towards a data publishing framework for primary biodiversity data: challenges and potentials for the biodiversity informatics community
Background: Currently primary scientific data, especially that dealing with biodiversity, is neither easily discoverable nor accessible. Amongst several impediments, one is a lack of professional recognition of scientific data publishing efforts. A possible solution is establishment of a ‘Data Publishing Framework’ which would encourage and recognise investments and efforts by institutions and individuals towards management, and publishing of primary scientific data potentially on a par with recognitions received for scholarly publications. Discussion: This paper reviews the state-of-the-art of primary biodiversity data publishing, and conceptualises a ‘Data Publishing Framework’ that would help incentivise efforts and investments by institutions and individuals in facilitating free and open access to biodiversity data. It further postulates the institutionalisation of a ‘Data Usage Index (DUI)’, that would attribute due recognition to multiple players in the data collection/creation, management and publishing cycle. Conclusion: We believe that institutionalisation of such a ‘Data Publishing Framework’ that offers socio-cultural, legal, technical, economic and policy environment conducive for data publishing will facilitate expedited discovery and mobilisation of an exponential increase in quantity of ‘fit-for-use’ primary biodiversity data, much of which is currently invisible.
On transistor level gate sizing for increased robustness to transient faults
In this paper we present a detailed analysis on how the critical charge (Q/sub crit/) of a circuit node, usually employed to evaluate the probability of transient fault (TF) occurrence as a consequence of a particle hit, depends on transistors' sizing. We derive an analytical model allowing us to calculate a node's Q/sub crit/ given the size of the node's driving gate and fan-out gate(s), thus avoiding time costly electrical level simulations. We verified that such a model features an accuracy of the 97% with respect to electrical level simulations performed by HSPICE. Our proposed model shows that Q/sub crit/ depends much more on the strength (conductance) of the gate driving the node, than on the node total capacitance. We also evaluated the impact of increasing the conductance of the driving gate on TFs' propagation, hence on soft error susceptibility (SES). We found that such a conductance increase not only improves the TF robustness of the hardened node, but also that of the whole circuit.
Design-inclusive UX research: design as a part of doing user experience research
Since the third wave in human–computer interaction HCI, research on user experience UX has gained momentum within the HCI community. The focus has shifted from systematic usability requirements and measures towards guidance on designing for experiences. This is a big change, since design has traditionally not played a large role in HCI research. Yet, the literature addressing this shift in focus is very limited. We believe that the field of UX research can learn from a field where design and experiential aspects have always been important: design research. In this article, we discuss why design is needed in UX research and how research that includes design as a part of research can support and advance UX design practice. We do this by investigating types of design-inclusive UX research and by learning from real-life cases of UX-related design research. We report the results of an interview study with 41 researchers in three academic research units where design research meets UX research. Based on our interview findings, and building on existing literature, we describe the different roles design can play in research projects. We also report how design research results can inform designing for experience methodologically or by providing new knowledge on UX. The results are presented in a structured palette that can help UX researchers reflect and focus more on design in their research projects, thereby tackling experience design challenges in their own research.
Multi-factory optimization enables kit reconfiguration in semiconductor manufacturing
To enable the huge saving of the kit-breakdown, we developed MaxIt v1.2 to generate an optimal capacity plan at the kit component level for the mid-range build plan in multi-factory environment. We describe the MILP (mixed integer linear programming) model and system architecture of MaxIt v1.2. We also conduct detailed sensitivity analysis on parameter setting and objective prioritizing. With the implementation in the Intel Shanghai and Manila sites, we have significantly improved data integrity and enabled a -US/spl ges/ cost savings.
Eudaemonic computing ('underwearables')
This paper presents a framework for wearable computing, based on the principle that it be unobtrusive, and that it be integrated into ordinary clothing. This design philosophy, called 'eudaemonic computing' (named in honor of the group of physicists who designed the first truly unobtrusive wearable computers with vibrotactile displays) is reduced to practice through the 'underwearable computer' ('underwearable' for short). The 'underwearable' is a computer system that is meant to be worn within or under ordinary clothing. The first 'underwearables' were built in the early 1980s, and have evolved into a form that very much resembles a tank-top. There were three reasons for the tank structure: (1) weight is evenly and comfortably distributed over the body, and bulk is distributed unobtrusively; (2) it provides privacy by situating the apparatus within the corporeal boundary we consider our own (personal) space, and others also so-regard; and (3) proximity to the body affords capability to both sense biological signal quantities (such as respiration and heart signals which are both accessible to a vest-based device), as well as produce output that we can sense, unobtrusively. The vibrotactile output modality (VibraVest) was explored as a means of assisting the visually challenged (to avoid bumping into objects through an ability to 'feel' objects at a distance). The success of VibraVest suggests other possibilities for similar unobtrusive devices that can be worn over an extended period of time, in all facets of day-to-day life.
Information systems for the age of consequences
This paper discusses what kinds of computer information systems might be of broad social value in the context of the increasingly severe ecological and social consequences of economic growth, and how they might be built and maintained. The paper has two parts. The first offers a particular understanding of the ecological and social “limits” to economic growth. The second considers how this understanding can inform computer information systems design and operation and characterizes good “limits-aware” computing research.
Active Selection of Training Examples for Meta-Learning
Meta-learning has been used to relate the performance of algorithms and the features of the problems being tackled. The knowledge in meta-learning is acquired from a set of meta-examples which are generated from the empirical evaluation of the algorithms on problems in the past. In this work, active learning is used to reduce the number of meta-examples needed for meta-learning. The motivation is to select only the most relevant problems for meta-example generation, and consequently to reduce the number of empirical evaluations of the candidate algorithms. Experiments were performed in two different case studies, yielding promising results.
Engineering Semantic Web Information Systems
Web Information Systems (WIS) use the Web paradigm and technologies to retrieve information from sources connected to the Web, and present the information in a web or hypermedia presentation to the user. Hera is a design methodology that supports the design of WIS. It is a model -driven method that distinguishes integration, data gathering, and presentation generation. In this paper we address the Hera methodology and specifically explain the integration model that covers the different aspects of integration, and the adaptation model, that specifies how the generated presentations are adaptable (e.g. device capabilities, user preferences). The Hera software framework provides a set of transformations that allow a WIS to go from integration to presentation generation. These transformations are based on RDF(S), and we show how RDF(S) has proven its value in combining all relevant aspects of WIS design. In this way, RDF(S) being the foundation of the Semantic Web, Hera allows the engineering of Semantic Web Information Systems (SWIS).
Optimal wiresizing for interconnects with multiple sources
The optimal wiresizing problem for nets with multiple sources is studied under the distributed Elmore delay model. We decompose such a net into a source subtree (SST) and a set of loading subtrees (LSTs), and show the optimal wiresizing solution satisfies a number of interesting properties, including: the LST separability, the LST monotone property, the SST local monotone property and the general dominance property. Furthermore, we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property. These properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions. Experiment results on nets from an Intel processor layout show an interconnect delay reduction of up to 35.9\% when compared to the minimum-width solution. In addition, the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy, when compared with the fixed grid based methods.
Magneto- and electroencephalographic manifestations of reward anticipation and delivery
article i nfo Article history: Accepted 19 April 2012 Available online 26 April 2012 The monetary incentive delay task was used to characterize reward anticipation and delivery with concur- rently acquired evoked magnetic fields, EEG potentials and EEG/MEG oscillatory responses, obtaining a pre- cise portrayal of their spatiotemporal evolution. In the anticipation phase, differential activity was most prominent over midline electrodes and parieto-occipital sensors. Differences between non-reward- and reward-predicting cues were localized in the cuneus and later in the dorsal PCC, suggesting a modulation by potential reward information during early visual processing, followed by a coarse emotional evaluation of the cues. Oscillatory analysis revealed increased theta power after non-reward cues over fronto-central sites. In the beta range, power decreased with the magnitude of the potential reward and increased with re- action time, probably reflecting the influence of the striatal response to potential reward on the sensorimotor cortex. At reward delivery, negative prediction errors led to a larger mediofrontal negativity. The spatiotem- poral evolution of reward processing was modulated by prediction error: whereas differences were located in PCC and putamen in the prediction error comparison, in the case of expected outcomes they were located in PCC, ACC and parahippocampal gyrus. In the oscillatory realm, theta power was largest following rewards and, in the case of non-rewards, was largest when these were unexpected. Higher beta activity following re- wards was also observed in both modalities, but MEG additionally showed a significant power decrease for this condition over parieto-occipital sensors. Our results show how visual, limbic and striatal structures are involved in the different stages of reward anticipation and delivery, and how theta and beta oscillations have a prominent role in the processing of these stimuli.
Ranking Weblogs by Analyzing Reading and Commenting Activities
In this paper, we analyze people’s reading and commenting behaviors in blogspace and proposed an algorithm for blog ranking. Upon two selected communities, AI and Medical, we show how comments, reading records, active browsing and multi time browsing can help to construct the weblog graph and reflect a blog’s popularity. Based on these analysis, we propose cRank, a graph based algorithm, to rank blog among community members. Finally, we divide our dataset temporally and present how the proposed algorithm can make prediction on blogs’ rankings. The experiment shows that cRank has a better performance upon several baseline systems.
Robust lip region segmentation for lip images with complex background
Robust and accurate lip region segmentation is of vital importance for lip image analysis. However, most of the current techniques break down in the presence of mustaches and beards. With mustaches and beards, the background region becomes complex and inhomogeneous. We propose in this paper a novel multi-class, shape-guided FCM (MS-FCM) clustering algorithm to solve this problem. For this new approach, one cluster is set for the object, i.e. the lip region, and a combination of multiple clusters for the background which generally includes the skin region, lip shadow or beards. The proper number of background clusters is derived automatically which maximizes a cluster validity index. A spatial penalty term considering the spatial location information is introduced and incorporated into the objective function such that pixels having similar color but located in different regions can be differentiated. This facilitates the separation of lip and background pixels that otherwise are inseparable due to the similarity in color. Experimental results show that the proposed algorithm provides accurate lip-background partition even for the images with complex background features like mustaches and beards.
First formant difference for /i/ and /u/: A cross-linguistic study and an explanation
The value of the first formant of high back and high front vowels (/u/ and /i/) has been determined for near minimal pairs in a 30-language sample. It is found that for 29 out of 30 languages the average of the first formant is higher for high back vowels than for high front vowels, and that for 26 out of 28 languages the majority of minimal pairs has a high back vowel with a higher first formant than that of the high front vowel. A trend towards smaller differences was found in women, but this is not significant in the present data set.#R##N##R##N#Two factors may explain this observation. Firstly, the human vocal tract can only vary the position of gradual (and not abrupt) transitions of cross-sectional area. Secondly, there is a narrow tube just above the glottis (the epilarynx tube). Both factors cause the first formant of high back vowels to be raised, but neither is sufficiently important to explain the observed differences on its own.
An e-Learning Library on the Web
The main topic addressed in this paper is how to help learners select some instructive hypermedia-based learning resources according to their learning contexts from the Web. Our approach is to provide a digital library for web-based learning called e-Learning Library, which includes learning resource repository, local indexing, and adaptive navigation support. This aims to promote their learning with diverse learning resources involving a certain topic.
Stability of a class of linear switching systems with applications to two consensus problems
In this paper, we first establish a stability result for a class of linear switching systems involving Kronecker product. The problem is intriguing in that the system matrix does not have to be Hurwitz in any time instant. We have established the main result by a combination of the Lyapunov stability analysis and a generalized Barbalat's Lemma applicable to piecewise continuous linear systems. As applications of this stability result, we study both the leaderless consensus problem and the leader-following consensus problem for general marginally stable linear multi-agent systems under switching network topology. In contrast with many existing results, our result only assume that the dynamic graph is uniformly connected.
Factoring nonnegative matrices with linear programs
This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs). The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features. More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X ≈ CX and some linear constraints. The constraints are chosen to ensure that the matrix C selects features; these features can then be used to find a low-rank NMF of X. A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al. (2012). In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms. Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice. An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes.
Three-dimensional subband coding of video
We describe and show the results of video coding based on a three-dimensional (3-D) spatio-temporal subband decomposition. The results include a 1-Mbps coder based on a new adaptive differential pulse code modulation scheme (ADPCM) and adaptive bit allocation. This rate is useful for video storage on CD-ROM. Coding results are also shown for a 384-kbps rate that are based on ADPCM for the lowest frequency band and a new form of vector quantization (geometric vector quantization (GVQ)) for the data in the higher frequency bands. GVQ takes advantage of the inherent structure and sparseness of the data in the higher bands. Results are also shown for a 128-kbps coder that is based on an unbalanced tree-structured vector quantizer (UTSVQ) for the lowest frequency band and GVQ for the higher frequency bands. The results are competitive with traditional video coding techniques and provide the motivation for investigating the 3-D subband framework for different coding schemes and various applications. >
Translation of UML state machines to Modelica: Handling semantic issues
ModelicaML is a UML profile that enables modeling and simulation of systems and their dynamic behavior. ModelicaML combines the power of the OMG UML standardized graphical notation for systems and software modeling, and the simulation power of Modelica. This addresses the increasing need for precise and integrated modeling of products containing both software and hardware. This article discusses the usage of executable UML state machines for system modeling, i.e. usage of the same formalism for describing the state-based dynamic behavior of physical system components and software. Moreover, it points out that the usage of Modelica as an action language enables an integrated simulation of continuous-time and reactive/event-based system dynamics. The main purpose of this article is however to highlight issues that are identified regarding the UML specification which are experienced with typical executable implementations of UML state machines. The issues identified are resolved and rationales for the taken design decisions are provided.
Compressed sensing with linear correlation between signal and measurement noise
Existing convex relaxation-based approaches to reconstruction in compressed sensing assume that noise in the measurements is independent of the signal of interest. We consider the case of noise being linearly correlated with the signal and introduce a simple technique for improving compressed sensing reconstruction from such measurements. The technique is based on a linear model of the correlation of additive noise with the signal. The modification of the reconstruction algorithm based on this model is very simple and has negligible additional computational cost compared to standard reconstruction algorithms, but is not known in existing literature. The proposed technique reduces reconstruction error considerably in the case of linearly correlated measurements and noise. Numerical experiments confirm the efficacy of the technique. The technique is demonstrated with application to low-rate quantization of compressed measurements, which is known to introduce correlated noise, and improvements in reconstruction error compared to ordinary Basis Pursuit De-Noising of up to approximately 7dB are observed for 1bit/sample quantization. Furthermore, the proposed method is compared to Binary Iterative Hard Thresholding which it is demonstrated to outperform in terms of reconstruction error for sparse signals with a number of non-zero coefficients greater than approximately 1/10th of the number of compressed measurements.
Architecture optimization of a 3-DOF translational parallel mechanism for machining applications, the orthoglide
This paper addresses the architecture optimization of a three-degree-of-freedom translational parallel mechanism designed for machining applications. The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances. The resulting machine, the Orthoglide, features three fixed parallel linear joints which are mounted orthogonally, and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions, and good compactness. A small-scale prototype of the Orthoglide under development is presented at the end of this paper.
Prototype learning with margin-based conditional log-likelihood loss
The classification performance of nearest prototype classifiers largely relies on the prototype learning algorithms, such as the learning vector quantization (LVQ) and the minimum classification error (MCE). This paper proposes a new prototype learning algorithm based on the minimization of a conditional log-likelihood loss (CLL), called log-likelihood of margin (LOGM). A regularization term is added to avoid over-fitting in training. The CLL loss in LOGM is a convex function of margin, and so, gives better convergence than the MCE algorithm. Our empirical study on a large suite of benchmark datasets demonstrates that the proposed algorithm yields higher accuracies than the MCE, the generalized LVQ (GLVQ), and the soft nearest prototype classifier (SNPC).
Smart sensor architecture customized for image processing applications
A system level design methodology is applied to the embedded system design for a typical sensor network application: face detection for security purpose. The tradeoff analysis is performed for hardware and software implementations of the tasks in this application. The best system design is achieved with limited hardware resources.
A problem-driven collaborative approach to eliciting requirements of internetwares
In the software development, most stakeholders cannot clearly and objectively express their needs for the envisioned software systems. In this paper, we propose a problem-driven collaborative requirements elicitation approach, with the purpose of helping identify and extract the requirements of the Internetwares (a complex and new software paradigm). The basic idea of our approach is that the requirements of the software systems should be stated by stakeholders in an objective way (i.e. problem-identifying-solving way). That is, first identify the problems existed in the as-is problem domain, and then find the solutions to the problems. The solutions to the problems are the requirements of the envisioned software systems. To this end, we propose the structure of problems and a collaborative process for achieving the solutions.
A diversity-based method for infrequent purchase decision support in e-commerce
In this paper we propose a method for supporting consumer buying decisions in e-commerce. We are advocating the diversity-driven approach to generating alternatives for infrequently purchased products (i.e., computers, vehicles, etc.). Our method is based upon the well-known ''divergence/convergence'' principle of problem solving. The paper discusses the method based on fuzzy weighted-sum model and cluster analysis, the architecture and the operation of the decision support system for generating product alternatives. The preliminary experiments with the prototype for notebook selection provide some support in favor of our approach over the catalog-based systems.
Heuristic scheduling of jobs on parallel batch machines with incompatible job families and unequal ready times
This research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication, where the machines can be modeled as parallel batch processors. We attempt to minimize total weighted tardiness on parallel batch machines with incompatible job families and unequal ready times of the jobs. Given that the problem is NP-hard, we propose two different decomposition approaches. The first approach forms fixed batches, then assigns these batches to the machines using a genetic algorithm (GA), and finally sequences the batches on individual machines. The second approach first assigns jobs to machines using a GA, then forms batches on each machine for the jobs assigned to it, and finally sequences these batches. Dispatching and scheduling rules are used for the batching phase and the sequencing phase of the two approaches. In addition, as part of the second decomposition approach, we develop variations of a time window heuristic based on a decision theory approach for forming and sequencing the batches on a single machine.
BrainKnowledge: A Human Brain Function Mapping Knowledge-Base System
Associating fMRI image datasets with the available literature is crucial for the analysis and interpretation of fMRI data. Here, we present a human brain function mapping knowledge-base system (BrainKnowledge) that associates fMRI data analysis and literature search functions. BrainKnowledge not only contains indexed literature, but also provides the ability to compare experimental data with those derived from the literature. BrainKnowledge provides three major functions: (1) to search for brain activation models by selecting a particular brain function; (2) to query functions by brain structure; (3) to compare the fMRI data with data extracted from the literature. All these functions are based on our literature extraction and mining module developed earlier (Hsiao, Chen, Chen. Journal of Biomedical Informatics 42, 912–922, 2009), which automatically downloads and extracts information from a vast amount of fMRI literature and generates co-occurrence models and brain association patterns to illustrate the relevance of brain structures and functions. BrainKnowledge currently provides three co-occurrence models: (1) a structure-to-function co-occurrence model; (2) a function-to-structure co-occurrence model; and (3) a brain structure co-occurrence model. Each model has been generated from over 15,000 extracted Medline abstracts. In this study, we illustrate the capabilities of BrainKnowledge and provide an application example with the studies of affect. BrainKnowledge, which combines fMRI experimental results with Medline abstracts, may be of great assistance to scientists not only by freeing up resources and valuable time, but also by providing a powerful tool that collects and organizes over ten thousand abstracts into readily usable and relevant sources of information for researchers.
Radio resource allocation for cellular networks based on OFDMA with QoS guarantees
In this paper we address the problem of radio resource allocation for QoS support in the downlink of a cellular OFDMA system. The major impairments considered are cochannel interference (CCI) and frequency selective fading. The allocation problem involves assignment of base stations and subcarriers, bit loading, and power control, for multiple users. We propose a three-stage, low-complexity, heuristic algorithm to distribute radio resources among multiple users according to their individual QoS requirements, while at the same time maintaining the QoS of already established links in all the cochannel cells. The allocation objective is to minimize the total transmit power, which adds to reducing CCI. Simulation results show a superior performance of the proposed method when compared to classical radio resource management techniques. Our scheme allows us to achieve almost 6 times higher capacity (sum data rate) than the method based on FDMA with power control, at a blocking probability of 0.02.
Spectral texturing for real-time applications
In this sketch we present a new method for rendering large-scale, high-resolution, non-repetitive textures in real-time using multi layer texturing. The basic idea of spectral texturing is to construct the nal texture by multiple texture layers where each layer provides a certain range of the spectrum of the texture’s spatial frequencies. Alpha channels are used to introduce statistical dependencies between the frequency bands. This approach extends a method called detail texturing, which does not use alpha channels to model higher statistical properties of the resulting texture. Other approaches to generate textures with specied statistical properties, like [DeBonet 1997], are not suitable for real-time use and would require storage of the generated texture. Spectral texturing is very easy to implement, runs on all contemporary 3d graphics cards, and is especially suitable for naturalistic textures in real-time applications which are viewed from a large range of distances.
Streams on wires: a query compiler for FPGAs
Taking advantage of many-core, heterogeneous hardware for data processing tasks is a difficult problem. In this paper, we consider the use of FPGAs for data stream processing as coprocessors in many-core architectures. We present Glacier, a component library and compositional compiler that transforms continuous queries into logic circuits by composing library components on an operator-level basis. In the paper we consider selection, aggregation, grouping, as well as windowing operators, and discuss their design as modular elements.#R##N##R##N#We also show how significant performance improvements can be achieved by inserting the FPGA into the system's data path (e.g., between the network interface and the host CPU). Our experiments show that queries on the FPGA can process streams at more than one million tuples per second and that they can do this directly from the network, removing much of the overhead of transferring the data to a conventional CPU.
Exploring component-based approaches in forest landscape modeling
Forest management issues are increasingly required to be addressed in a spatial context, which has led to the development of spatially explicit forest landscape models. The numerous processes, complex spatial interactions, and diverse applications in spatial modeling make the development of forest landscape models difficult for any single research group. New developments in componentbased modeling approaches provide a viable solution. Component-based modeling breaks a monolithic model into small, interchangeable, and binary components. They have these advantages compared to the traditional modeling work: 1) developing a component is a much smaller task than developing a whole model, 2) a component can be developed using most programming languages, since the interface format is binary, and 3) new components can replace the existing ones under the same model framework; this reduces the duplication and allows the modeling community to focus resources on the common products, and to compare results. In this paper, we explore the design of a spatially explicit forest landscape model in a component-based modeling framework, based on our work on object-oriented forest landscape modeling. We examine the representation of the major components and the interactions between them. Our goal is to facilitate the use of the component-based modeling approach at the early stage of spatially explicit landscape modeling.  2002 Elsevier Science Ltd. All rights reserved.
Extensive feature detection of N-terminal protein sorting signals
Motivation: The prediction of localization sites of various proteins is an important and challenging problem in the field of molecular biology. TargetP, by Emanuelsson et al. (J. Mol. Biol., 300, 1005‐1016, 2000) is a neural network based system which is currently the best predictor in the literature for N-terminal sorting signals. One drawback of neural networks, however, is that it is generally difficult to understand and interpret how and why they make such predictions. In this paper, we aim to generate simple and interpretable rules as predictors, and still achieve a practical prediction accuracy. We adopt an approach which consists of an extensive search for simple rules and various attributes which is partially guided by human intuition. Results: We have succeeded in finding rules whose prediction accuracies come close to that of TargetP, while still retaining a very simple and interpretable form. We also discuss and interpret the discovered rules. Availability: An (experimental) web service using rules obtained by our method is provided at http:
Optimum color masking matrix determination for digital color platemaking, using virtual color samples
Abstract#R##N##R##N#For high-fidelity color reproduction with digital color platemaking systems, it is the most important to determine the color masking matrix which converts the red, green and blue intensities of the monitor to cyan, magenta, yellow and black inks halftone dot area rates. With regard to this determination process, the author previously developed a color difference least square method by which the optimum matrix is determined, using a simulation with the Neugebauer equation and virtual color samples. First, this paper evaluates the method with actual images. Next, the method is extended for adaptive masking matrix optimization. By adapting the matrix to each color image using black ink, it was shown that the average color differences of reproduced images could be reduced to below six when black ink is used as much as possible, i.e., in the case of achromatic printing. This masking matrix adaption is a new concept which was impossible by conventional color scanner processing, but became possible by using virtual color samples.
Downlink Optimization with Interference Pricing and Statistical CSI
In this paper, we propose a downlink transmission strategy based on intercell interference pricing and a distributed algorithm that enables each base station (BS) to design locally its own beamforming vectors without relying on downlink channel state information of links from other BSs to the users. This algorithm is the solution to an optimization problem that minimizes a linear combination of data transmission power and the resulting weighted intercell interference with pricing factors at each BS and maintains the required signal-to-interference-plus-noise ratios (SINR) at user terminals. We provide a convergence analysis for the proposed distributed algorithm and derive conditions for its existence. We characterize the impact of the pricing factors in expanding the operational range of SINR targets at user terminals in a power-efficient manner. Simulation results confirm that the proposed algorithm converges to a network-wide equilibrium point by balancing and stabilizing the intercell interference levels and assigning power optimal beamforming vectors to the BSs. The results also show the effectiveness of the proposed algorithm in closely following the performance limits of its centralized coordinated beamforming counterpart.
Secure authentication watermarking for localization against the Holliman---Memon attack
Authentication watermarking schemes using block-wise watermarks for tamper localization are vulnerable to the Holliman---Memon attack. In this paper, we propose a novel method based on the Wong's localization scheme (Proceedings of the IS&T PIC, Portland) to resist this attack. A unique image index scheme is used for computing the authentication signature that is embedded in the least significant bit-plane of the block. The informed detector estimates the correct image index by using the side information about the watermarked image. The image index estimation from the fake image can definitely be an alternative to keeping a directory of image indices. So it is not necessary to manage the database of image indices for the verification purpose. The authenticity measure is defined to quantify the attack severity by taking the connectivity among possible authentic blocks into consideration. There are more blocks verified as authentic when this measure is high for a fake image constructed using this attack. As such, the blocks for a fake image can be chosen from a reduced number of database images. The blocks from any such image are to be connected with each other to maximize the authenticity measure. Thus, the attacker's task to generate a fake image of reasonable perceptual quality becomes increasingly difficult. With the proposed method there is no loss or ambiguity in localization after the Holliman---Memon attack and content tampering in an image. The localization accuracy in the proposed method is demonstrated by the simulation results and is equal to the chosen block size, similar to the Wong's scheme.
Directions of external knowledge search: investigating their different impact on firm performance in high-technology industries
Purpose – The aim of the paper is to identify the different directions of external knowledge search and to investigate their individual effect on performance at the firm level. Design/methodology/approach – The empirical study is based on survey data gathered from two distinct informants of 248 large- and medium-sized high-tech manufacturing Spanish firms. In dealing with concerns on simultaneity and reverse causality, perceived time-lags among dependent and independent variables were introduced. Quantitative methods based on questionnaire answers were used. Findings – Findings reveal six distinct external search patterns and indicate that, while market sources such as customers and competitors are positively associated with performance, knowledge acquired from general information sources, other firms beyond the core business and patents and databases have no significant effect. Moreover, knowledge obtained from science and technology organizations and from suppliers displays an inversed U-shaped effect o...
An Empirical Evaluation of the Student-Net Delay Tolerant Network
Radio equipped mobile devices have enjoyed tremendous growth in the past few years. We observe that in the near future it might be possible to build a network that routes delay-tolerant packets by harnessing user mobility and the pervasive availability of wireless devices. Such a delay-tolerant network could be used to supplement wireless infrastructure or provide service where none is available. Since mobile devices in a delay-tolerant network forward packets to nearby users, the devices can use short-range radio, which potentially reduces device power consumption and radio contention. The design of a user mobility based delay-tolerant network raises two key challenges: determining the connectivity of such a network, and determining the latency characteristics and replication requirements of routing algorithms in such a network. To determine realistic contact patterns, we collected user mobility data by conducting two user studies. We outfitted groups of students with instrumented wireless-enabled PDAs that logged pairwise contacts between study participants over a period of several weeks. Experiments conducted on these traces show that it is possible to form a delay-tolerant network based on human mobility. The network has good connectivity, so that routes exist between almost all study participants via some multi-hop path. Moreover, it is possible to effectively route packets with modest replication.
Local Strategy Improvement for Parity Game Solving
Theproblemofsolvinga paritygameis atthecore ofmanyproblemsin modelchecking,satisfiability checking and program synthesis. Some of the best algorithms for solving parity game are strategy improvement algorithms. These are global in nature since they require the entire parity game to be present at the beginning. This is a distinct disadvantagebecause in many applications one only needs to know which winning region a particular node belongs to, an daw itnessing winning strategy may cover only a fractional part of the entire game graph. We present a local strategy improvement algorithm which explores the game graph on-the-fly whilst performing the improvement steps. We also compare it empirically with existing global strategy improvementalgorithms and the currently only other local algorithm for solving parity games. It turnsout that local strategy improvementcan outperformthese othersby severalordersof magnitude.
White matter integrity, fiber count, and other fallacies: The do's and don'ts of diffusion MRI
Diffusion-weighted MRI (DW-MRI) has been increasingly used in imaging neuroscience over the last decade. An early form of this technique, diffusion tensor imaging (DTI) was rapidly implemented by major MRI scanner companies as a scanner selling point. Due to the ease of use of such implementations, and the plausibility of some of their results, DTI was leapt on by imaging neuroscientists who saw it as a powerful and unique new tool for exploring the structural connectivity of human brain. However, DTI is a rather approximate technique, and its results have frequently been given implausible interpretations that have escaped proper critique and have appeared misleadingly in journals of high reputation. In order to encourage the use of improved DW-MRI methods, which have a better chance of characterizing the actual fiber structure of white matter, and to warn against the misuse and misinterpretation of DTI, we review the physics of DW-MRI, indicate currently preferred methodology, and explain the limits of interpretation of its results. We conclude with a list of ‘Do's and Don'ts’ which define good practice in this expanding area of imaging neuroscience.
The International Technology Alliance in Network and Information Sciences
In May 2006, the US Army Research Laboratory and UK Ministry of Defense created the international technology alliance. The consortium of 26 partners including the ARL and MoD offers an open research environment in which leading US and UK companies and universities can collaborate (see table 1). It will also fuse the best aspects of the US Army's Collaborative Technology Alliances and UK MoD's Defense Technology Centers on an international scale. The ITA aims to develop flexible, distributed, and secure decision-making procedures to improve networked coalition operations. Network science is a young discipline we have limited information models and network theories to describe the behavior and scaling of large, complex mobile ad hoc networks.1 moreover, you can't understand a coalition network's performance without understanding its cognitive and sociocultural aspects and physical characteristics. A key ITA goal is to perform basic research in network-centric coalition decision making across four technical areas: network theory, security across a system of systems, sensor information processing and delivery, and distributed coalition planning and decision making, 2. we focus on the last area because this is where intelligent systems will play the biggest role.
Standoff Detection Using Millimeter and Submillimeter Wave Spectroscopy
The millimeter (MM) wave and sub-MM wave (30-600 GHz) frequency band contains fundamental rotational and vibrational resonances of many molecular gases composed of carbon, nitrogen, oxygen, and sulphur. The high specificity of rotational spectra to organic molecules affords MM wave spectroscopy having potential use in remotely sensing atmospheric pollutants and the detection of airborne chemicals is important for arms control treaty verification, intelligence collection, and environmental monitoring. This paper considers the sensitivity requirements of radiofrequency receiver systems for measuring MM wave absorption/emission signatures. The significance of receiver sensitivity and material optical depth to sensing is highlighted. A background to the technology needed for sensing at MM and sub-MM wavelengths then provides the basis for a review of MM wave spectroscopy and its role on profiling the concentrations of trace polar molecules and ionized radicals in the high altitude atmosphere. The application of the MM wave spectroscopic technique in ambient conditions is then reviewed and the issues associated with developing the technique for standoff remote sensing is discussed.
Let's talk about rings
Defining the ring topology of a molecule belongs to the central and elementary problems of cheminformatics. Questions like 'How many rings does a molecule contain?' or 'In how many rings is a specific atom involved?' are based on such a definition. Obviously, the ring topology must be unique, i.e. it does not depend on atom order, chemical meaningful, and of reasonable size, at most polynomial in the number of atoms. For a long period, the smallest set of smallest rings (SSSR) was used in cheminformatics applications ignoring a very critical flaw, namely that it is not unique even for simple structures. Other definitions like the set of relevant cycles heal this flaw; however they are sometimes chemically not meaningful and can become exponential in size. Among all attempts made, none fulfils all three criteria at the same time [1].#R##N##R##N#Recently, we developed a ring definition named 'unique ring family' (URF) and a corresponding algorithm for calculating it in polynomial time [2]. URFs match the common chemical sense of a molecule's ring topology. The definition results in a unique ring description consisting of a number of ring prototypes which is at most quadratic in the number of atoms. In this talk, we will present the algorithm, benchmarks as well as several examples demonstrating the usefulness of URFs.
Instability of submicron anisotropic liquid cylinders and jets in magnetic field
The capillary instability of a magnetically anisotropic liquid cylinder and jets, such as Nematic liquid crystals (LC), in magnetic fields, is considered using an energy approach. The boundary problem is solved in the linear approximation of the anisotropy /spl chi//sub a/ of the magnetic susceptibility /spl chi/. The effect of the anisotropy, in the region 1 |/spl chi/|>| /spl chi//sub a/| /spl chi//sup 2/, can be strong enough to counteract and even reverse the tendency of the field to enhance stabilization by increasing the cut-off wave number k/sub s/, beyond the conventional one set by Rayleigh. It is shown that the elastic effect, which is typical of LC, is significant on the scale of nano-jets, where it prevails over the magnetic effect. The jet instability is determined by surface tension, elasticity, and magnetic permeability and anisotropy. The relative influence of the elasticity and permeability on the jet stability depends on its radius. This is particularly true on the nano-scale.
Information retrieval methods for automatic speech recognition
In this paper, we use information retrieval (IR) techniques to improve a speech recognition (ASR) system. The potential benefits include improved speed, accuracy, and scalability. Where conventional HMM-based speech recognition systems decode words directly, our IR-based system first decodes subword units. These are then mapped to a target word by the IR system. In this decoupled system, the IR serves as a lightweight, data-driven pronunciation model. Our proposed method is evaluated in the Windows Live Search for Mobile (WLS4M) task, and our best system has 12% fewer errors than a comparable HMM classifier. We show that even using an inexpensive IR weighting scheme (TF-IDF) yields a 3% relative error rate reduction while maintaining all of the advantages of the IR approach.
A decision making model using soft set and rough set on fuzzy approximation spaces
In modern era of computing, there is a need of development in data analysis and decision making. Most of our tools are crisp, deterministic and precise in character. But general real life situations contains uncertainties. To handle such uncertainties many theories are developed such as fuzzy set, rough set, rough set on fuzzy approximation spaces etc. But all these theories have their own limitations. To overcome the limitations, the concept of soft set is introduced. But, soft set also fails if the attributes in the information system are almost identical rather exactly identical. In this paper, we propose a decision making model that consists of two processes such as preprocess and postprocess to mine decisions. In preprocess we use rough set on fuzzy approximation spaces to get the almost equivalence classes whereas in postprocess we use soft set techniques to obtain decisions. The proposed model is tested over an institutional dataset and the results show practical viability of the proposed research.
The influence of parental and peer attachment on Internet usage motives and addiction
The impact of parental and peer attachment on four Internet usage motives and Internet addiction was compared using path modelling of survey data from 1,577 adolescent Malaysian school students. The model accounted for 31 percent of Internet addiction score variance. Lesser parental attachment was associated with greater Internet addiction risk. Psychological escape motives were more strongly related to Internet addiction than other motives, and had the largest mediating effect upon the parental attachment–addiction relationship. Peer attachment was unrelated to addiction risk, its main influence on Internet usage motives being encouragement of use for social interaction. It is concluded that dysfunctional parental attachment has a greater influence than peer attachment upon the likelihood of adolescents becoming addicted to Internet–related activities. It is also concluded that the need to relieve dysphoria resulting from poor adolescent–parent relationships may be a major reason for Internet addiction, and that parents’ fostering of strong bonds with their children should reduce addiction risk.
An integer programming based approach for verification and diagnosis of workflows
Workflow analysis is indispensable to capture modeling errors in workflow designs. While several workflow analysis approaches have been defined previously, these approaches do not give precise feedback, thus making it hard for a designer to pinpoint the exact cause of modeling errors. In this paper we introduce a novel approach for analyzing and diagnosing workflows based on integer programming (IP). Each workflow model is translated into a set of IP constraints. Faulty control flow connectors can be easily detected using the approach by relaxing the corresponding constraints. We have implemented this diagnosis approach in a tool called DiagFlow which reads and diagnoses XPDL models using an existing open source IP solver as a backend. We show that the diagnosis approach is correct and illustrate it with realistic examples. Moreover, the approach is flexible and can be extended to handle a variety of new constraints, as well as to support new workflow patterns. Results of testing on large process models show that DiagFlow outperforms a state of the art tool like Woflan in terms of the solution time.
A hybrid wireless network enhanced with multihopping for emergency communications
This paper proposes a hybrid wireless network scheme enhanced with ad hoc networking for disaster damage assessment and emergency communications. The network aims to maintain the connection between a base station (BS) and nodes by way of multihopping. In the event that a direct link between BS and a node is disconnected, the node switches modes from cellular to ad hoc in order to access BS via neighboring nodes. A routing protocol proposed in this paper is capable of building a route using unicast-based route discovery process without route request flooding. A proposed MAC protocol satisfies the requirement of maintaining accessibility and a short delay even in emergency circumstances. We discuss an analytical model based on a Markov process. Experimental results are shown regarding reachability, throughput and delay.
A wavelet-like filter based on neuron action potentials for analysis of human scalp electroencephalographs
This paper describes the development and testing of a wavelet-like filter, named the SNAP, created from a neural activity simulation and used, in place of a wavelet, in a wavelet transform for improving EEG wavelet analysis, intended for brain-computer interfaces. The hypothesis is that an optimal wavelet can be approximated by deriving it from underlying components of the EEG. The SNAP was compared to standard wavelets by measuring Support Vector Machine-based EEG classification accuracy when using different wavelets/filters for EEG analysis. When classifying P300 evoked potentials, the error, as a function of the wavelet/filter used, ranged from 6.92% to 11.99%, almost twofold. Classification using the SNAP was more accurate than that with any of the six standard wavelets tested. Similarly, when differentiating between preparation for left- or right-hand movements, classification using the SNAP was more accurate (10.03% error) than for four out of five of the standard wavelets (9.54% to 12.00% error) and internationally competitive (7% error) on the 2001 NIPS competition test set. Phenomena shown only in maps of discriminatory EEG activity may explain why the SNAP appears to have promise for improving EEG wavelet analysis. It represents the initial exploration of a potential family of EEG-specific wavelets.
Distributed Multi Class SVM for Large Data Sets
Data mining algorithms are originally designed by assuming the data is available at one centralized site. These algorithms also assume that the whole data is fit into main memory while running the algorithm. But in today's scenario the data has to be handled is distributed even geographically. Bringing the data into a centralized site is a bottleneck in terms of the bandwidth when compared with the size of the data. In this paper for multiclass SVM we propose an algorithm which builds a global SVM model by merging the local SVMs using a distributed approach(DSVM). And the global SVM will be communicated to each site and made it available for further classification. The experimental analysis has shown promising results with better accuracy when compared with both the centralized and ensemble method. The time complexity is also reduced drastically because of the parallel construction of local SVMs. The experiments are conducted by considering the data sets of size 100s to hundred of 100s which also addresses the issue of scalability.
Across boundaries of influence and accountability: the multiple scales of public sector information systems
The use of ICTs in the public sector has long been touted for its potential to transform the institutions that govern and provide social services. The focus, however, has largely been on systems that are used within particular scales of the public sector, such as at the scale of state or national government, the scale of regional or municipal entity, or at the scale of local service providers. The work presented here takes aim at examining ICT use that crosses these scales of influence and accountability. We report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information system crosses the boundaries of these very distinct organizations. We put forward that such systems are central to the work done in the public sector and represent a class of collaborative work that has gone understudied.
Symmetric Exponential Integrators with an Application to the Cubic Schrödinger Equation
In this article, we derive and study symmetric exponential integrators. Numerical experiments are performed for the cubic Schrodinger equation and comparisons with classical exponential integrators and other geometric methods are also given. Some of the proposed methods preserve the L 2-norm and/or the energy of the system.
Limits of homology detection by pairwise sequence comparison
Motivation: Noise in database searches resulting from random sequence similarities increases as the databases expand rapidly. The noise problems are not a technical shortcoming of the database search programs, but a logical consequence of the idea of homology searches. The effect can be observed in simulation experiments. Results: We have investigated noise levels in pairwise alignment based database searches. The noise levels of 38 releases of the SwissProt database, display perfect logarithmic growth with the total length of the databases. Clustering of real biological sequences reduces noise levels, but the effect is marginal.
Business Process Development in Semantically-Enriched Environment
Middleware support for business process Management BPM has met some of the challenges with respect to encoding, performance and maintenance of workflows. A remaining challenge is complexity: business processes are becoming widely distributed, interoperating across a range of inter- and intra-organizational behaviours, vocabularies and semantics. It is important that this semantic complexity is checked and analyzed for optimality and trustworthiness prior to deployment. Petri nets are a formal method that successfully provides behavioural analysis. A shortcoming of Petri nets is that the data exchanged between business activities abstract too far away from the importance of data in actual business processes. This paper addresses this abstraction gap via additional semantic enrichment, through a two stage, model-driven approach.
Distributed dynamic scheduling for end-to-end rate guarantees in wireless ad hoc networks
We present a framework for the provision of deterministic end-to-end bandwidth guarantees in wireless ad hoc networks. Guided by a set of local feasibility conditions, multi-hop sessions are dynamically offered allocations, further translated to link demands. Using a distributed Time Division Multiple Access (TDMA) protocol nodes adapt to the demand changes on their adjacent links by local, conflict-free slot reassignments. As soon as the demand changes stabilize, the nodes must incrementally converge to a TDMA schedule that realizes the global link (and session) demand allocation.We first derive sufficient local feasibility conditions for certain topology classes and show that trees can be maximally utilized.We then introduce a converging distributed link scheduling algorithm that exploits the logical tree structure that arises in several ad hoc network applications.Decoupling bandwidth allocation to multi-hop sessions from link scheduling allows support of various end-to-end Quality of Service (QoS) objectives. We focus on the max-min fairness (MMF) objective and design an end-to-end asynchronous distributed algorithm for the computation of the session MMF rates. Once the end-to-end algorithm converges, the link scheduling algorithm converges to a TDMA schedule that realizes these rates.We demonstrate the applicability of this framework through an implementation over an existing wireless technology. This implementation is free of restrictive assumptions of previous TDMA approaches: it does not require any a-priori knowledge on the number of nodes in the network nor even network-wide slot synchronization.
Evaluating Various Branch-Prediction Schemes for Biomedical-Implant Processors
This paper evaluates various branch-prediction schemes under different cache configurations in terms of performance, power, energy and area on suitably selected biomedical workloads. The benchmark suite used consists of compression, encryption and data-integrity algorithms as well as real implant applications, all executed on realistic biomedical input datasets. Results are used to drive the (micro)architectural design of a novel microprocessor targeting microelectronic implants. Our profiling study has revealed that, under strict or relaxed area constraints and regardless of cache size, the ALWAYS TAKEN and ALWAYS NOT-TAKEN static prediction schemes are, in almost all cases, the most suitable choices for the envisioned implant processor. It is further shown that bimodal predictors with small Branch-Target-Buffer (BTB) tables are suboptimal yet also attractive solutions when processor I/D-cache sizes are up to 1024KB/512KB, respectively.
A New User Authentication Protocol for Mobile Terminals in Wireless Network
For constructing a ubiquitous network, the highspeed wireless LAN (WLAN) attracts attention as an infrastructure for global access. However, some issues are impeding further adoption of the technology, in particular, security problems including user authentication, message compromising, password theft, connection hijacking, etc. In this paper, we discuss a fast authentication method of mobile ubiquitous terminals in WLAN. To achieve an efficient access control between Access Points (APs) and mobile terminals and sharing of a session key between terminals, we propose a new user secure authentication method and a session key distribution protocol based on service ticket issuing system.
On the Basis of the Generated Foundation of Blended E-learning - Transcendence and Integration
Learning theory develops in the constant process of transcendence and integration. Not only from knowledge to people, but also its theory approaches are all beyond each other and integration. Therefore, the emergence of blended e_learning is inevitable, which is based on the theoretical study?s transcendence and integration, and as a learning concepts and theories, blended e_learning is also bound to each other than with the integration. Such mutual transcendence and integration of learning theory is just an important basis for the starting point.
An Improved Signcryption Scheme and Its Variation
Signcryption is a new cryptographic primitive which simultaneously provides both confidentiality and authenticity. This paper proposes an improved signcryption scheme and a variant scheme providing message recovery. The first scheme is revised from an authenticated encryption scheme which has been found to have a security-flaw. Our scheme solves the security-flaw and provides an additional property called the public verifiability of the signature. The second scheme is a message recovery type. It surpasses most of the current signcryption schemes on the size of the signcrypted ciphertext. That is, in our second scheme, we require only two parameters, (r, s), with r epsi Z p  and s epsi Z  q  while most signcryption schemes require three parameters (c, r, s) with the additional parameter c epsi Z p . This second scheme is modified from an authenticated encryption scheme with message recovery and surpasses the based authenticated encryption scheme on the property of non-repudiation of the origin
A new network architecture with intelligent node (IN) to enhance IEEE 802.14 HFC networks
In the hybrid fiber/coax (HFC) architecture, over several hundreds subscribers in CATV (community antenna TV) network may cause serious collisions. In this paper, we propose a new network architecture which using an intelligent node (IN) to stand for a group of subscribers to request the demand resources. The IN has the ability to reduce the collision probability as well as the collision resolving period. The simulation results show that the proposed architecture in terms of throughput, buffer delay, and fairness outperforms the standard architecture.
Dynamic load balancing schemes for computing accessible surface area of protein molecules
This paper presents an experimental study of dynamic load balancing methods for a parallelized solution to a well-known problem in computational molecular biology: computing the accessible surface areas (ASA) of proteins. The main contribution is a better understanding of how certain techniques for load estimation and redistribution must be combined carefully for effectiveness and how these combinations need to change during the course of a computation. In particular, the Shrake-Rupley ASA algorithm is implemented and three aspects of dynamic load balancing are studied: how to estimate load imbalance (the estimation problem); when to invoke load redistribution (the invocation problem); and how to load balance (the mapping problem). The results in this paper show that a dynamically-selected mix of algorithms in each category that adapts to changing structure within the protein works better than a static periodic application of a static mix of algorithms.
Level crossing rate and average fade duration of MRC and EGC diversity in Ricean fading
The average level crossing rate and average fade duration of the output signal of a maximal ratio combiner (MRC) and equal gain combiner (EGC), operating on independent Ricean fading input branch signals, are derived. Exact, closed-form results are obtained for MRC diversity, while precise expressions for EGC diversity are presented with an infinite series method. The results are valid for an arbitrary number of independent, identically distributed diversity branches, isotropic scattering, and a specular component perpendicular to the line of motion of the mobile.
Design and implementation of WIRE Diameter
This paper presents the design and implementation of WIRE Diameter. The WIRE Diameter is an open source implementation of Diameter Based Protocol and Diameter EAP application developed by the Wireless Internet Research & Engineering (WIRE) Laboratory. Research has shown that traditional RADIUS protocol may suffer performance degradation and data loss in a large system. Diameter, thus, was proposed to address the deficiencies in RADIUS. Both 3GPP and 3GPP2 have adopted Diameter as their AAA protocol. The WIRE Diameter could be used to authenticate and authorize 802.1x supplicant. It provides various authentication schemes, including EAP-MD5, EAP-TLS, EAP-TTLS, and PEAP. The WIRE Diameter is developed to be independent of OS as much as possible. Currently it supports Linux, FreeBSD and various versions of MS Windows. It is believed that the WIRE Diameter is the first open source implementation of Diameter EAP Application in the world. The source code can be downloaded freely. The WIRE Diameter should be useful for the research community. This paper demonstrates the design and implementation of the WIRE Diameter.
Stability and performance of intersecting aircraft flows under decentralized conflict avoidance rules
This paper considers the problem of two intersecting aircraft flows under decentralized conflict resolution rules. Considering aircraft flowing through a fixed control volume, new air traffic control models and scenarios are defined that enable the study of long-term aircraft flow stability. For a class of two intersecting aircraft flows, this paper considers conflict scenarios involving arbitrary encounter angles. It is shown that aircraft flow stability, defined both in terms of safety and performance, is preserved under the decentralized conflict resolution algorithm considered. It is shown that the lateral deviations experienced by aircraft in each flow are bounded.
Informational acquisition and cognitive models
Abstract#R##N##R##N#Life forms must organize information into cognitive models reflecting the outside environment, and in a complex and changing environment a life form must constantly select and organize this mass of information to avoid slipping into a chaotic cognitive state. The task of developing and maintaining adaptive cognitive models can be understood through two processes, crucial to regulating the interconnections between environmental elements. The inclusion and exclusion of information follows a process designated by P and the process by which cognitive models change is designated by K. Higher order concepts are created by reducing the interconnections between elements to a minimal number to avoid cognitive chaos. © 2004 Wiley Periodicals, Inc. Complexity 9:31–37, 2004
Contextual motion field-based distance for video analysis
In this work, we propose a general method for computing distance between video frames or sequences. Unlike conventional appearance-based methods, we first extract motion fields from original videos. To avoid the huge memory requirement demanded by the previous approaches, we utilize the “bag of motion vectors” model, and select Gaussian mixture model as compact representation. Thus, estimating distance between two frames is equivalent to calculating the distance between their corresponding Gaussian mixture models, which is solved via earth mover distance (EMD) in this paper. On the basis of the inter-frame distance, we further develop the distance measures for both full video sequences.#R##N##R##N#Our main contribution is four-fold. Firstly, we operate on a tangent vector field of spatio-temporal 2D surface manifold generated by video motions, rather than the intensity gradient space. Here we argue that the former space is more fundamental. Secondly, the correlations between frames are explicitly exploited using a generative model named dynamic conditional random fields (DCRF). Under this framework, motion fields are estimated by Markov volumetric regression, which is more robust and may avoid the rank deficiency problem. Thirdly, our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering. Lastly, our definition for frame distance allows for partial distance.
Minimum effort inverse kinematics for redundant manipulators
This paper investigates the use of an infinity norm in formulating the optimization measures for computing the inverse kinematics of redundant arms. The infinity norm of a vector is its maximum absolute value component and hence its minimization implies the determination of a minimum effort solution as opposed to the minimum-energy criterion associated with the Euclidean norm. In applications where individual magnitudes of the vector components are of concern, this norm represents the physical requirements more closely than does the Euclidean norm. We first study the minimization of the infinity-norm of the joint velocity vector itself, and discuss its physical interpretation. Next, a new method of optimizing a subtask criterion, defined using the infinity-norm, to perform additional tasks such as obstacle avoidance or joint limit avoidance is introduced. Simulations illustrating these methods and comparing the results with the Euclidean norm solutions are presented.
A Traceability Technique for Specifications
Traceability in software involves discovering links between different artifacts, and is useful for a myriad of tasks in the software life cycle. We compare several different Information Retrieval techniques for this task, across two datasets involving real-world software with the accompanying specifications and documentation. The techniques compared include dimensionality reduction methods, probabilistic and information theoretic approaches, and the standard vector space model.
A generic tension-closure analysis method for fully-constrained cable-driven parallel manipulators
Cable-driven parallel manipulators (CDPMs) are a special class of parallel manipulators that are driven by cables instead of rigid links. Due to the unilateral property of the cables, all the driving cables in a fully-constrained CDPM must always maintain positive tension. As a result, tension analysis is the most essential issue for these CDPMs. By drawing upon the mathematical theory from convex analysis, a sufficient and necessary tension-closure condition is proposed in this paper. The key point of this tension-closure condition is to construct a critical vector that must be positively expressed by the tension vectors associated with the driving cables. It has been verified that such a tension-closure condition is general enough to cater for CDPMs with different numbers of cables and DOFs. Using the tension-closure condition, a computationally efficient algorithm is developed for the tension-closure pose analysis of CDPMs, in which only a limited set of deterministic linear equation systems need to be resolved. This algorithm has been employed for the tension-closure workspace analysis of CDPMs and verified by a number of computational examples. The computational time required by the proposed algorithm is always shorter as compared to other existing algorithms.
Parallel Support Vector Machines: The Cascade SVM
We describe an algorithm for support vector machines (SVM) that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors. Instead of analyzing the whole training set in one optimization step, the data are split into subsets and optimized separately with multiple SVMs. The partial results are combined and filtered again in a 'Cascade' of SVMs, until the global optimum is reached. The Cascade SVM can be spread over multiple processors with minimal communication overhead and requires far less memory, since the kernel matrices are much smaller than for a regular SVM. Convergence to the global optimum is guaranteed with multiple passes through the Cascade, but already a single pass provides good generalization. A single pass is 5x - 10x faster than a regular SVM for problems of 100,000 vectors when implemented on a single processor. Parallel implementations on a cluster of 16 processors were tested with over 1 million vectors (2-class problems), converging in a day or two, while a regular SVM never converged in over a week.
Estimation of heart-surface potentials using regularized multipole sources
Direct inference of heart-surface potentials from body-surface potentials has been the goal of most recent work on electrocardiographic inverse solutions. We developed and tested indirect methods for inferring heart-surface potentials based on estimation of regularized multipole sources. Regularization was done using Tikhonov, constrained-least-squares, and multipole-truncation techniques. These multipole-equivalent methods (MEMs) were compared to the conventional mixed boundary-value method (BVM) in a realistic torso model with up to 20% noise added to body-surface potentials and /spl plusmn/1 cm error in heart position and size. Optimal regularization was used for all inverse solutions. The relative error of inferred heart-surface potentials of the MEM was significantly less (p<0.05) than that of the BVM using zeroth-order Tikhonov regularization in 10 of the 12 cases tested. These improvements occurred with a fourth-degree (24 coefficients) or smaller multipole moment. From these multipole coefficients, heart-surface potentials can be found at an unlimited number of heart-surface locations. Our indirect methods for estimating heart-surface potentials based on multipole inference appear to offer significant improvement over the conventional direct approach.
Towards runtime testing in automotive embedded systems
Runtime testing is a common way to detect faults during normal system operation. To achieve a specific diagnostic coverage runtime testing is also used in safety critical, automotive embedded systems. In this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard real-time embedded system as applied to the automotive domain. Special emphasis is put to timing requirements of embedded systems with respect to hard real-time and concurrent hardware resource accesses of runtime tests and tasks running on the target system.
An Environment for (re)configuration and Execution Managenment of Flexible Radio Platforms
This paper presents the Flexible Radio Kernel (FRK), a configuration and execution management environment for hybrid hardware/software flexible radio platform. The aim of FRK is to manage platform reconfiguration for multi-mode, multi-standard operation, with different levels of abstraction. A high level framework is described, to manage multiple MAC layers, and to enable MAC cooperation algorithms for cognitive radio. A low-level environment is also available to manage platform reconfiguration for radio operations. Radio can be implemented using hardware or software elements. Configuration state is hidden to the high-level layers, offering pseudo concurrency (time sharing) properties. This study presents a global view of FRK, with details on some specific parts of the environment. A practical study with algorithmic description is presented.
A simple approach to evaluate the ergodic capacity and outage probability of correlated Rayleigh diversity channels with unequal signal-to-noise ratios
In this article, we propose a novel method to derive exact closed-form ergodic capacity and outage probability expressions for correlated Rayleigh fading channels with receive diversity. Unlike the existing works, the proposed method employ a simple approach for the capacity and outage analysis for receiver diversity channels operating at different signal-to-noise ratios depicted in the diagonal elements of matrix Ω. With x being the channel gain vector, random variable of the form Y(a)=a + x∗Ω x is considered. Novelty of the work resides in the fact that the distribution of Y(a) is accurately determined by employing Fourier representation of unit step function followed by complex integration in a straight forward way. The ergodic channel capacity is thus calculated by using the first-order moment, #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      E#N#                      [#N#                      #N#                        #N#                          log#N#                        #N#                        #N#                          2#N#                        #N#                      #N#                      (#N#                      Y#N#                      (#N#                      1#N#                      )#N#                      )#N#                      ]#N#                    #N#                  #N#                , while the outage probability for a certain threshold γ0is evaluated using #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      #N#                        #N#                          ∫#N#                        #N#                        #N#                          0#N#                        #N#                        #N#                          #N#                            #N#                              γ#N#                            #N#                            #N#                              0#N#                            #N#                          #N#                        #N#                      #N#                      #N#                        #N#                          f#N#                        #N#                        #N#                          Y#N#                          (#N#                          0#N#                          )#N#                        #N#                      #N#                      (#N#                      y#N#                      )#N#                      dy#N#                    #N#                  #N#                . Extensive experiments have been conducted demonstrating the accuracy of the proposed approach.
On the feasibility of synthesizing CAD software from specifications: generating maze router tools in ELF
The application of program synthesis techniques to the generation of technology-sensitive VLSI physical design tools is described. The architecture and implementation of a particular software generator (called ELF) targeted at the generation of maze routing software is described. ELF strives to meet the demands of the target technology by automatically generating maze router implementations to match the application requirements. ELF has three key features. First, a very high level language, lacking data structure implementation specifications, is used to describe algorithm design styles. Second, application-specific expertise about routing and application independent code synthesis techniques are used to guide search among alternative design styles for algorithms and data structures. Third, code generation is used to transform the resulting abstract descriptions of selected algorithms and data structures into final, executable code. Code generation is an incremental, stepwise refinement process. Experimental results are presented covering several correct. fully functional routers synthesized by ELF from varying high-level specifications. Results from synthetic and industrial benchmarks are examined to illustrate ELF's capabilities. >
Canonical Sequence Directed Tactics Analyzer for Computer Go Games
We present an approach used in CSDTA (canonical sequence directed tactics analyzer) that uses canonical sequences (Joseki) in hoping to improve computer Go programs. We collect 1278 canonical sequences and their deviations in our system. Instead of trivially matching the current game to the collected sequences, we define a notion of similarity to extract the most suitable move from the candidate sequences for the next move. The simplicity of our method and its positive outcome make our approach a promising tool to be integrated into a complete computer Go program for a foreseeable improvement
An improved approximation to the estimation of the critical F values in best subset regression.
Variable selection methods are routinely applied in regression modeling to identify a small number of descriptors which "best" explain the variation in the response variable. Most statistical packages that perform regression have some form of stepping algorithm that can be used in this identification process. Unfortunately, when a subset of p variables measured on a sample of n objects are selected from a set of k(>p) to maximize the squared sample multiple regression coefficient, the significance of the resulting regression is upwardly biased. The extent of this bias is investigated by using Monte Carlo simulation and is presented as an inflation factor which when multiplied by the usual tabulated F ratio gives an estimate of the true 5% critical value. The results show that selection bias can be very high even for moderate-size data sets. Selecting three variables from 50 generated at random with 20 observations will almost certainly provide a significant result if the usual tabulated F values are used. An interpolation formula is provided for the calculation of the inflation factor for different combinations of (n, p, k). Four real data sets are examined to illustrate the effect of correlated descriptor variables on the degree of inflation.
MultiK-MHKS: A Novel Multiple Kernel Learning Algorithm
In this paper, we develop a new effective multiple kernel learning algorithm. First, we map the input data into m different feature spaces by m empirical kernels, where each generated feature space is taken as one view of the input space. Then, through borrowing the motivating argument from Canonical Correlation Analysis (CCA) that can maximally correlate the m views in the transformed coordinates, we introduce a special term called Inter-Function Similarity Loss R IFSI . into the existing regularization framework so as to guarantee the agreement of multiview outputs. In implementation, we select the Modification of Ho-Kashyap algorithm with Squared approximation of the misclassification errors (MHKS) as the incorporated paradigm and the experimental results on benchmark data sets demonstrate the feasibility and effectiveness of the proposed algorithm named MultiK-MHKS.
On the rank of random sparse matrices
We investigate the rank of random (symmetric) sparse matrices. Our main finding is that with high probability, any dependency that occurs in such a matrix is formed by a set of few rows that contains an overwhelming number of zeros. This allows us to obtain an exact estimate for the co-rank.
Maximum Utility Peer Selection for P2P Streaming in Wireless Ad Hoc Networks
In the recent years, the peer-to-peer (P2P) overlay network has been a promising architecture for multimedia streaming services besides its common use for efficient file sharing. By simply increasing the number of peers, the P2P overlay network can meet the high bit rate requirements of multimedia applications. Optimal peer selection for newly joining peers is one of the important problems, especially in wireless networks which have limited resources and capacity, since the peer selection process has a direct impact on the throughput of the underlay network and the co-existing unicast traffic. In this paper we tackle the problem of peer selection for streaming applications over wireless ad hoc networks. We devise a novel peer selection algorithm which maximizes the throughput of the underlay network, and at the same time makes P2P streaming friendly towards the co-existing data traffic. The proposed receiver based rate allocation and peer selection (RPS) algorithm is derived using the network utility maximization (NUM) framework. The algorithm solves the peer selection and rate allocation problem distributedly while optimally adapting the medium access control (MAC) layer parameters and is easily extensible to large P2P networks. Simulation results show that by using the proper price exchange mechanism, the peer receivers can effectively maximize the throughput of the underlay network by intelligently selecting its source peers.
Time‐dependent groundwater modeling using spreadsheet
Abstract#R##N##R##N#Time-dependent groundwater modeling using spreadsheet simulation (TGMSS) model is developed as solution technique. It is a practical method that uses spreadsheets instead of the conventional solution methods. All of the aquifer parameters can easily be described in TGMSS model. The results of TGMSS are validated with MODFLOW. Results showed that TGMSS and MODFLOW results were in good agreement in terms of resulting values of hydraulic heads. © 2005 Wiley Periodicals, Inc. Comput Appl Eng Educ 13: 192–199, 2005; Published online in Wiley InterScience (www.interscience.wiley.com); DOI 10.1002/cae.20048
A Simulation Tool to Study High-Frequency Chest Compression Energy Transfer Mechanisms and Waveforms for Pulmonary Disease Applications
High-frequency chest compression (HFCC) can be used as a therapeutic intervention to assist in the transport and clearance of mucus and enhance water secretion for cystic fibrosis patients. An HFCC pump-vest and half chest-lung simulation, with 23 lung generations, has been developed using inertance, compliance, viscous friction relationships, and Newton's second law. The simulation has proven to be useful in studying the effects of parameter variations and nonlinear effects on HFCC system performance and pulmonary system response. The simulation also reveals HFCC waveform structure and intensity changes in various segments of the pulmonary system. The HFCC system simulation results agree with measurements, indicating that the HFCC energy transport mechanism involves a mechanically induced pulsation or vibration waveform with average velocities in the lung that are dependent upon small air displacements over large areas associated with the vest-chest interface. In combination with information from lung physiology, autopsies and a variety of other lung modeling efforts, the results of the simulation can reveal a number of therapeutic implications.
Distributed-information neural control: the case of dynamic routing in traffic networks
Large-scale traffic networks can be modeled as graphs in which a set of nodes are connected through a set of links that cannot be loaded above their traffic capacities. Traffic flows may vary over time. Then the nodes may be requested to modify the traffic flows to be sent to their neighboring nodes. In this case, a dynamic routing problem arises. The decision makers are realistically assumed 1) to generate their routing decisions on the basis of local information and possibly of some data received from other nodes, typically, the neighboring ones and 2) to cooperate on the accomplishment of a common goal, that is, the minimization of the total traffic cost. Therefore, they can be regarded as the cooperating members of informationally distributed organizations, which, in control engineering and economics, are called team organizations. Team optimal control problems cannot be solved analytically unless special assumptions on the team model are verified. In general, this is not the case with traffic networks. An approximate resolutive method is then proposed, in which each decision maker is assigned a fixed-structure routing function where some parameters have to be optimized. Among the various possible fixed-structure functions, feedforward neural networks have been chosen for their powerful approximation capabilities. The routing functions can also be computed (or adapted) locally at each node. Concerning traffic networks, we focus attention on store-and-forward packet switching networks, which exhibit the essential peculiarities and difficulties of other traffic networks. Simulations performed on complex communication networks point out the effectiveness of the proposed method.
Matching of PDB chain sequences to information in public databases as a prerequisite for 3D functional site visualization
The 3D structures of biomacromolecules stored in the Protein Data Bank [1] were correlated with different external, biological information from public databases. We have matched the feature table of SWISS-PROT [2] entries as well InterPro [3] domains and function sites with the corresponding 3D-structures. OMIM [4] (Online Mendelian Inheritance in Man) records, containing information of genetic disorders, were extracted and linked to the structures. The exhaustive all-against-all 3D structure comparison of protein structures stored in DALI [5] was condensed into single files for each PDB entry. Results are stored in XML format facilitating its incorporation into related software. The resulting annotation of the protein structures allows functional sites to be identified upon visualization. Availability: http://leger.gbf.de/PDBXML/
$L^2$ Optimization in Discrete FIR Estimation: Exploiting State-Space Structure
This paper studies the $L^2$ (mean-square) optimal design of discrete-time FIR estimators. A solution procedure, which reduces the problem to a static matrix optimization problem admitting a closed-form solution, is proposed. In the latter solution, a special state-space structure of the associated matrices is exploited to obtain efficient formulae with the computational complexity proportional to the length of the impulse response of the estimator. Unlike previously available least-square FIR results, our treatment does not impose unnecessarily restrictive assumptions on the process dynamics and can handle interpolation constraints on the unit circle, which facilitates the inclusion of steady-state performance requirements.
Application of plane waves for accurate measurement of microwave scattering from geophysical surfaces
The authors utilized the concept of a compact antenna range to obtain plane-wave illumination to accurately measure scattering properties of simulated sea ice. They also made simultaneous measurements using conventional antennas. Measured scattering coefficients obtained with the plane-wave system at 10 GHz decreased by about 35 dB when the incidence angle increased from 0/spl deg/ to 10/spl deg/. Scattering coefficients derived from data collected with the radar system at 13.5 GHz using conventional far-field antennas decreased by about 20 dB over the same angular region. This demonstrates that the far-field properties of a widebeam antenna are inadequate for measuring the angular scattering response of smooth surfaces. They believe that application of the compact antenna range concept for scattering measurements has a wide range of applications and is the solution to the long-standing problem of how to directly measure scattering consisting of coherent and incoherent components. >
A Parallelized Surface Extraction Algorithm for Large Binary Image Data Sets Based on an Adaptive 3-D Delaunay Subdivision Strategy
In this paper, we describe a novel 3D subdivision strategy to extract the surface of binary image data. This iterative approach generates a series of surface meshes that capture different levels of detail of the underlying structure. At the highest level of detail, the resulting surface mesh generated by our approach uses only about 10 percent of the triangles in comparison to the Marching Cube (MC) algorithm, even in settings where almost no image noise is present. Our approach also eliminates the so-called "staircase effect," which voxel-based algorithms like the MC are likely to show, particularly if nonuniformly sampled images are processed. Finally, we show how the presented algorithm can be parallelized by subdividing 3D image space into rectilinear blocks of subimages. As the algorithm scales very well with an increasing number of processors in a multithreaded setting, this approach is suited to process large image data sets of several gigabytes. Although the presented work is still computationally more expensive than simple voxel-based algorithms, it produces fewer surface triangles while capturing the same level of detail, is more robust toward image noise, and eliminates the above-mentioned "staircase" effect in anisotropic settings. These properties make it particularly useful for biomedical applications, where these conditions are often encountered.
AutoVision - flexible processor architecture for video-assisted driving
Summary form only given. Future automotive security systems will benefit from visual scene analysis based on a fusion of video, infrared, and radar images. Today we have already functions like lane departure warning and automatic cruise control (ACC) for pretty well defined driving environments, such as highways and primary roads. Recent research activities concentrate on more complex environments, such as city traffic with a wide variety of traffic participants moving in an unpredictable manner, e.g. bikes, pedestrians, children, and even animals, and under changing weather and lighting conditions. The ITRS semiconductor roadmap for microelectronics forecasts a continued doubling of transistor capacity per chip every 2 to 2.5 years enabling billion transistor ASIC designs in the near future. Multi processor system on chip (MPSoC) solutions with 8, 16 or even more standard RISC CPU cores, mega-bytes of fast (ns access latencies) on-chip SRAM memories, giga-byte per second interconnect buses or NoC (network on chip) meshes, high-speed serial I/Os and, last but not least, million gate equivalent dedicated hardware accelerator functions in eFPGA (embedded field programmable gate array) logic are becoming reality on a single silicon substrate. Examples of current research projects shall illustrate our perception on how this tremendous increase in functionality and computational performance per chip area may impact automotive control unit (ACU) architectures for driver assistance applications. The AutoVision processor is a dynamically reconfigurable MPSoC prototype where video-specific pixel processing engines are on-the-fly loaded or exchanged without interrupting regular system operations. For the time being, pixel processing engines cover functions such as object edge detection or luminance segmentation, and are implemented as dedicated hardware accelerators to ensure real-time frame processing capabilities of the AutoVision processor. Dynamic replacement of processing engines ensures an automatic and area efficient adaptation to various driving conditions. Segmented objects are, in a subsequent step, characterized by means of standard MPEG-7 descriptors and entered as search criteria into traffic scene analysis databases. Goal is to obtain a clean distinction between passenger cars, trucks, and big rectangular traffic signs, and to identify pedestrians or bikers in complex traffic situations. The AutoVision processor project is supported by the German Research Foundation (DFG) in the special emphasis research programme "reconfigurable computing".
TALP at GikiCLEF 2009
This paper describes our experiments in Geographical Information Retrieval with the Wikipedia collection in the context of our participation in the GikiCLEF 2009 Multilingual task in English and Spanish. Our system, called gikiTALP, follows a very simple approach that uses standard Information Retrieval with the Sphinx full-text search engine and some Natural Language Processing techniques without Geographical Knowdledge.
Transmit Beamforming for Frequency-Selective Channels
In this paper, we propose beamforming schemes for frequency-selective channels with decision-feedback equalization (DFE) at the receiver. We consider both finite impulse response (FIR) and infinite impulse response (IIR) beamforming filters (BFFs). In case of IIR beamforming, we are able to derive closed-form expressions for the optimum BFFs. In addition, we provide an efficient numerical method for recursive calculation of the optimum FIR BFFs. Simulation and numerical results for typical GSM/EDGE channels confirm the significant performance gains achievable with beamforming compared to single-antenna transmission and optimized delay diversity.
All possible second-order four-impedance two-stage Colpitts oscillators
The authors report all the possible four-impedance settings that yield a valid second-order two-stage Colpitts oscillator. These settings are obtained following an exhaustive search conducted on two possible structures of the oscillator modelled through two-port network transmission parameters. Only valid second-order cases with a maximum of three reactive elements are reported. Experimental and Spice verification of a selected example using both MOS and BJT transistors is given.
Mining Frequent Sequential Patterns under a Similarity Constraint
Many practical applications are related to frequent sequential pattern mining, ranging from Web Usage Mining to Bioinformatics. To ensure an appropriate extraction cost for useful mining tasks, a key issue is to push the user-defined constraints deep inside the mining algorithms. In this paper, we study the search for frequent sequential patterns that are also similar to an user-defined reference pattern. While the effective processing of the frequency constraints is well-understood, our contribution concerns the identification of a relaxation of the similarity constraint into a convertible anti-monotone constraint. Both constraints are then used to prune the search space during a levelwise search. Preliminary experimental validations have confirmed the algorithm efficiency.
Dispatching Petroleum Products
Petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers. Proper dispatching of shipments of such products, packaged and in bulk, may result in significant transportation and inventory cost savings. This work examines the variety of operational environments which exist in dispatching petroleum products, and the operations research tools used by oil companies to dispatch such products. In addition, it identifies gaps where additional research is needed.
CLASSIFICATION, AVERAGING AND RECONSTRUCTION OF MACROMOLECULES IN ELECTRON TOMOGRAPHY
Electron tomography provides opportunities to determine three-dimensional cellular architecture at resolutions high enough to identify individual macromolecules such as proteins. Image analysis of such data poses a challenging problem due to the extremely low signal-to-noise ratios that makes individual volumes simply too noisy to allow reliable structural interpretation. This requires using averaging techniques to boost the signal-to-noise ratios, a common practice in electron microscopy single particle analysis where they have proven to be very powerful in elucidating high resolution structure. Although there are significant similarities in the way data is processed, several new problems arise in the tomography case that have to be properly dealt with. Such problems involve dealing with the missing wedge characteristic of limited angle tomography, the need for robust and efficient 3D alignment routines, and design of methods that account for diverse conformations through the use of classification. We present a framework for reconstruction via alignment, classification and averaging of volumes obtained from limited angle electron tomography, providing a powerful tool for high resolution structure determination and description of conformational variability in a biological context
Performance analysis of the conventional complex LMS and augmented complex LMS algorithms
Recently, the augmented complex LMS (ACLMS) algorithm has been proposed for modeling complex-valued signal relationships in which a widely-linear model can be more appropriate [1]. It is not clear, however, how the behavior of ACLMS differs from that of the conventional complex LMS (CCLMS) algorithm. In this paper, we leverage a recently-developed analysis for the complex LMS algorithm [2] to illuminate the performance relationships between the ACLMS and CCLMS algorithms. Our analysis shows that the ACLMS algorithm can potentially achieve a lower steady-state mean-squared error as compared to that of CCLMS, but the convergence speed of ACLMS is slowed in the presence of highly non-circular complex-valued input signals. An adaptive beamforming example indicates the utility of the results.
Extending wireless sensor network lifetime through order-based genetic algorithm
Extending the lifetime is a key issue in wireless sensor networks. An effective way to extend the lifetime is to partition the sensors into several covers and activate the covers one by one. Thus, the more the covers, the longer the lifetime. To find the maximum number of covers has been modeled as the Set K-Cover problem. In this paper we propose using order-based genetic algorithm to solve the Set K-Cover problem for extending the lifetime of wireless sensor networks. The proposed algorithm needs neither an upper bound nor any assumption about the maximum number of covers. Experimental results show that the order-based genetic algorithm can achieve near-optimal solutions efficiently.
Robust neural predictor for noisy chaotic time series prediction
A robust neural predictor is designed for noisy chaotic time series prediction in this paper. The main idea is based on the consideration of the bounded uncertainty in predictor input, and it is a typical Errors-in-Variables problem. The robust design is based on the linear-in-parameters ESN (Echo State Network) model. By minimizing the worst-case residual induced by the bounded perturbations in the echo state variables, the robust predictor is obtained in coping with the uncertainty in the noisy time series. In the experiment, the classical Mackey-Glass 84-step benchmark prediction task is investigated. The prediction performance is studied for the nominal and robust design of ESN predictors.
Minimization strategies for maximally parallel multiset rewriting systems
Maximally parallel multiset rewriting systems (MPMRS) give a convenient way to express relations between unstructured objects. The functioning of various computational devices may be expressed in terms of MPMRS (e.g., register machines and many variants of P systems). In particular, this means that MPMRS are Turing universal; however, a direct translation leads to quite a large number of rules. Like for other classes of computationally complete devices, there is a challenge to find a universal system having the smallest number of rules. In this article we present different rule minimization strategies for MPMRS based on encodings and structural transformations. We apply these strategies to the translation of a small universal register machine (Korec (1996) [9]) and we show that there exists a universal MPMRS with 23 rules. Since MPMRS are identical to a restricted variant of P systems with antiport rules, the results we obtained improve previously known results on the number of rules for those systems.
The Introduction of the OSCAR Database API (ODA)
The OSCAR [14] cluster installation toolkit was created by the Open Cluster Group (OCG) for one particular type of High Performance Computing (HPC) cluster. OSCAR is currently one of the widely used cluster installation toolkits; it boasts hundreds of thousands of downloads and active mailing lists. OSCAR has expanded its area with several sub-projects targeting other types of HPC clusters. Each of these projects share a core set of OSCAR code, including the OSCAR Database and its access API, "ODA" (OSCAR Database API). The ODA abstraction layer, consisting of a database schema and corresponding API, hides a commodity back-end database (e.g., MySQL [15]). Because OSCAR and its sub-projects are targeted at new, innovative environments (including non-HPC environments), there are significant issues with managing various configurations of each project. For example, as we previously showed [8], previous versions of ODA were unable to represent the complex, ever-growing set of data required to accurately describe the clusters that it manages. Further, its API was extremely complex, requiring a steep learning curve for OSCAR developers. Therefore, we have designed and implemented a new database schema to deal with these issues. This new version of ODA has not only resolved the above problems but also, as proposed in our previous paper, enabled storage and retrieval of various configuration information, and encouraged data re-use between the main OSCAR project and its derivative projects. In addition, the new version of ODA has sped up the OSCAR installation process. This document presents a simpler, highly flexible design and implementation of ODA slated to be included in OSCAR v5.0. It also suggests a blueprint for maintaining the database modules of ODA in a systematic, organized way.
Learning for Sustainability Transition through Bounded Socio-technical Experiments in Personal Mobility
Abstract A bounded socio-technical experiment (BSTE) attempts to introduce a new technology, service, or a social arrangement on a small scale. Many such experiments in personal mobility are ongoing worldwide. They are carried out by coalitions of diverse actors, and are driven by long term and large scale visions of advancing society’s sustainability agenda. This paper focuses on the processes of higher-order learning that occur through BSTEs. Based on the conceptual frameworks from theories of organizational learning, policy-oriented learning, and diffusion of innovation, we identify two types of learning: the first type occurs among the participants in the experiment and their immediate professional networks; the second type occurs in the society at large. Both types play a key role in the societal transition towards sustainable mobility systems. Two case studies, in which the Design for Sustainability Group at Technical University of Delft has participated, provide empirical data for the analysis. One...
Attitude Control of a Quadruped Trot While Turning
During a complete running stride, which involves significant periods of flight during which no legs are contacting the ground, a quadruped cannot employ static stability techniques. Instead, the corrective forces necessary to maintain dynamic stability must be applied during the short stance intervals inherent to high-speed running. Because of this complexity and the large coupled forces required to run, much of the research on the control of quadruped running has focused on planar systems which are not required to simultaneously control attitude in all three dimensions. The 3D trot controller presented here overcomes these and other complexities to control a trot up to 3.75 m/s, approximately 3 body lengths per second, and turning rates up to 20 deg/s. The biomimetic method of banking into a high-speed turn is also investigated here. Along with the details of the attitude control algorithm, a set of control principles for high-speed legged motion is presented. These principles, such as the need to counteract the disturbance of swing leg return and the usefulness of force redistribution during stance, are not dependent on a particular scale or actuation scheme and can be applied to a wider range of legged systems.
Robust fuzzy and recurrent neural network motion control among dynamic obstacles for robot manipulators
An integration of a fuzzy controller and modified Elman neural networks (NN) approximation-based computed-torque controller is proposed for motion control of autonomous manipulators in dynamic and partially known environments containing moving obstacles. The navigation technique of robot control using artificial potential fields is based on the fuzzy controller. The NN controller can deal with unmodeled bounded disturbances and or unstructured unmodeled dynamics of the robot arm. The NN weights are tuned online, with no off-line learning phase required. The stability of the closed-loop system is guaranteed by the Lyapunov theory. The purpose of the controller, which is designed as a neuro-fuzzy controller, is to generate the commands for the servo-systems of the robot so it may choose its way to its goal autonomously, while reacting in real-time to unexpected events. The proposed scheme has been successfully tested. The controller also demonstrates remarkable performance in adaptation to changes in manipulator dynamics. Sensor-based motion control is an essential feature for dealing with model uncertainties and unexpected obstacles in real-time world systems.
Non‐binary protograph low‐density parity‐check codes for space communications
SUMMARY#R##N##R##N#Protograph-based non-binary low-density parity-check (LDPC) codes with ultra-sparse parity-check matrices are compared with binary LDPC and turbo codes (TCs) from space communication standards. It is shown that larger coding gains are achieved, outperforming the binary competitors by more than 0.3 dB on the additive white Gaussian noise channel (AWGN). In the short block length regime, the designed codes gain more than 1 dB with respect to the binary protograph LDPC codes recently proposed for the next generation up-link standard of the Consultative Committee for Space Data Systems. Copyright © 2012 John Wiley & Sons, Ltd.
Coding theoretic approach to image segmentation
This paper introduces multi-scale tree-based approaches to image segmentation, using Rissanen's coding theoretic minimum description length (MDL) principle to penalize overly complex segmentations. Images are modelled as Gaussian random fields of independent pixels, with piecewise constant mean and variance. This model captures variations in both intensity (mean value) and texture (variance). Segmentation thus amounts to detecting changes in the mean and/or variance. One algorithm is based on an adaptive (greedy) rectangular recursive partitioning scheme. The second algorithm is an optimally pruned "wedgelet" decorated dyadic partitioning. We compare the two schemes with an alternative constant variance dyadic CART (classification and regression tree) scheme which accounts only for variations in mean, and demonstrate their performance on SAR images.
An empirical Bayes approach to inferring large-scale gene association networks
Motivation: Genetic networks are often described statistically using graphical models (e.g. Bayesian networks). However, inferring the network structure offers a serious challenge in microarray analysis where the sample size is small compared to the number of considered genes. This renders many standard algorithms for graphical models inapplicable, and inferring genetic networks an 'ill-posed' inverse problem.#R##N##R##N#Methods: We introduce a novel framework for small-sample inference of graphical models from gene expression data. Specifically, we focus on the so-called graphical Gaussian models (GGMs) that are now frequently used to describe gene association networks and to detect conditionally dependent genes. Our new approach is based on (1) improved (regularized) small-sample point estimates of partial correlation, (2) an exact test of edge inclusion with adaptive estimation of the degree of freedom and (3) a heuristic network search based on false discovery rate multiple testing. Steps (2) and (3) correspond to an empirical Bayes estimate of the network topology.#R##N##R##N#Results: Using computer simulations, we investigate the sensitivity (power) and specificity (true negative rate) of the proposed framework to estimate GGMs from microarray data. This shows that it is possible to recover the true network topology with high accuracy even for small-sample datasets. Subsequently, we analyze gene expression data from a breast cancer tumor study and illustrate our approach by inferring a corresponding large-scale gene association network for 3883 genes.#R##N##R##N#Availability: The authors have implemented the approach in the R package 'GeneTS' that is freely available from http://www.stat.uni-muenchen.de/~strimmer/genets/, from the R archive (CRAN) and from the Bioconductor website.#R##N##R##N#Contact: korbinian.strimmer@lmu.de
Analytical models for crosstalk excitation and propagation in VLSI circuits
The authors develop a general methodology to analyze crosstalk effects that are likely to cause errors in deep submicron high-speed circuits. They focus on crosstalk due to capacitive coupling between a pair of lines. Closed form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit parameters, the rise/fall times of the input transitions, and the skew between the transitions. For noise propagation, they present a new way for predicting the output waveform produced by an inverter due to a nonsquare wave pulse at its input. To expedite the computation of the response of a logic gate to an input pulse, the authors have developed a novel way of modeling such gates by an equivalent inverter. The results of their analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk. They present data to demonstrate accuracy of their results, including example runs of a test generator that uses these results.
Land use Dynamic Monitoring using Multi-Temporal SPOT Data in Beijing City from 1986 to 2004
Remote sensing dynamic monitoring of land use can detect the change information of land use and update the current land use map, which is important for rational utilization and scientific management to land resources. This paper discussed the technological procedure of land use dynamic monitoring, including the process of remote sensed images, the information classification and extraction of remote sensed imagery, and analysis of land use changes. Based on SPOT imagery data in three periods, the paper took Beijing city as an example, extracted the land use information during 1986-2004, and the land use changes were required in the period. The object-oriented method was used to extract information, and contrastive method after classification was used to confirm change zones.
VoiceXML and the W3C speech interface framework
VoiceXML is a markup language for creating voice-user interfaces. It uses speech and telephone touchtone recognition for input and prerecorded audio and text-to-speech synthesis (TTS) for output. It's based on the World Wide Web Consortium's (W3C's) Extensible Markup Language (XML) and leverages the Web paradigm for application development and deployment. By having a common language, application developers, platform vendors, and tool providers all can benefit from code portability and reuse. The paper discusses VoiceXML and the W3C speech interface framework.
Flexible ASIC: shared masking for multiple media processors
ASIC provides more than an order of magnitude advantage in terms of density, speed, and power requirement per gate. However, economic (cost of masks) and technological (deep micron manufacturability) trends favor FPGA as an implementation platform. In order to combine the advantages of both platforms and alleviate their disadvantages, recently a number of approaches, such as structured ASIC/regular fabrics, have been proposed. Our goal is to introduce an approach that has the same objective, but is orthogonal to those already proposed. The idea is to implement several ASIC designs in such a way that they share the datapath, memory structure, and several bottom layers of interconnect, while each design has only a few unique metal layers. We identified and addressed two main problems in our quest to develop a CAD flow for realization of such designs. They are: (i) the creation of the datapath, and (ii) the identification of common and unique interconnects for each design. Both problems are solved optimally using ILP formulations. We assembled a design flow platform using two new programs and the Trimaran and Shade tools. We quantitatively analyzed the advantages and disadvantages of the approach using the Mediabench benchmark suite.
Buy-at-Bulk Network Design with Protection
We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against edge or node failures in the network. In practice, the most popular model used in high speed telecommunication networks for protection against failures, is the so-called 1+1 model. In this model, two edge or node-disjoint paths are provisioned for each demand pair. We obtain the first non-trivial approximation algorithms for buy-at-bulk network design in the 1+1 model for both edge and node-disjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More specifically, we present a constant-factor approximation for the single-sink case, and an O(log 3  n) approximation for the multi-commodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems.
Software-defined infrastructure and the Future Central Office
This paper discusses the role of virtualization and software-defined infrastructure (SDI) in the design of future application platforms, and in particular the Future Central Office (CO). A multi-tier computing cloud is presented in which resources in the Smart Edge of the network play a crucial role in the delivery of low-latency and data-intensive applications. Resources in the Smart Edge are virtualized and managed using cloud computing principles, but these resources are more diverse than in conventional data centers, including programmable hardware, GPUs, etc. We propose an architecture for future application platforms, and we describe the SAVI Testbed (TB) design for the Smart Edge. The design features a novel Software-Defined Infrastructure manager that operates on top of OpenStack and OpenFlow. We conclude with a discussion of the implications of the Smart Edge design on the Future CO.
Fisher Kernels for Handwritten Word-spotting
The Fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches to pattern classification. In this contribution, we propose to apply this framework to handwritten word-spotting. Given a word image and a keyword generative model, the idea is to generate a vector which describes how the parameters of the keyword model should be modified to best fit the word image.This vector can then be used as the input of a discriminative classifier. We compare the performance of the proposed approach with that of a generative baseline on a challenging real-world dataset of customer letters. When the kernel used by the classifier is linear, the performance improvement is marginal but the proposed system is approximately 15 times faster than the baseline. If we use a non-linear kernel devised for this task, we obtain a 15\% relative reduction of the error but the detector is approximately 15 times slower.
Nonintrusive Load Monitoring and Diagnostics in Power Systems
This paper describes a transient event classification scheme, system identification techniques, and implementation for use in nonintrusive load monitoring. Together, these techniques form a system that can determine the operating schedule and find parameters of physical models of loads that are connected to an AC or DC power distribution system. The monitoring system requires only off-the-shelf hardware and recognizes individual transients by disaggregating the signal from a minimal number of sensors that are installed at a central location in the distribution system. Implementation details and field tests for AC and DC systems are presented.
False alarm reduction by improved filler model and post-processing in speech keyword spotting
This paper proposes four methods for improving the performance of keyword spotting (KWS) systems. Keyword models are usually created by concatenating the phoneme HMMs and garbage models consist of all phonemes HMMs. We present the results of investigations involving the use of skips in states of keyword HMMs and we focus on improving the hit ratio; then for false alarm reduction in KWS we model the words that are similar to keywords and we create HMMs for highly frequent words. These models help to improve the performance of the filler model. Two post-processing steps based on phoneme and word probabilities are used on the results of KWS to reduce the false alarms. We evaluate the performance of the improved keyword spotting in FarsDat corpus and compare the approaches. The presented techniques depict better performances than the popular KWS systems.
Automated Pathfinding tool chain for 3D-stacked integrated circuits: Practical case study
New technologies for manufacturing 3D Stacked ICs offer numerous opportunities for the design of complex and effcient embedded systems. But these technologies also introduce many design options at system/chip design level, hard to grasp during the complete design cycle. Because of the sequential nature of current design practices, designers are often forced to introduce design margins to meet required specications, resulting in sub-optimal designs. In this paper we introduce new design methodology and practical tool chain, called PathFinding Flow, that can help designers to easily trade-off between different system level design choices, physical design and/or technology options and understand their impact on typical design parameters such as cost, performance and power. Proposed methodology and the tool chain will be demonstrated on a practical case study, involving fairly complex Multi-Processor System-on-Chip using Network-on-Chip for communication medium. With this example we will show how High-Level Synthesis can be used to quickly move from high-level to RTL models, necessary for accurate physical prototyping for both computation and communication. We will also show how the possibility of design iteration, through the mechanism of feedback based on physical information from physical prototyping, can improve design performance. Finally, we will show how we can move in no time from traditional 2D to 3D design and how we can measure benets of such design choice.
Managing traceability information in manufacture
In this paper, an approach to design information systems for traceability is proposed. The paper applies gozinto graph modelling for traceability of the goods flow. A gozinto graph represents a graphical listing of raw materials, parts, intermediates and subassemblies, which a process transforms into an end product, through a sequence of operations. Next, the graphical listing has been translated into a reference data model that is the basis for designing an information system for tracking and tracing. Materials that are modelled this way represent production and/or purchase lots or batches. The composition of a certain end product is then represented through modelling all its constituent materials along with their intermediate relations. By registering all relations between sub-ordinate and super-ordinate material lots, a method of tracking the composition of the end product is obtained. When the entire sequence of operations required for manufacturing an end product adheres to this registering of relations, a multilevel bill of lots can be compiled. That bill of lots then, provides the necessary information to determine the composition of a material item out of component items. These composition data can be used to recall any items having consumed a certain component of specific interest (e.g., deficient), but also to certify product quality or to pro-actively adjust production processes to optimise the product quality in relation to its production characteristics (e.g., scarcity, costs or time).
PhD forum: Calibrating and using the global network of outdoor webcams
The vast imaging resources available via the Internet are underutilized. We propose to lay the foundation for the use of cameras attached to the Internet, also known as webcams, as free and flexible sensors. Developing an understanding of the relationship between signals in the world and the image variations they cause is critical to this effort. We use this understanding to develop methods to calibrate webcams, to estimate scene properties, and to report the weather.
Acquiring a Holistic Picture: The 4Screens Web-Based Simulator Helping Students to Unify Behaviours of Electronic Systems
Engineering knowledge is complex. Therefore, discipline concepts cannot usually be introduced to students all at once. Normally, individual models and behaviours are taught in several courses, by different teaching staff and in separate years of study. This often results in inadequate comprehension of the disciplinepsilas ldquobig picturerdquo - that is, the interrelationship of various concepts, models and behaviours. Students studying electronic circuit design, for example, often overlook important aspects of linear circuit performance. Their knowledge of the effects of steady-state and transient responses requires improved unification. Simple computer-based simulators can significantly help students to establish well-developed holistic views of systems in many engineering disciplines. The 4Screens Web-based simulator has helped hundreds of students in becoming more proficient with electronics systems. It has been developed by a group of undergraduates to make the Four screens model easy to utilise. A student using the simulator can simultaneously display four important characteristics of an electronic system on a computer screen: its algebraic transfer function H(s), systempsilas pole-zero plot, its Bode plots of magnitude and phase, as well as a graph of its time response to a unit step. Questionnaire responses and test performances over the last seven years confirm the effectiveness of the Four screens and the 4Screens Web-based simulator in improving student learning.
Benefits derived from restructuring the practical component of an introductory course in electronic communication systems
In an effort to address students' complaints regarding the tedious and frustrating nature of the practical component of the department's introductory communication course we embarked on a restructuring exercise. The restructuring of this component of the course shows promise based on preliminary results. The students are obtaining higher grades and gave the practical component a more favorable rating. Additionally the restructuring exercise allows for the addition of several more experiments enabling us to cover a broader portion of the course in the practical component. This paper looks at the changes that were made in the restructuring exercise and present preliminary results on the improvement in student's grade which can be attributed to this exercise. Over 16% of students are now obtaining grades over 80% compare to 0% before the restructuring exercise. Results from students' evaluations of the practical component of the course before and after the restructuring exercise is also presented, which shows an effective 24% improvement in the student rating.
Minimizing data and synchronization costs in one-way communication
Minimizing communication and synchronization costs is crucial to the realization of the performance potential of parallel computers. This paper presents a general technique which uses a global data-flow framework to optimize communication and synchronization in the context of the one-way communication model. In contrast to the conventional send/receive message-passing communication model, one-way communication is a new paradigm that decouples message transmission and synchronization. In parallel machines with appropriate low-level support, this may open up new opportunities not only to further optimize communication, but also to reduce the synchronization overhead. We present optimization techniques using our framework for eliminating redundant data communication and synchronization operations. Our approach works with the most general data alignments and distributions in languages like High Performance Fortran (HPF) and uses a combination of the traditional data-flow analysis and polyhedral algebra. Empirical results for several scientific benchmarks on a Cray T3E multiprocessor machine demonstrate that our approach is successful in reducing the number of data (communication) and synchronization messages, thereby reducing the overall execution times.
Pattern Mining in Visual Concept Streams
Pattern mining algorithms are often much easier applied than quantitatively assessed. In this paper we address the pattern evaluation problem by looking at both the capability of models and the difficulty of target concepts. We use four different data mining models: frequent itemset mining, k-means clustering, hidden Markov model, and hierarchical hidden Markov model to mine 39 concept streams from the a 137-video broadcast news collection from TRECVID-2005. We hypothesize that the discovered patterns can reveal semantics beyond the input space, and thus evaluate the patterns against a much larger concept space containing 192 concepts defined by LSCOM. Results show that HHMM has the best average prediction among all models, however different models seem to excel in different concepts depending on the concept prior and the ontological relationship. Results also show that the majority of the target concepts are better predicted with temporal or combination hypotheses, and there are novel concepts found that are not part of the original lexicon. This paper presents the first effort on temporal pattern mining in the large concept space. There are many promising directions to use concept mining to help construct better concept detectors or to guide the design of multimedia ontology.
Adaptive Goals for Self-Adaptive Service Compositions
Service compositions need to continuously self- adapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine.
Integrating Replenishment Decisions with Advance Demand Information
There is a growing consensus that a portfolio of customers with different demand lead times can lead to higher, more regular revenues and better capacity utilization. Customers with positive demand lead times place orders in advance of their needs, resulting inadvance demand information. This gives rise to the problem of finding effective inventory control policies under advance demand information. We show that state-dependent ( s, S) and base-stock policies are optimal for stochastic inventory systems with and without fixed costs. The state of the system reflects our knowledge of advance demand information. We also determine conditions under which advance demand information has no operational value. A numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information.
A new construction for n-track (d, k) codes with redundancy
Digital magnetic and optical storage systems employing NRZI recording use (d, k) codes. The d-parameter specifies the minimum number of 0's occurring between 1's while the k-parameter specifies the maximum number of 0's between l's. The n-track (d,k) codes (denoted as (d,k;n) codes) are extensions of (d, k) codes for use in multiple-track systems. Instead of imposing each track to individually satisfy both constraints, (d,k;n) codes satisfy the d-constraint in each track individually while relaxing the k-constraint by allowing it to be satisfied jointly by the multiple tracks. Although (d,k;n) codes can provide significant capacity increases over (d, k) codes, they suffer from the fact that a single faulty track can cause loss of synchronization and hence, loss of the data on all tracks. Orcutt and Marcellin (see IEEE Trans. on Inform. Theory, Sept., 1993) introduced n-track (d,k) codes with a redundancy of r (denoted as (d,k;n,r) codes) which allow for r faulty tracks by mandating that all subsets of n-r tracks satisfy the joint k-constraint. We propose a new method to construct (d,k; n, r) codes. These codes have simple encoding and decoding schemes, gain a large part of the capacity increase possible when using (d,k; n,r) codes, and are considerably more robust to faulty tracks. >
Structure and reaction based evaluation of synthetic accessibility
De novo design systems provide powerful methods to suggest a set of novel structures with high estimated binding affinity. One deficiency of these methods is that some of the suggested structures could be synthesized only with great difficulty. We devised a scoring method that rapidly evaluates synthetic accessibility of structures based on structural complexity, similarity to available starting materials and assessment of strategic bonds where a structure can be decomposed to obtain simpler fragments. These individual components were combined to an overall score of synthetic accessibility by an additive scheme. The weights of the scoring function components were calculated by linear regression analysis based on accessibility scores derived from medicinal chemists. The calculated values for synthetic accessibility agree with the values proposed by chemists to an extent that compares well with how chemists agree with each other.
Evolution of Protection Technologies in Metro Core Optical Networks
The market of metro optical networking has increased rapidly over the last few years. Traditional telecommunication infrastructure has an emphasis on long-haul optical transmission with ultra broadband capacity, relying mostly on large pure Dense Wavelength Division Multiplexing (DWDM) systems. Today, however, metro core optical networks take the major role in provisioning local access services and interconnecting service points of presences (POPs) with long-haul transmission. This represents a pivotal point in business operations of data communication services for service providers and large enterprises. In addition, the upper layer data services completely leans upon the substrate wavelength communication, and hence the survivability and reliability issues in the optical domain are now becoming crucial topics. This paper provides a detailed discussion around the development process of protection technologies in metro core optical transport infrastructure.
Packet radio and the factory of the future
