{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset (title + abstract)\n",
    "\n",
    "The titles are in the paper folder while the abstracts are in the abstract folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Set your S2 API key\n",
    "api_key = \"\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"x-api-key\": api_key\n",
    "}\n",
    "\n",
    "# Base output directory locally\n",
    "base_output_dir = \"./s2ag_dataset_title_abstracts\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 1: Get the latest release ID\n",
    "latest_release_url = \"https://api.semanticscholar.org/datasets/v1/release/latest\"\n",
    "response = requests.get(latest_release_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to fetch latest release: {response.status_code} - {response.text}\")\n",
    "    exit()\n",
    "\n",
    "latest_release = response.json()\n",
    "release_id = latest_release[\"release_id\"]\n",
    "print(\"Latest release ID:\", release_id)\n",
    "\n",
    "# Step 2: Datasets to download\n",
    "datasets = [\"papers\", \"abstracts\"]\n",
    "\n",
    "# Dictionary to store data\n",
    "data = {\"title\": [], \"abstract\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    # Create dataset-specific directory\n",
    "    output_dir = os.path.join(base_output_dir, dataset_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 3: Get metadata for the dataset\n",
    "    dataset_url = f\"https://api.semanticscholar.org/datasets/v1/release/{release_id}/dataset/{dataset_name}\"\n",
    "    response = requests.get(dataset_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch metadata for {dataset_name}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    dataset_info = response.json()\n",
    "    print(f\"\\n{dataset_name.capitalize()} dataset metadata:\")\n",
    "    print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "    # Step 4: Download all dataset files with simplified file names\n",
    "    for index, file_url in enumerate(dataset_info[\"files\"]):  # Process all files\n",
    "        # Generate a shorter file name (e.g., papers_0.gz, abstracts_1.gz)\n",
    "        file_name = f\"{dataset_name}_{index}.gz\"\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        print(f\"Downloading {dataset_name}/{file_name}...\")\n",
    "        file_response = requests.get(file_url, headers=headers, stream=True)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                for chunk in file_response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            print(f\"Saved {file_name} to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: {file_response.status_code} - {file_response.text}\")\n",
    "\n",
    "# Step 5: Process the datasets to extract titles and abstracts for Computer Science papers\n",
    "print(\"\\nProcessing datasets to extract titles and abstracts for Computer Science papers...\")\n",
    "\n",
    "# Dictionary to store titles and abstracts by corpusid\n",
    "title_dict = {}\n",
    "abstract_dict = {}\n",
    "\n",
    "# Process papers dataset (for titles and field of study)\n",
    "papers_dir = os.path.join(base_output_dir, \"papers\")\n",
    "if os.path.exists(papers_dir):\n",
    "    for file_name in os.listdir(papers_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(papers_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            title = record.get(\"title\")\n",
    "                            fields_of_study = record.get(\"fieldsOfStudy\", [])\n",
    "                            # Filter for Computer Science\n",
    "                            if corpusid and title and \"Computer Science\" in fields_of_study:\n",
    "                                title_dict[corpusid] = title\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Papers directory not found. Skipping paper processing.\")\n",
    "\n",
    "# Process abstracts dataset\n",
    "abstracts_dir = os.path.join(base_output_dir, \"abstracts\")\n",
    "if os.path.exists(abstracts_dir):\n",
    "    for file_name in os.listdir(abstracts_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(abstracts_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            abstract = record.get(\"abstract\")\n",
    "                            if corpusid and abstract:\n",
    "                                abstract_dict[corpusid] = abstract\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Abstracts directory not found. Skipping abstract processing.\")\n",
    "\n",
    "# Step 6: Combine titles and abstracts by corpusid for Computer Science papers\n",
    "for corpusid in title_dict:\n",
    "    if corpusid in abstract_dict:\n",
    "        data[\"title\"].append(title_dict[corpusid])\n",
    "        data[\"abstract\"].append(abstract_dict[corpusid])\n",
    "\n",
    "# Step 7: Create DataFrame and save to CSV\n",
    "if data[\"title\"]:\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nSample of DataFrame (Computer Science papers):\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved DataFrame to {csv_output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo matching title and abstract pairs found for Computer Science papers. CSV not created.\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the datasets and filter English Only papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Detect if the text is in English using langdetect.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text.strip()) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def combine_english_title_abstracts(titles_csv, abstracts_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Combine English title-abstract pairs from CSV files into a single output CSV,\n",
    "    processing data in chunks to handle large datasets.\n",
    "    \"\"\"\n",
    "    english_pairs = []\n",
    "    chunk_size = 50000  # Adjust based on memory; process 10,000 rows at a time\n",
    "\n",
    "    logger.info(f\"Starting processing of {titles_csv} and {abstracts_csv}\")\n",
    "    for chunk_titles in pd.read_csv(titles_csv, usecols=[1], chunksize=chunk_size):\n",
    "        # Align abstracts chunk by skipping rows and matching length\n",
    "        start_row = chunk_titles.index[0]\n",
    "        chunk_abstracts = pd.read_csv(abstracts_csv, usecols=[1], skiprows=range(1, start_row + 1), nrows=chunk_size).squeeze()\n",
    "        min_len = min(len(chunk_titles), len(chunk_abstracts))\n",
    "        titles = chunk_titles.iloc[:min_len].squeeze()\n",
    "        abstracts = chunk_abstracts.iloc[:min_len]\n",
    "\n",
    "        # Process each pair in the chunk\n",
    "        for title, abstract in zip(titles, abstracts):\n",
    "            if pd.notna(title) and pd.notna(abstract):\n",
    "                if is_english(title) and is_english(abstract):\n",
    "                    english_pairs.append({'title': title.strip(), 'abstract': abstract.strip()})\n",
    "\n",
    "        logger.info(f\"Processed chunk starting at row {start_row} with {min_len} pairs\")\n",
    "\n",
    "    # Write to CSV file\n",
    "    if english_pairs:\n",
    "        df_output = pd.DataFrame(english_pairs)\n",
    "        df_output.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"English-only combined file written to: {output_csv}\")\n",
    "        print(f\"English-only combined file written to: {output_csv}\")\n",
    "    else:\n",
    "        logger.warning(f\"No English entries found. No file written to: {output_csv}\")\n",
    "        print(f\"No English entries found. No file written to: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with file paths\n",
    "    titles_csv = 's2ag_dataset_title_abstracts/output/papers_csv/papers_0.csv'\n",
    "    abstracts_csv = 's2ag_dataset_title_abstracts/output/abstracts_csv/abstracts_0.csv'\n",
    "    output_csv = 'paper_dataset/paper_dataset_v0.csv'\n",
    "\n",
    "    # Validate input files exist\n",
    "    if not (os.path.exists(titles_csv) and os.path.exists(abstracts_csv)):\n",
    "        logger.error(\"One or both input CSV files not found\")\n",
    "        print(\"Error: One or both input CSV files not found\")\n",
    "    else:\n",
    "        combine_english_title_abstracts(titles_csv, abstracts_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for to complete whole process, 10000 rows took 185,3 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypermedia: 0.9265\n",
      "web_2_0: 0.9016\n",
      "centric: 0.9002\n",
      "digital_library: 0.8961\n",
      "envisioned: 0.8956\n",
      "end-user: 0.8942\n",
      "software_agents: 0.8903\n",
      "smart_city: 0.8900\n",
      "database_systems: 0.8857\n",
      "leveraged: 0.8836\n"
     ]
    }
   ],
   "source": [
    "test_word = 'semantic_web'# web, learning, science\n",
    "if test_word in model:\n",
    "    similar_words = model.most_similar(test_word, topn=10)\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
