{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset (title + abstract)\n",
    "\n",
    "The titles are in the paper folder while the abstracts are in the abstract folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Set your S2 API key\n",
    "api_key = \"\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"x-api-key\": api_key\n",
    "}\n",
    "\n",
    "# Base output directory locally\n",
    "base_output_dir = \"./s2ag_dataset_title_abstracts\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 1: Get the latest release ID\n",
    "latest_release_url = \"https://api.semanticscholar.org/datasets/v1/release/latest\"\n",
    "response = requests.get(latest_release_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to fetch latest release: {response.status_code} - {response.text}\")\n",
    "    exit()\n",
    "\n",
    "latest_release = response.json()\n",
    "release_id = latest_release[\"release_id\"]\n",
    "print(\"Latest release ID:\", release_id)\n",
    "\n",
    "# Step 2: Datasets to download\n",
    "datasets = [\"papers\", \"abstracts\"]\n",
    "\n",
    "# Dictionary to store data\n",
    "data = {\"title\": [], \"abstract\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    # Create dataset-specific directory\n",
    "    output_dir = os.path.join(base_output_dir, dataset_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 3: Get metadata for the dataset\n",
    "    dataset_url = f\"https://api.semanticscholar.org/datasets/v1/release/{release_id}/dataset/{dataset_name}\"\n",
    "    response = requests.get(dataset_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch metadata for {dataset_name}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    dataset_info = response.json()\n",
    "    print(f\"\\n{dataset_name.capitalize()} dataset metadata:\")\n",
    "    print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "    # Step 4: Download all dataset files with simplified file names\n",
    "    for index, file_url in enumerate(dataset_info[\"files\"]):  # Process all files\n",
    "        # Generate a shorter file name (e.g., papers_0.gz, abstracts_1.gz)\n",
    "        file_name = f\"{dataset_name}_{index}.gz\"\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        print(f\"Downloading {dataset_name}/{file_name}...\")\n",
    "        file_response = requests.get(file_url, headers=headers, stream=True)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                for chunk in file_response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            print(f\"Saved {file_name} to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: {file_response.status_code} - {file_response.text}\")\n",
    "\n",
    "# Step 5: Process the datasets to extract titles and abstracts for Computer Science papers\n",
    "print(\"\\nProcessing datasets to extract titles and abstracts for Computer Science papers...\")\n",
    "\n",
    "# Dictionary to store titles and abstracts by corpusid\n",
    "title_dict = {}\n",
    "abstract_dict = {}\n",
    "\n",
    "# Process papers dataset (for titles and field of study)\n",
    "papers_dir = os.path.join(base_output_dir, \"papers\")\n",
    "if os.path.exists(papers_dir):\n",
    "    for file_name in os.listdir(papers_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(papers_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            title = record.get(\"title\")\n",
    "                            fields_of_study = record.get(\"fieldsOfStudy\", [])\n",
    "                            # Filter for Computer Science\n",
    "                            if corpusid and title and \"Computer Science\" in fields_of_study:\n",
    "                                title_dict[corpusid] = title\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Papers directory not found. Skipping paper processing.\")\n",
    "\n",
    "# Process abstracts dataset\n",
    "abstracts_dir = os.path.join(base_output_dir, \"abstracts\")\n",
    "if os.path.exists(abstracts_dir):\n",
    "    for file_name in os.listdir(abstracts_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(abstracts_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            abstract = record.get(\"abstract\")\n",
    "                            if corpusid and abstract:\n",
    "                                abstract_dict[corpusid] = abstract\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Abstracts directory not found. Skipping abstract processing.\")\n",
    "\n",
    "# Step 6: Combine titles and abstracts by corpusid for Computer Science papers\n",
    "for corpusid in title_dict:\n",
    "    if corpusid in abstract_dict:\n",
    "        data[\"title\"].append(title_dict[corpusid])\n",
    "        data[\"abstract\"].append(abstract_dict[corpusid])\n",
    "\n",
    "# Step 7: Create DataFrame and save to CSV\n",
    "if data[\"title\"]:\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nSample of DataFrame (Computer Science papers):\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved DataFrame to {csv_output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo matching title and abstract pairs found for Computer Science papers. CSV not created.\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the datasets and filter English Only papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Detect if the text is in English using langdetect.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text.strip()) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def combine_english_title_abstracts(titles_csv, abstracts_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Combine English title-abstract pairs from CSV files into a single output CSV,\n",
    "    processing data in chunks to handle large datasets.\n",
    "    \"\"\"\n",
    "    english_pairs = []\n",
    "    chunk_size = 50000  # Adjust based on memory; process 10,000 rows at a time\n",
    "\n",
    "    logger.info(f\"Starting processing of {titles_csv} and {abstracts_csv}\")\n",
    "    for chunk_titles in pd.read_csv(titles_csv, usecols=[1], chunksize=chunk_size):\n",
    "        # Align abstracts chunk by skipping rows and matching length\n",
    "        start_row = chunk_titles.index[0]\n",
    "        chunk_abstracts = pd.read_csv(abstracts_csv, usecols=[1], skiprows=range(1, start_row + 1), nrows=chunk_size).squeeze()\n",
    "        min_len = min(len(chunk_titles), len(chunk_abstracts))\n",
    "        titles = chunk_titles.iloc[:min_len].squeeze()\n",
    "        abstracts = chunk_abstracts.iloc[:min_len]\n",
    "\n",
    "        # Process each pair in the chunk\n",
    "        for title, abstract in zip(titles, abstracts):\n",
    "            if pd.notna(title) and pd.notna(abstract):\n",
    "                if is_english(title) and is_english(abstract):\n",
    "                    english_pairs.append({'title': title.strip(), 'abstract': abstract.strip()})\n",
    "\n",
    "        logger.info(f\"Processed chunk starting at row {start_row} with {min_len} pairs\")\n",
    "\n",
    "    # Write to CSV file\n",
    "    if english_pairs:\n",
    "        df_output = pd.DataFrame(english_pairs)\n",
    "        df_output.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"English-only combined file written to: {output_csv}\")\n",
    "        print(f\"English-only combined file written to: {output_csv}\")\n",
    "    else:\n",
    "        logger.warning(f\"No English entries found. No file written to: {output_csv}\")\n",
    "        print(f\"No English entries found. No file written to: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with file paths\n",
    "    titles_csv = 's2ag_dataset_title_abstracts/output/papers_csv/papers_0.csv'\n",
    "    abstracts_csv = 's2ag_dataset_title_abstracts/output/abstracts_csv/abstracts_0.csv'\n",
    "    output_csv = 'paper_dataset/paper_dataset_v0.csv'\n",
    "\n",
    "    # Validate input files exist\n",
    "    if not (os.path.exists(titles_csv) and os.path.exists(abstracts_csv)):\n",
    "        logger.error(\"One or both input CSV files not found\")\n",
    "        print(\"Error: One or both input CSV files not found\")\n",
    "    else:\n",
    "        combine_english_title_abstracts(titles_csv, abstracts_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for to complete whole process, 10000 rows took 185,3 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test word: semantic_web\n",
      "semantic_web_technologies: 0.8505\n",
      "web_ontology_language: 0.8075\n",
      "mashups: 0.8045\n",
      "semantic_web_services: 0.8000\n",
      "query_languages: 0.7976\n",
      "world_wide_web: 0.7876\n",
      "owl: 0.7862\n",
      "service-based: 0.7839\n",
      "domain_ontologies: 0.7814\n",
      "knowledge_representation: 0.7806\n"
     ]
    }
   ],
   "source": [
    "test_word = 'semantic_web'# image_segmentation, web, learning, science, semantic_web\n",
    "if test_word in model:\n",
    "    similar_words = model.most_similar(test_word, topn=10)\n",
    "    print(f\"Test word: {test_word}\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concepts from CSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Top similar terms to 'semantic web':\n",
      "1. rdf graph\n",
      "2. ontology alignment\n",
      "3. rdf data\n",
      "4. ontology pattern\n",
      "5. semantic web rule languages\n",
      "6. graphrag\n",
      "7. semantic web applications\n",
      "8. semantic search engine\n",
      "9. sparql queries\n",
      "10. ontology creation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Initialize\n",
    "ontology = Ontology()\n",
    "\n",
    "# Target word to find related topics\n",
    "target = \"semantic web\"\n",
    "\n",
    "# Find related topics by keyword match\n",
    "related_by_name = [topic for topic in ontology.topics if target.lower() in topic.lower()]\n",
    "\n",
    "# Get descendants (children, subtopics)\n",
    "descendants = ontology.get_all_descendants_of_topics([target]) if target in ontology.topics else []\n",
    "\n",
    "# Combine and deduplicate\n",
    "similar_terms = list(set(related_by_name + descendants))\n",
    "\n",
    "# Show top N\n",
    "top_n = 10\n",
    "print(f\"Top similar terms to '{target}':\")\n",
    "for i, term in enumerate(similar_terms[:top_n]):\n",
    "    print(f\"{i+1}. {term}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add ontology path\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "from ontology import Ontology\n",
    "\n",
    "\n",
    "def load_word2vec_model(path: str):\n",
    "    try:\n",
    "        model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "        print(f\"‚úÖ Loaded Word2Vec model from: {path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"‚ùå Model file not found: {path}\")\n",
    "\n",
    "\n",
    "def load_ontology():\n",
    "    try:\n",
    "        ontology = Ontology()\n",
    "        print(\"‚úÖ CSO Ontology loaded.\")\n",
    "        return ontology\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Failed to load ontology: {e}\")\n",
    "\n",
    "\n",
    "def get_related_topics(wet2: str, ontology: Ontology):\n",
    "    related_by_name = [topic for topic in ontology.topics if wet2.lower() in topic.lower()]\n",
    "    descendants = ontology.get_all_descendants_of_topics([wet2]) if wet2 in ontology.topics else []\n",
    "    return list(set(related_by_name + descendants))\n",
    "\n",
    "\n",
    "def match_terms(model, ontology, top_n=10, word_similarity=0.7, min_similarity=0.90):\n",
    "    output = {}\n",
    "    total_vocab = len(model.key_to_index)\n",
    "\n",
    "    for i, wet in enumerate(model.key_to_index):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"üîÅ Processing {i}/{total_vocab} | Matches so far: {len(output)}\")\n",
    "\n",
    "        output[wet] = []\n",
    "        similar_words = [(wet, 1.0)]\n",
    "\n",
    "        try:\n",
    "            similar_words.extend(model.most_similar(wet, topn=top_n))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        for wet2, sim_w in similar_words:\n",
    "            if sim_w < word_similarity:\n",
    "                continue\n",
    "\n",
    "            related_topics = get_related_topics(wet2, ontology)\n",
    "\n",
    "            for topic in related_topics:\n",
    "                sim_t = Levenshtein.normalized_similarity(topic, wet2)\n",
    "                if sim_t >= min_similarity:\n",
    "                    output[wet].append({\n",
    "                        \"topic\": topic,\n",
    "                        \"sim_t\": round(sim_t, 4),\n",
    "                        \"wet\": wet2,\n",
    "                        \"sim_w\": round(sim_w, 4)\n",
    "                    })\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_output(output: dict, filename: str):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, indent=4)\n",
    "        print(f\"‚úÖ Results saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save results: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === CONFIGURATION ===\n",
    "    model_path = '9M[256-10]_sg.bin'\n",
    "    top_n_similar_words = 10\n",
    "    word2vec_threshold = 0.7\n",
    "    levenshtein_threshold = 0.90\n",
    "    output_file = 'token-to-cso-combined_production.json'\n",
    "\n",
    "    # === EXECUTION ===\n",
    "    model = load_word2vec_model(model_path)\n",
    "    ontology = load_ontology()\n",
    "    results = match_terms(\n",
    "        model,\n",
    "        ontology,\n",
    "        top_n=top_n_similar_words,\n",
    "        word_similarity=word2vec_threshold,\n",
    "        min_similarity=levenshtein_threshold\n",
    "    )\n",
    "    save_output(results, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Computer Science Ontology loaded.\n",
      "Processed 1000 words, output size: 999\n",
      "Processed 2000 words, output size: 1999\n",
      "Processed 3000 words, output size: 2999\n",
      "Processed 4000 words, output size: 3999\n",
      "Processed 5000 words, output size: 4999\n",
      "Processed 6000 words, output size: 5999\n",
      "Processed 7000 words, output size: 6999\n",
      "Processed 8000 words, output size: 7999\n",
      "Processed 9000 words, output size: 8999\n",
      "Processed 10000 words, output size: 9999\n",
      "Processed 11000 words, output size: 10999\n",
      "Processed 12000 words, output size: 11999\n",
      "Processed 13000 words, output size: 12999\n",
      "Processed 14000 words, output size: 13999\n",
      "Processed 15000 words, output size: 14999\n",
      "Processed 16000 words, output size: 15999\n",
      "Processed 17000 words, output size: 16999\n",
      "Saved mappings to 'token-to-cso-combined_v1.json'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Updated script to combine Word2Vec and CSO ontology\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from rapidfuzz.distance import Levenshtein  # For Levenshtein similarity\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "try:\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Load ontology\n",
    "try:\n",
    "    ontology = Ontology()\n",
    "    print(\"Computer Science Ontology loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ontology: {e}\")\n",
    "    raise\n",
    "\n",
    "# Parameters\n",
    "min_similarity = 0.90  # Levenshtein similarity threshold (0 to 1)\n",
    "word_similarity = 0.7  # Word2Vec similarity threshold (0 to 1)\n",
    "top_amount_of_words = 10  # Number of similar words to retrieve\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "\n",
    "for wet in model.key_to_index:  # Iterate over Word2Vec vocabulary\n",
    "    i += 1\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"Processed {i} words, output size: {len(output)}\")\n",
    "    \n",
    "    output[wet] = []\n",
    "    \n",
    "    # Get similar words from Word2Vec\n",
    "    similar_words = []\n",
    "    similar_words.append((wet, 1.0))  # Include the word itself\n",
    "    try:\n",
    "        similarities = model.most_similar(wet, topn=top_amount_of_words)\n",
    "        similar_words.extend(similarities)\n",
    "    except KeyError:\n",
    "        print(f\"Word '{wet}' not in model vocabulary, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    for wet2, sim in similar_words:\n",
    "        if sim >= word_similarity:\n",
    "            # Find ontology topics related to wet2 (substring matching and descendants)\n",
    "            related_by_name = [topic for topic in ontology.topics if wet2.lower() in topic.lower()]\n",
    "            descendants = ontology.get_all_descendants_of_topics([wet2]) if wet2 in ontology.topics else []\n",
    "            topics = list(set(related_by_name + descendants))\n",
    "            \n",
    "            for topic in topics:\n",
    "                # Compute Levenshtein similarity using rapidfuzz (normalized to 0-1)\n",
    "                m = Levenshtein.normalized_similarity(topic, wet2)  # Using Levenshtein normalized similarity\n",
    "                if m >= min_similarity:\n",
    "                    output[wet].append({\n",
    "                        \"topic\": topic,\n",
    "                        \"sim_t\": m,\n",
    "                        \"wet\": wet2,\n",
    "                        \"sim_w\": sim\n",
    "                    })\n",
    "\n",
    "# Save the cached model\n",
    "try:\n",
    "    with open('token-to-cso-combined_v1.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)\n",
    "    print(\"Saved mappings to 'token-to-cso-combined_v1.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Computer Science Ontology loaded.\n",
      "Processed 1000 words, output size: 999\n",
      "Processed 2000 words, output size: 1999\n",
      "Processed 3000 words, output size: 2999\n",
      "Processed 4000 words, output size: 3999\n",
      "Processed 5000 words, output size: 4999\n",
      "Processed 6000 words, output size: 5999\n",
      "Processed 7000 words, output size: 6999\n",
      "Processed 8000 words, output size: 7999\n",
      "Processed 9000 words, output size: 8999\n",
      "Processed 10000 words, output size: 9999\n",
      "Processed 11000 words, output size: 10999\n",
      "Processed 12000 words, output size: 11999\n",
      "Processed 13000 words, output size: 12999\n",
      "Processed 14000 words, output size: 13999\n",
      "Processed 15000 words, output size: 14999\n",
      "Processed 16000 words, output size: 15999\n",
      "Processed 17000 words, output size: 16999\n",
      "Saved mappings to 'token-to-cso-combined_v2.json'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Updated script to combine Word2Vec and CSO ontology\n",
    "Running in main.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from rapidfuzz import fuzz  # For Levenshtein similarity\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "try:\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Load ontology\n",
    "try:\n",
    "    ontology = Ontology()\n",
    "    print(\"Computer Science Ontology loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ontology: {e}\")\n",
    "    raise\n",
    "\n",
    "# Parameters\n",
    "min_similarity = 0.90  # Levenshtein similarity threshold (0 to 1)\n",
    "word_similarity = 0.7  # Word2Vec similarity threshold (0 to 1)\n",
    "top_amount_of_words = 10  # Number of similar words to retrieve\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "\n",
    "for wet in model.key_to_index:  # Iterate over Word2Vec vocabulary\n",
    "    i += 1\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"Processed {i} words, output size: {len(output)}\")\n",
    "    \n",
    "    output[wet] = []\n",
    "    \n",
    "    # Get similar words from Word2Vec\n",
    "    similar_words = []\n",
    "    similar_words.append((wet, 1.0))  # Include the word itself\n",
    "    try:\n",
    "        similarities = model.most_similar(wet, topn=top_amount_of_words)\n",
    "        similar_words.extend(similarities)\n",
    "    except KeyError:\n",
    "        print(f\"Word '{wet}' not in model vocabulary, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    for wet2, sim in similar_words:\n",
    "        if sim >= word_similarity:\n",
    "            # Find ontology topics related to wet2 (substring matching and descendants)\n",
    "            related_by_name = [topic for topic in ontology.topics if wet2.lower() in topic.lower()]\n",
    "            descendants = ontology.get_all_descendants_of_topics([wet2]) if wet2 in ontology.topics else []\n",
    "            topics = list(set(related_by_name + descendants))\n",
    "            \n",
    "            for topic in topics:\n",
    "                # Compute Levenshtein similarity using rapidfuzz (normalized to 0-1)\n",
    "                m = fuzz.ratio(topic, wet2) / 100.0  # rapidfuzz returns 0-100, normalize to 0-1\n",
    "                if m >= min_similarity:\n",
    "                    output[wet].append({\n",
    "                        \"topic\": topic,\n",
    "                        \"sim_t\": m,\n",
    "                        \"wet\": wet2,\n",
    "                        \"sim_w\": sim\n",
    "                    })\n",
    "\n",
    "# Save the cached model\n",
    "try:\n",
    "    with open('token-to-cso-combined_v2.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)\n",
    "    print(\"Saved mappings to 'token-to-cso-combined_v2.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Computer Science Ontology loaded.\n",
      "Processed 1000 words, output size: 999\n",
      "Processed 2000 words, output size: 1999\n",
      "Processed 3000 words, output size: 2999\n",
      "Processed 4000 words, output size: 3999\n",
      "Processed 5000 words, output size: 4999\n",
      "Processed 6000 words, output size: 5999\n",
      "Processed 7000 words, output size: 6999\n",
      "Processed 8000 words, output size: 7999\n",
      "Processed 9000 words, output size: 8999\n",
      "Processed 10000 words, output size: 9999\n",
      "Processed 11000 words, output size: 10999\n",
      "Processed 12000 words, output size: 11999\n",
      "Processed 13000 words, output size: 12999\n",
      "Processed 14000 words, output size: 13999\n",
      "Processed 15000 words, output size: 14999\n",
      "Processed 16000 words, output size: 15999\n",
      "Processed 17000 words, output size: 16999\n",
      "Saved mappings to 'token-to-cso-combined_v3.json'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Updated script to combine Word2Vec and CSO ontology with first 4-char matching\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from rapidfuzz.distance import Levenshtein  # For Levenshtein similarity\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "try:\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Load ontology\n",
    "try:\n",
    "    ontology = Ontology()\n",
    "    print(\"Computer Science Ontology loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ontology: {e}\")\n",
    "    raise\n",
    "\n",
    "# Parameters\n",
    "min_similarity = 0.9  # Levenshtein similarity threshold (0 to 1)\n",
    "word_similarity = 0.7  # Word2Vec similarity threshold (0 to 1)\n",
    "top_amount_of_words = 10  # Number of similar words to retrieve\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "\n",
    "for wet in model.key_to_index:  # Iterate over Word2Vec vocabulary\n",
    "    i += 1\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"Processed {i} words, output size: {len(output)}\")\n",
    "    \n",
    "    output[wet] = []\n",
    "    \n",
    "    # Get similar words from Word2Vec\n",
    "    similar_words = []\n",
    "    similar_words.append((wet, 1.0))  # Include the word itself\n",
    "    try:\n",
    "        similarities = model.most_similar(wet, topn=top_amount_of_words)\n",
    "        similar_words.extend(similarities)\n",
    "    except KeyError:\n",
    "        print(f\"Word '{wet}' not in model vocabulary, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    for wet2, sim in similar_words:\n",
    "        if sim >= word_similarity:\n",
    "            # Find ontology topics starting with first 4 characters of wet2\n",
    "            topics = [topic for topic in ontology.topics if topic.lower().startswith(wet2.lower()[:4])]\n",
    "            for topic in topics:\n",
    "                # Compute Levenshtein similarity using rapidfuzz (normalized to 0-1)\n",
    "                m = Levenshtein.normalized_similarity(topic, wet2)\n",
    "                if m >= min_similarity:\n",
    "                    try:\n",
    "                        output[wet].append({\n",
    "                            \"topic\": topic,\n",
    "                            \"sim_t\": m,\n",
    "                            \"wet\": wet2,\n",
    "                            \"sim_w\": sim\n",
    "                        })\n",
    "                    except KeyError as e:\n",
    "                        print(f\"KeyError for word: {wet}, error: {e}\")\n",
    "\n",
    "# Save the cached model\n",
    "try:\n",
    "    with open('token-to-cso-combined_v3.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)\n",
    "    print(\"Saved mappings to 'token-to-cso-combined_v3.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tomorrow, I have to merge this cashing model.py with pipline, and then it can start from step one till 8 and then pipeline can work accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Computer Science Ontology loaded.\n",
      "\n",
      "Processing word 1: 'web'\n",
      "No mappings found for 'web' with given thresholds.\n",
      "\n",
      "Processing word 2: 'semantic_web'\n",
      "Mappings for 'semantic_web':\n",
      "  Topic: owl, Levenshtein Sim: 1.0000, Word2Vec Word: owl, Word2Vec Sim: 0.7862\n",
      "\n",
      "Processing word 3: 'learning'\n",
      "Mappings for 'learning':\n",
      "  Topic: elearning, Levenshtein Sim: 0.9412, Word2Vec Word: learning, Word2Vec Sim: 1.0000\n",
      "\n",
      "Processing word 4: 'ontology'\n",
      "Mappings for 'ontology':\n",
      "  Topic: ontology, Levenshtein Sim: 1.0000, Word2Vec Word: ontology, Word2Vec Sim: 1.0000\n",
      "  Topic: ontologies, Levenshtein Sim: 1.0000, Word2Vec Word: ontologies, Word2Vec Sim: 0.8011\n",
      "  Topic: owl, Levenshtein Sim: 1.0000, Word2Vec Word: owl, Word2Vec Sim: 0.7391\n",
      "\n",
      "Saved mappings to 'token-to-cso-combined.json'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Updated script to combine Word2Vec and CSO ontology for specific test words\n",
    "Running in main.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from rapidfuzz import fuzz  # For Levenshtein similarity\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "try:\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Load ontology\n",
    "try:\n",
    "    ontology = Ontology()\n",
    "    print(\"Computer Science Ontology loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ontology: {e}\")\n",
    "    raise\n",
    "\n",
    "# Parameters\n",
    "min_similarity = 0.94  # Levenshtein similarity threshold (0 to 1)\n",
    "word_similarity = 0.7  # Word2Vec similarity threshold (0 to 1)\n",
    "top_amount_of_words = 10  # Number of similar words to retrieve\n",
    "\n",
    "# Test words\n",
    "test_words = ['web', 'semantic_web', 'learning', 'ontology']  # Add or modify words here\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "\n",
    "for wet in test_words:  # Iterate over test words only\n",
    "    i += 1\n",
    "    print(f\"\\nProcessing word {i}: '{wet}'\")\n",
    "    \n",
    "    if wet not in model.key_to_index:\n",
    "        print(f\"Word '{wet}' not in model vocabulary, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    output[wet] = []\n",
    "    \n",
    "    # Get similar words from Word2Vec\n",
    "    similar_words = []\n",
    "    similar_words.append((wet, 1.0))  # Include the word itself\n",
    "    try:\n",
    "        similarities = model.most_similar(wet, topn=top_amount_of_words)\n",
    "        similar_words.extend(similarities)\n",
    "    except KeyError:\n",
    "        print(f\"Error retrieving similar words for '{wet}', skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Process similar words\n",
    "    for wet2, sim in similar_words:\n",
    "        if sim >= word_similarity:\n",
    "            # Find ontology topics related to wet2 (substring matching and descendants)\n",
    "            related_by_name = [topic for topic in ontology.topics if wet2.lower() in topic.lower()]\n",
    "            descendants = ontology.get_all_descendants_of_topics([wet2]) if wet2 in ontology.topics else []\n",
    "            topics = list(set(related_by_name + descendants))\n",
    "            \n",
    "            for topic in topics:\n",
    "                # Compute Levenshtein similarity using rapidfuzz (normalized to 0-1)\n",
    "                m = fuzz.ratio(topic, wet2) / 100.0  # rapidfuzz returns 0-100, normalize to 0-1\n",
    "                if m >= min_similarity:\n",
    "                    output[wet].append({\n",
    "                        \"topic\": topic,\n",
    "                        \"sim_t\": m,\n",
    "                        \"wet\": wet2,\n",
    "                        \"sim_w\": sim\n",
    "                    })\n",
    "    \n",
    "    # Print mappings for the current word\n",
    "    if output[wet]:\n",
    "        print(f\"Mappings for '{wet}':\")\n",
    "        for mapping in output[wet]:\n",
    "            print(f\"  Topic: {mapping['topic']}, Levenshtein Sim: {mapping['sim_t']:.4f}, \"\n",
    "                  f\"Word2Vec Word: {mapping['wet']}, Word2Vec Sim: {mapping['sim_w']:.4f}\")\n",
    "    else:\n",
    "        print(f\"No mappings found for '{wet}' with given thresholds.\")\n",
    "\n",
    "# Save the cached model\n",
    "try:\n",
    "    with open('token-to-cso-combined.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)\n",
    "    print(\"\\nSaved mappings to 'token-to-cso-combined_v2.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science Ontology loaded.\n",
      "Computer Science Ontology loaded.\n",
      "\n",
      "Processing word 1: 'web'\n",
      "No mappings found for 'web' with given thresholds.\n",
      "\n",
      "Processing word 2: 'semantic_web'\n",
      "Mappings for 'semantic_web':\n",
      "  Topic: owl, Levenshtein Sim: 1.0000, Word2Vec Word: owl, Word2Vec Sim: 0.7862\n",
      "\n",
      "Processing word 3: 'learning'\n",
      "No mappings found for 'learning' with given thresholds.\n",
      "\n",
      "Processing word 4: 'ontology'\n",
      "Mappings for 'ontology':\n",
      "  Topic: ontology, Levenshtein Sim: 1.0000, Word2Vec Word: ontology, Word2Vec Sim: 1.0000\n",
      "  Topic: ontologies, Levenshtein Sim: 1.0000, Word2Vec Word: ontologies, Word2Vec Sim: 0.8011\n",
      "  Topic: owl, Levenshtein Sim: 1.0000, Word2Vec Word: owl, Word2Vec Sim: 0.7391\n",
      "\n",
      "Saved mappings to 'token-to-cso-combined.json'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Updated script to combine Word2Vec and CSO ontology for specific test words\n",
    "Using rapidfuzz.distance.Levenshtein\n",
    "Running in main.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from rapidfuzz.distance import Levenshtein  # For Levenshtein similarity\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Add path to ontology.py\n",
    "sys.path.append(r\"C:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso-reader-main\\cso_reader\")\n",
    "\n",
    "from ontology import Ontology\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = '9M[256-10]_sg.bin'\n",
    "try:\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file '{model_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Load ontology\n",
    "try:\n",
    "    ontology = Ontology()\n",
    "    print(\"Computer Science Ontology loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ontology: {e}\")\n",
    "    raise\n",
    "\n",
    "# Parameters\n",
    "min_similarity = 0.94  # Levenshtein similarity threshold (0 to 1)\n",
    "word_similarity = 0.7  # Word2Vec similarity threshold (0 to 1)\n",
    "top_amount_of_words = 10  # Number of similar words to retrieve\n",
    "\n",
    "# Test words\n",
    "test_words = ['web', 'semantic_web', 'learning', 'ontology']  # Add or modify words here\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "\n",
    "for wet in test_words:  # Iterate over test words only\n",
    "    i += 1\n",
    "    print(f\"\\nProcessing word {i}: '{wet}'\")\n",
    "    \n",
    "    if wet not in model.key_to_index:\n",
    "        print(f\"Word '{wet}' not in model vocabulary, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    output[wet] = []\n",
    "    \n",
    "    # Get similar words from Word2Vec\n",
    "    similar_words = []\n",
    "    similar_words.append((wet, 1.0))  # Include the word itself\n",
    "    try:\n",
    "        similarities = model.most_similar(wet, topn=top_amount_of_words)\n",
    "        similar_words.extend(similarities)\n",
    "    except KeyError:\n",
    "        print(f\"Error retrieving similar words for '{wet}', skipping.\")\n",
    "        continue\n",
    "    \n",
    "    for wet2, sim in similar_words:\n",
    "        if sim >= word_similarity:\n",
    "            # Find ontology topics related to wet2 (substring matching and descendants)\n",
    "            related_by_name = [topic for topic in ontology.topics if wet2.lower() in topic.lower()]\n",
    "            descendants = ontology.get_all_descendants_of_topics([wet2]) if wet2 in ontology.topics else []\n",
    "            topics = list(set(related_by_name + descendants))\n",
    "            \n",
    "            for topic in topics:\n",
    "                # Compute Levenshtein similarity using rapidfuzz.distance.Levenshtein\n",
    "                m = Levenshtein.normalized_similarity(topic, wet2)  # Returns 0-1\n",
    "                if m >= min_similarity:\n",
    "                    output[wet].append({\n",
    "                        \"topic\": topic,\n",
    "                        \"sim_t\": m,\n",
    "                        \"wet\": wet2,\n",
    "                        \"sim_w\": sim\n",
    "                    })\n",
    "    \n",
    "    # Print mappings for the current word\n",
    "    if output[wet]:\n",
    "        print(f\"Mappings for '{wet}':\")\n",
    "        for mapping in output[wet]:\n",
    "            print(f\"  Topic: {mapping['topic']}, Levenshtein Sim: {mapping['sim_t']:.4f}, \"\n",
    "                  f\"Word2Vec Word: {mapping['wet']}, Word2Vec Sim: {mapping['sim_w']:.4f}\")\n",
    "    else:\n",
    "        print(f\"No mappings found for '{wet}' with given thresholds.\")\n",
    "\n",
    "# Save the cached model\n",
    "try:\n",
    "    with open('token-to-cso-combined.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)\n",
    "    print(\"\\nSaved mappings to 'token-to-cso-combined.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
