{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset (title + abstract)\n",
    "\n",
    "The titles are in the paper folder while the abstracts are in the abstract folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code downloads and processes academic paper data from the Semantic Scholar API. Here's what it does:\n",
    "\n",
    "1. Authentication:\n",
    "- Uses an API key to authenticate with Semantic Scholar API\n",
    "- Creates a local output directory './s2ag_dataset_title_abstracts'\n",
    "\n",
    "2. Data Download:\n",
    "- Gets the latest data release ID\n",
    "- Downloads two datasets: 'papers' and 'abstracts' \n",
    "- Saves the downloaded files as gzipped files in dataset-specific subdirectories:\n",
    "  - ./s2ag_dataset_title_abstracts/papers/papers_*.gz\n",
    "  - ./s2ag_dataset_title_abstracts/abstracts/abstracts_*.gz\n",
    "\n",
    "3. Data Processing:\n",
    "- Extracts titles and abstracts specifically for Computer Science papers\n",
    "- Matches papers with their abstracts using corpusid\n",
    "- Creates a pandas DataFrame with matched title-abstract pairs\n",
    "\n",
    "4. Output Files:\n",
    "- Gzipped raw data files in the papers/ and abstracts/ subdirectories\n",
    "- Final CSV file (csv_output_path) containing:\n",
    "  - Column 'title': Paper titles\n",
    "  - Column 'abstract': Corresponding paper abstracts\n",
    "  - Only includes Computer Science papers that have both title and abstract\n",
    "\n",
    "Note: The csv_output_path variable appears to be undefined in the code snippet,\n",
    "but the final CSV would contain the processed title-abstract pairs.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Set your S2 API key\n",
    "api_key = \"09CuB8BwpZ8j8vzF3WtEK9aMSLHM98HM77BzJNym\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"x-api-key\": api_key\n",
    "}\n",
    "\n",
    "# Base output directory locally\n",
    "base_output_dir = \"./s2ag_dataset_title_abstracts\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 1: Get the latest release ID\n",
    "latest_release_url = \"https://api.semanticscholar.org/datasets/v1/release/latest\"\n",
    "response = requests.get(latest_release_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to fetch latest release: {response.status_code} - {response.text}\")\n",
    "    exit()\n",
    "\n",
    "latest_release = response.json()\n",
    "release_id = latest_release[\"release_id\"]\n",
    "print(\"Latest release ID:\", release_id)\n",
    "\n",
    "# Step 2: Datasets to download\n",
    "datasets = [\"papers\", \"abstracts\"]\n",
    "\n",
    "# Dictionary to store data\n",
    "data = {\"title\": [], \"abstract\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    # Create dataset-specific directory\n",
    "    output_dir = os.path.join(base_output_dir, dataset_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 3: Get metadata for the dataset\n",
    "    dataset_url = f\"https://api.semanticscholar.org/datasets/v1/release/{release_id}/dataset/{dataset_name}\"\n",
    "    response = requests.get(dataset_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch metadata for {dataset_name}: {response.status_code} - {response.text}\")\n",
    "        continue\n",
    "\n",
    "    dataset_info = response.json()\n",
    "    print(f\"\\n{dataset_name.capitalize()} dataset metadata:\")\n",
    "    print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "    # Step 4: Download all dataset files with simplified file names\n",
    "    for index, file_url in enumerate(dataset_info[\"files\"]):  # Process all files\n",
    "        # Generate a shorter file name (e.g., papers_0.gz, abstracts_1.gz)\n",
    "        file_name = f\"{dataset_name}_{index}.gz\"\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        print(f\"Downloading {dataset_name}/{file_name}...\")\n",
    "        file_response = requests.get(file_url, headers=headers, stream=True)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                for chunk in file_response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            print(f\"Saved {file_name} to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: {file_response.status_code} - {file_response.text}\")\n",
    "\n",
    "# Step 5: Process the datasets to extract titles and abstracts for Computer Science papers\n",
    "print(\"\\nProcessing datasets to extract titles and abstracts for Computer Science papers...\")\n",
    "\n",
    "# Dictionary to store titles and abstracts by corpusid\n",
    "title_dict = {}\n",
    "abstract_dict = {}\n",
    "\n",
    "# Process papers dataset (for titles and field of study)\n",
    "papers_dir = os.path.join(base_output_dir, \"papers\")\n",
    "if os.path.exists(papers_dir):\n",
    "    for file_name in os.listdir(papers_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(papers_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            title = record.get(\"title\")\n",
    "                            fields_of_study = record.get(\"fieldsOfStudy\", [])\n",
    "                            # Filter for Computer Science\n",
    "                            if corpusid and title and \"Computer Science\" in fields_of_study:\n",
    "                                title_dict[corpusid] = title\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Papers directory not found. Skipping paper processing.\")\n",
    "\n",
    "# Process abstracts dataset\n",
    "abstracts_dir = os.path.join(base_output_dir, \"abstracts\")\n",
    "if os.path.exists(abstracts_dir):\n",
    "    for file_name in os.listdir(abstracts_dir):\n",
    "        if file_name.endswith(\".gz\"):\n",
    "            file_path = os.path.join(abstracts_dir, file_name)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            corpusid = record.get(\"corpusid\")\n",
    "                            abstract = record.get(\"abstract\")\n",
    "                            if corpusid and abstract:\n",
    "                                abstract_dict[corpusid] = abstract\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "else:\n",
    "    print(\"Abstracts directory not found. Skipping abstract processing.\")\n",
    "\n",
    "# Step 6: Combine titles and abstracts by corpusid for Computer Science papers\n",
    "for corpusid in title_dict:\n",
    "    if corpusid in abstract_dict:\n",
    "        data[\"title\"].append(title_dict[corpusid])\n",
    "        data[\"abstract\"].append(abstract_dict[corpusid])\n",
    "\n",
    "# Step 7: Create DataFrame and save to CSV\n",
    "if data[\"title\"]:\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nSample of DataFrame (Computer Science papers):\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved DataFrame to {csv_output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo matching title and abstract pairs found for Computer Science papers. CSV not created.\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET CONSTRUCTION\n",
    "Combine the datasets and filter English Only papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 3-step title + abstract processing pipeline...\n",
      "\n",
      " Processing English titles...\n",
      " Error reading papers_0.gz: 'NoneType' object has no attribute 'strip'\n",
      " Step 1 complete: 239,202 titles written to C://Users//Faisal Ramzan//Desktop//kmi_project_cso//paper_dataset\\clean_titles.txt\n",
      "\n",
      " Processing English abstracts...\n",
      " Step 2 complete: 859,776 abstracts written to C://Users//Faisal Ramzan//Desktop//kmi_project_cso//paper_dataset\\clean_abstracts.txt\n",
      "\n",
      " Merging titles and abstracts into final output...\n",
      " Step 3 complete: 1,711,374 lines written to C://Users//Faisal Ramzan//Desktop//kmi_project_cso//paper_dataset\\paper_dataset.txt\n",
      "\n",
      " Completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code processes and cleans academic paper titles and abstracts:\n",
    "\n",
    "1. Configuration:\n",
    "- Sets up input/output directories for papers, abstracts and results\n",
    "- Defines regex patterns to filter non-English and special characters\n",
    "- Uses buffering for efficient file I/O\n",
    "\n",
    "2. Key Functions:\n",
    "- is_strictly_english(): Validates if text is English using common academic words\n",
    "- process_titles(): Reads gzipped paper titles, cleans them, writes to output\n",
    "- process_abstracts(): Similar to process_titles() but for paper abstracts\n",
    "- merge_titles_and_abstracts(): Combines cleaned titles and abstracts\n",
    "\n",
    "3. Main Processing Pipeline:\n",
    "- Step 1: Process paper titles from gzipped JSON files\n",
    "- Step 2: Process paper abstracts from gzipped JSON files  \n",
    "- Step 3: Merge cleaned titles and abstracts into single output file\n",
    "\n",
    "The code uses:\n",
    "- Regex for text validation and cleaning\n",
    "- JSON parsing for reading input files\n",
    "- Buffered writing for performance\n",
    "- Error handling for robust processing\n",
    "- Progress tracking and user feedback\n",
    "\n",
    "Output: Clean, English-only academic text suitable for further NLP tasks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Config ---\n",
    "papers_dir = \"./s2ag_dataset_title_abstracts/papers/test_title\"\n",
    "abstracts_dir = \"./s2ag_dataset_title_abstracts/abstracts/test_abstract\"\n",
    "#output_dir = \"./s2ag_dataset_title_abstracts/output\"\n",
    "output_dir = r\"C://Users//Faisal Ramzan//Desktop//kmi_project_cso//paper_dataset\"\n",
    "\n",
    "\n",
    "titles_output_file = os.path.join(output_dir, \"clean_titles.txt\")\n",
    "abstracts_output_file = os.path.join(output_dir, \"clean_abstracts.txt\")\n",
    "merged_output_file = os.path.join(output_dir, \"paper_dataset.txt\")\n",
    "\n",
    "buffer_size = 5000 # FOR US 5000\n",
    "\n",
    "# --- Regex Patterns ---\n",
    "#SPECIAL_CHARS_PATTERN = re.compile(r'[/={}[\\]:;<>\\\\]')\n",
    "#LATEX_PATTERN = re.compile(r'<.*?>|\\\\usepackage|\\\\documentclass|\\\\begin\\{document\\}|\\\\cite\\{|\\\\ref\\{')\n",
    "LATEX_PATTERN = re.compile(r'\\\\usepackage|\\\\documentclass|\\\\begin\\{document\\}|\\\\cite\\{|\\\\ref\\{')\n",
    "#NON_ASCII_PATTERN = re.compile(r'[^\\x00-\\x7F]')\n",
    "NON_ENGLISH_UNICODE_PATTERN = re.compile(r'[٠-٩\\u0600-\\u06FF\\u4e00-\\u9fff\\uac00-\\ud7af]')\n",
    "\n",
    "def is_strictly_english(text: str, min_length: int = 10, min_words: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Validates if the given text is strictly English by checking:\n",
    "    1. Minimum length and word requirements\n",
    "    2. No non-ASCII or non-English Unicode characters\n",
    "    3. No special characters or LaTeX markup\n",
    "    4. Contains sufficient common academic English words\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to validate\n",
    "        min_length (int): Minimum required text length (default: 10)\n",
    "        min_words (int): Minimum required common words (default: 2)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text passes all English validation checks\n",
    "    \"\"\"\n",
    "    if not text or len(text) < min_length:\n",
    "        return False\n",
    "    #if NON_ASCII_PATTERN.search(text) or NON_ENGLISH_UNICODE_PATTERN.search(text) or SPECIAL_CHARS_PATTERN.search(text):\n",
    "    if NON_ENGLISH_UNICODE_PATTERN.search(text):\n",
    "        return False\n",
    "    if LATEX_PATTERN.search(text.lower()):\n",
    "        return False\n",
    "    text_sample = f\" {text.lower()[:200]} \"\n",
    "    common_words = {\n",
    "        \"the\", \"this\", \"that\", \"we\", \"our\", \"an\", \"a\", \"and\", \"is\", \"are\",\n",
    "        \"for\", \"with\", \"from\", \"by\", \"on\", \"of\", \"in\", \"to\", \"using\", \"can\",\n",
    "        \"have\", \"has\", \"as\", \"be\", \"based\", \"new\", \"approach\", \"method\",\n",
    "        \"study\", \"paper\", \"research\", \"results\", \"present\", \"analysis\",\n",
    "        \"model\", \"data\", \"system\", \"algorithm\", \"network\", \"learning\",\n",
    "        \"detection\", \"classification\", \"prediction\", \"optimization\",\n",
    "        \"propose\", \"show\", \"demonstrate\", \"evaluate\", \"performance\",\n",
    "        \"experimental\", \"implementation\", \"framework\", \"problem\", \"solution\"\n",
    "    }\n",
    "    return sum(1 for word in common_words if f\" {word} \" in text_sample) >= min_words\n",
    "\n",
    "# --- Step 1: Process Titles ---\n",
    "def process_titles():\n",
    "    print(\" Processing English titles...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(papers_dir) if f.endswith(\".gz\")]\n",
    "    total_written = 0\n",
    "    buffer = []\n",
    "\n",
    "    with open(titles_output_file, \"w\", encoding=\"utf-8\", buffering=16384) as out_f:\n",
    "        for file in files:\n",
    "            file_path = os.path.join(papers_dir, file)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            title = record.get(\"title\", \"\").strip()\n",
    "                            if title and is_strictly_english(title):\n",
    "                                buffer.append(title + \"\\n\")\n",
    "                                total_written += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "\n",
    "                        if len(buffer) >= buffer_size:\n",
    "                            out_f.writelines(buffer)\n",
    "                            buffer.clear()\n",
    "            except Exception as e:\n",
    "                print(f\" Error reading {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if buffer:\n",
    "            out_f.writelines(buffer)\n",
    "\n",
    "    print(f\" Step 1 complete: {total_written:,} titles written to {titles_output_file}\")\n",
    "\n",
    "# --- Step 2: Process Abstracts ---\n",
    "def process_abstracts():\n",
    "    print(\"\\n Processing English abstracts...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(abstracts_dir) if f.endswith(\".gz\")]\n",
    "    total_written = 0\n",
    "    buffer = []\n",
    "\n",
    "    with open(abstracts_output_file, \"w\", encoding=\"utf-8\", buffering=16384) as out_f:\n",
    "        for file in files:\n",
    "            file_path = os.path.join(abstracts_dir, file)\n",
    "            try:\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line.strip())\n",
    "                            abstract = record.get(\"abstract\", \"\").strip()\n",
    "                            if abstract and is_strictly_english(abstract, min_length=20, min_words=3):\n",
    "                                buffer.append(abstract + \"\\n\")\n",
    "                                total_written += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "\n",
    "                        if len(buffer) >= buffer_size:\n",
    "                            out_f.writelines(buffer)\n",
    "                            buffer.clear()\n",
    "            except Exception as e:\n",
    "                print(f\" Error reading {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if buffer:\n",
    "            out_f.writelines(buffer)\n",
    "\n",
    "    print(f\" Step 2 complete: {total_written:,} abstracts written to {abstracts_output_file}\")\n",
    "\n",
    "# --- Step 3: Merge Cleaned Titles + Abstracts ---\n",
    "def merge_titles_and_abstracts():\n",
    "    print(\"\\n Merging titles and abstracts into final output...\")\n",
    "\n",
    "    title_f = open(titles_output_file, \"r\", encoding=\"utf-8\")\n",
    "    abstract_f = open(abstracts_output_file, \"r\", encoding=\"utf-8\")\n",
    "    merged_f = open(merged_output_file, \"w\", encoding=\"utf-8\", buffering=16384)\n",
    "\n",
    "    count = 0\n",
    "    while True:\n",
    "        title = title_f.readline()\n",
    "        abstract = abstract_f.readline()\n",
    "\n",
    "        if not title and not abstract:\n",
    "            break\n",
    "\n",
    "        if title:\n",
    "            merged_f.write(title.strip() + \"\\n\")\n",
    "            count += 1\n",
    "        if abstract:\n",
    "            merged_f.write(abstract.strip() + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    title_f.close()\n",
    "    abstract_f.close()\n",
    "    merged_f.close()\n",
    "\n",
    "    print(f\" Step 3 complete: {count:,} lines written to {merged_output_file}\")\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting 3-step title + abstract processing pipeline...\\n\")\n",
    "    process_titles()\n",
    "    process_abstracts()\n",
    "    merge_titles_and_abstracts()\n",
    "    print(\"\\n Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
